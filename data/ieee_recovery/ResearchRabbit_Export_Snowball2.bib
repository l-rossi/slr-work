@ARTICLE{Otto_2007,title={Addressing Legal Requirements in Requirements Engineering},year={2007},author={Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1109/re.2007.65},pmid={null},pmcid={null},mag_id={2114305512},journal={null},abstract={Legal texts, such as regulations and legislation, are playing an increasingly important role in requirements engineering and system development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, regulatory compliance is critical in systems that are governed by regulations and law, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new regulations and case law. Requirements engineers and compliance auditors must be able to identify relevant regulations, extract requirements and other key concepts, and monitor compliance throughout the software lifecycle. This paper surveys research efforts over the past 50 years in handling legal texts for systems development. These efforts include the use of symbolic logic, logic programming, first-order temporal logic, deontic logic, defeasible logic, goal modeling, and semi-structured representations. This survey can aid requirements engineers and auditors to better specify, monitor, and test software systems for compliance.}}
@ARTICLE{Ghanavati_2009,title={Compliance Analysis Based on a Goal-oriented Requirement Language Evaluation Methodology},year={2009},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton and Liam Peyton},doi={10.1109/re.2009.42},pmid={null},pmcid={null},mag_id={2129011764},journal={null},abstract={In recent years, many governmental regulations have been introduced to protect the privacy of person-al information. As a result, organizations must take a systematic approach to ensure that their business processes comply with these regulations. In the past, we introduced a requirements framework that mapped regulations documents and goals to goal and scenario models of organizational processes. The intent was to help organizations document and manage the compliance of their processes in the face of evolutionary changes. In this paper, we extend our framework by incorporating regulation scenario models and by adding the notion of contribution link level to the compliance link types. These extensions result in a frame-work that is more aligned to the needs of an organization when it must evaluate and ensure the legal compliance of its organizational processes.}}
@ARTICLE{Sergot_1986,title={The British Nationality Act as a logic program},year={1986},author={Marek Sergot and Marek Sergot and Fariba Sadri and Fariba Sadri and Robert Kowalski and Robert A. Kowalski and Frank Kriwaczek and F. Kriwaczek and Peter Hammond and Peter Hammond and Hans Cory and H. T. Cory},doi={10.1145/5689.5920},pmid={null},pmcid={null},mag_id={2048600620},journal={Communications of The ACM},abstract={The formalization of legislation and the development of computer systems to assist with legal problem solving provide a rich domain for developing and testing artificial-intelligence technology.}}
@ARTICLE{Massey_2010,title={Evaluating existing security and privacy requirements for legal compliance},year={2010},author={Aaron K. Massey and Aaron K. Massey and Paul N. Otto and Paul N. Otto and Lauren J. Hayward and Lauren J. Hayward and Annie I. Antón and Annie I. Antón},doi={10.1007/s00766-009-0089-5},pmid={null},pmcid={null},mag_id={2149581025},journal={Requirements Engineering},abstract={Governments enact laws and regulations to safeguard the security and privacy of their citizens. In response, requirements engineers must specify compliant system requirements to satisfy applicable legal security and privacy obligations. Specifying legally compliant requirements is challenging because legal texts are complex and ambiguous by nature. In this paper, we discuss our evaluation of the requirements for iTrust, an open-source Electronic Health Records system, for compliance with legal requirements governing security and privacy in the healthcare domain. We begin with an overview of the method we developed, using existing requirements engineering techniques, and then summarize our experiences in applying our method to the iTrust system. We illustrate some of the challenges that practitioners face when specifying requirements for a system that must comply with law and close with a discussion of needed future research focusing on security and privacy requirements.}}
@ARTICLE{Hohfeld_1923,title={Fundamental legal conceptions as applied in judicial reasoning},year={1923},author={Wesley Newcomb Hohfeld and Wesley Newcomb Hohfeld and Wesley Newcomb Hohfeld},doi={null},pmid={null},pmcid={null},mag_id={1557385649},journal={null},abstract={Hohfeld's writings, from 1913 and 1917, delve into the distinctions between such legal concepts as privilege and duty, and right, immunity and liability, etc.}}
@ARTICLE{May_2006,title={Privacy APIs: access control techniques to analyze and verify legal privacy policies},year={2006},author={Michael J. May and Michael J. May and Carl A. Gunter and Carl A. Gunter and I. Lee and Insup Lee},doi={10.1109/csfw.2006.24},pmid={null},pmcid={null},mag_id={1903917666},journal={null},abstract={There is a growing interest in establishing rules to regulate the privacy of citizens in the treatment of sensitive personal data such as medical and financial records. Such rules must be respected by software used in these sectors. The regulatory statements are somewhat informal and must be interpreted carefully in the software interface to private data. This paper describes techniques to formalize regulatory privacy rules and how to exploit this formalization to analyze the rules automatically. Our formalism, which we call privacy APIs, is an extension of access control matrix operations to include (1) operations for notification and logging and (2) constructs that ease the mapping between legal and formal language. We validate the expressive power of privacy APIs by encoding the 2000 and 2003 HIPAA consent rules in our system. This formalization is then encoded into Promela and we validate the usefulness of the formalism by using the SPIN model checker to verify properties that distinguish the two versions of HIPAA.}}
@ARTICLE{Berman_1993,title={Representing teleological structure in case-based legal reasoning: the missing link},year={1993},author={Donald H. Berman and Donald H. Berman and Carole D. Hafner and Carole D. Hafner},doi={10.1145/158976.158982},pmid={null},pmcid={null},mag_id={1999283223},journal={null},abstract={We argue that robust case-based models of legal knowledge that represent the way in which practicing professionals use legal decisions must contain a deeper domain model that represents the purposes behind the rules articulated in the cases. We propose a  model for representing the teleological components of legal decisions, and we suggest a method for utilizing this representation in a HYPO-like framework for case-based legal argument.}}
@ARTICLE{Breaux_2008,title={Analyzing Regulatory Rules for Privacy and Security Requirements},year={2008},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Annie I. Antón},doi={10.1109/tse.2007.70746},pmid={null},pmcid={null},mag_id={2107930672},journal={IEEE Transactions on Software Engineering},abstract={Information practices that use personal, financial, and health-related information are governed by US laws and regulations to prevent unauthorized use and disclosure. To ensure compliance under the law, the security and privacy requirements of relevant software systems must properly be aligned with these regulations. However, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. These "rules" are often precursors to software requirements that must undergo considerable refinement and analysis before they become implementable. To support the software engineering effort to derive security requirements from regulations, we present a methodology for directly extracting access rights and obligations from regulation texts. The methodology provides statement-level coverage for an entire regulatory document to consistently identify and infer six types of data access constraints, handle complex cross references, resolve ambiguities, and assign required priorities between access rights and obligations to avoid unlawful information disclosures. We present results from applying this methodology to the entire regulation text of the US Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule.}}
@ARTICLE{Hohfeld_1913,title={Some Fundamental Legal Conceptions as applied in judicial reasoning},year={1913},author={Hohfeld and Hohfeld},doi={null},pmid={null},pmcid={null},mag_id={3143775862},journal={Yale Law Journal},abstract={null}}
@ARTICLE{Antón_2009,title={Legal requirements acquisition for the specification of legally compliant information systems},year={2009},author={Annie I. Antón and Annie I. Antón and Travis D. Breaux and Travis D. Breaux},doi={null},pmid={null},pmcid={null},mag_id={1493372904},journal={null},abstract={U.S. federal and state regulations impose mandatory and discretionary requirements on industry-wide business practices to achieve non-functional, societal goals such as improved accessibility, privacy and safety. The structure and syntax of regulations affects how well software engineers identify and interpret legal requirements. Inconsistent interpretations can lead to noncompliance and violations of the law. To support software engineers who must comply with these regulations, I propose a Frame-Based Requirements Analysis Method (FBRAM) to acquire and specify legal requirements from U.S. federal regulatory documents. The legal requirements are systematically specified using a reusable, domain-independent upper ontology, natural language phrase heuristics, a regulatory document model and a frame-based markup language. The methodology maintains traceability from regulatory statements and phrases to formal properties in a frame-based model and supports the resolution of multiple types of legal ambiguity. The methodology is supported by a software prototype to assist engineers with applying the model and with analyzing legal requirements. This work is validated in three domains, information privacy, information accessibility and aviation safety, which are governed by the Health Insurance Portability and Accountability Act of 1996, the Rehabilitation Act Amendments of 1998, and the Federal Aviation Act of 1958, respectively.}}
@ARTICLE{Aleven_1997,title={Teaching case-based argumentation through a model and examples},year={1997},author={Vincent Aleven and Vincent Aleven},doi={null},pmid={null},pmcid={null},mag_id={1524164055},journal={null},abstract={CATO is an intelligent learning environment designed to help beginning law students learn basic skills of making arguments with cases. Using CATO, students practice tasks of induction and analogical argumentation. They practice testing theories against a body of cases and making written arguments about a problem, comparing and contrasting it to past cases.
CATO's model addresses arguments in which two opponents analogize a problem to favorable cases, distinguish unfavorable cases, assess the significance of similarities and differences between cases in light of normative knowledge about the domain, and use that knowledge to organize multi-case arguments. CATO communicates the model to students by presenting dynamically-generated argumentation examples and by reifying argument structure based on the model. CATO also provides a case database and tools based on the model that help make students' tasks more manageable.
CATO was evaluated in the context of an actual legal writing course, in a study involving 30 first-year law students. We found that instruction with CATO leads to statistically significant improvement in students' basic argumentation skills, comparable to that achieved by an experienced legal writing instructor teaching groups of 4-10 students. However, on a more advanced legal writing assignment, meant to explore the frontier of the CATO instruction, students taught by the legal writing instructor had higher grades, suggesting a need for more integrated practice with the CATO model.
CATO contributes to AI research fields modeling educational techniques as well as case-based and legal reasoning. It is a novel result that students can learn basic argumentation skills by studying computer-generated examples. It means that an instructional system does not necessarily need to rely on a very sophisticated understanding of students' arguments, which would be a significant obstacle to developing such systems.
Also, CATO presents novel techniques for using background knowledge to support similarity assessment in case-based reasoning. Drawing on its background knowledge, CATO characterizes and re-characterizes cases in order to argue that two cases are similar or different. This is an important skill in the legal domain not previously modeled. CATO's arguments may help a user in assessing the similarity of cases in a more discriminatory way.}}
@ARTICLE{Ashley_1991,title={Modeling Legal Argument: Reasoning with Cases and Hypotheticals},year={1991},author={Kevin D. Ashley and Kevin D. Ashley},doi={null},pmid={null},pmcid={null},mag_id={1953136326},journal={null},abstract={This dissertation is about adversarial, case-based reasoning and the HYPO program that performs adversarial reasoning with cases and hypotheticals in the legal domain. The dissertation identifies and describes basic case-based operations, an adversarial, case-based reasoning process, a schematic structure for case-based arguments, the kinds of counter-examples that arise and the knowledge sources necessary to support adversarial, case-based reasoning.
The HYPO program embodies the methodology. It comprises: (1) a structured Case Knowledge Base ("CKB") of actual legal cases; (2) an indexing scheme ("dimensions") for retrieval of relevant cases from the CKB; (3) methods for analyzing problem situations and retrieving relevant cases; (4) methods for interpreting and assessing the relevancy of past cases by "positioning" the problem situation with respect to relevant existing cases in the CKB as seen from the viewpoint of the problem at hand and finding the most-on-point cases; (5) methods for comparing/contrasting cases (e.g., citing, distinguishing, finding counter-examples); (6) methods for posing hypotheticals that test the sensitivity of the problem situation to changes, particularly with regard to potentially adverse effects of new damaging facts coming to light and existing favorable ones being discredited; (7) methods for generating "3-ply" argument outlines to play out realistic legal arguments citing cases in a manner familiar to attorneys; and (8) methods for explaining alternative decisions of the problem situation by posing hypotheticals, comparing arguments and summarizing the precedents. HYPO's performance compares favorably to that of judges and attorneys in actual legal cases.
The law is an excellent domain to study case-based reasoning since by its very nature it: (1) espouses a doctrine of precedent in which prior cases are the primary tools for justifying legal conclusions; and (2) employs precedential reasoning to make up for the lack of strong domain models with which to reason deductively about problem situations. The law is also a paradigm for adversarial case-based reasoning; there are "no right answers", only arguments pitting interpretations of cases and facts against each other.
The dissertation addresses issues of central concern to Artificial Intelligence including: relevance and credit assignment, indexing and inference control, argumentation, analogical reasoning and explanation.}}
@ARTICLE{Prakken_1998,title={Modelling Reasoning with Precedents in a Formal Dialogue Game},year={1998},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1007/978-94-015-9010-5_5},pmid={null},pmcid={null},mag_id={129283682},journal={Artificial Intelligence and Law},abstract={This paper analyses legal reasoning with precedents in the setting of a formally defined dialogue game. After giving a legal-theoretical account of judicial reasoning with precedents, a formal method is proposed for representing precedents and it is discussed how such representations can be used in a formally defined dialectical protocol for dispute. The basic ideas are to represent cases as argument structures (including pro and con arguments, and the arguments for adjudicating their conflicts) and to define certain case-based reasoning moves as strategies for introducing information into a dispute. In particular, analogizing and distinguishing are conceived as elementary theory construction moves, which produce new information on the basis of an existing stock of cases. The approach also offers the possibility of using portions of precedents and of expressing criteria for determining the outcome of precedent-based disputes.}}
@ARTICLE{Ghanavati_2007,title={Towards a framework for tracking legal compliance in healthcare},year={2007},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton and Liam Peyton},doi={10.1007/978-3-540-72988-4_16},pmid={null},pmcid={null},mag_id={1549681102},journal={null},abstract={Hospitals strive to improve the quality of the healthcare they provide. To achieve this, they require access to health data. These data are sensitive since they contain personal information. Governments have legislation to ensure that privacy is respected and hospitals must comply with it. Unfortunately, most of the procedures meant to control access to health information remain paper-based, making it difficult to trace. In this paper, we introduce a framework based on the User Requirements Notation that models the business processes of a hospital and links them with legislation such as the Ontario Personal Health Information Privacy Act (PHIPA). We analyze different types of links, their functionality, and usefulness in complying with privacy law. This framework will help health information custodians track compliance and indicate how their business processes can be improved.}}
@ARTICLE{Brüninghaus_2003,title={Predicting outcomes of case based legal arguments},year={2003},author={Stefanie Brüninghaus and Stefanie Brüninghaus and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/1047788.1047838},pmid={null},pmcid={null},mag_id={1995770214},journal={null},abstract={In this paper, we introduce IBP, an algorithm that combines reasoning with an abstract domain model and case-based reasoning techniques to predict the outcome of case-based legal arguments. Unlike the predictions generated by statistical or machine-learning techniques, IBP's predictions are accompanied by explanations.We describe an empirical evaluation of IBP, in which we compare our algorithm to prediction based on Hypo's and CATO's relevance criteria, and to a number of widely used machine learning algorithms. IBP reaches higher accuracy than all competitors, and hypothesis testing shows that the observed differences are statistically significant. An ablation study indicates that both sources of knowledge in IBP contribute to the accuracy of its predictions.}}
@ARTICLE{Toulmin_1958,title={The uses of argument},year={1958},author={Stephen Toulmin and Stephen Toulmin},doi={null},pmid={null},pmcid={null},mag_id={1997210479},journal={null},abstract={Preface Introduction 1. Fields of argument and modals 2. Probability 3. The layout of arguments 4. Working logic and idealised logic 5. The origins of epistemological theory Conclusion References Index.}}
@ARTICLE{Mackenzie_1979,title={Question-begging in non-cumulative systems},year={1979},author={Jim Mackenzie and Jim Mackenzie},doi={10.1007/bf00258422},pmid={null},pmcid={null},mag_id={2029320362},journal={Journal of Philosophical Logic},abstract={null}}
@ARTICLE{Skalak_1992,title={Arguments and cases: An inevitable intertwining},year={1992},author={David B. Skalak and David B. Skalak and Edwina L. Rissland and Edwina L. Rissland},doi={10.1007/bf00118477},pmid={null},pmcid={null},mag_id={2087843564},journal={Artificial Intelligence and Law},abstract={We discuss several aspects of legal arguments, primarily arguments about the meaning of statutes. First, we discuss how the requirements of argument guide the specification and selection of supporting cases and how an existing case base influences argument formation. Second, we present,our evolving taxonomy of patterns of actual legal argument. This taxonomy builds upon our much earlier work on `argument moves" and also on our more recent analysis of how cases are used to support arguments for the interpretation of legal statutes. Third, we show how the theory of argument used by CABARET, a hybrid case-based/rule-based reasoner, can support many of the argument patterns in our taxonomy.}}
@ARTICLE{Dung_1995,title={On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n -person games},year={1995},author={Phan Minh Dung and Phan Minh Dung},doi={10.1016/0004-3702(94)00041-x},pmid={null},pmcid={null},mag_id={2104126268},journal={Artificial Intelligence},abstract={The purpose of this paper is to study the fundamental mechanism, humans use in argumentation, and to explore ways to implement this mechanism on computers. We do so by first developing a theory for argumentation whose central notion is the acceptability of arguments. Then we argue for the “correctness” or “appropriateness” of our theory with two strong arguments. The first one shows that most of the major approaches to nonmonotonic reasoning in AI and logic programming are special forms of our theory of argumentation. The second argument illustrates how our theory can be used to investigate the logical structure of many practical problems. This argument is based on a result showing that our theory captures naturally the solutions of the theory of n-person games and of the well-known stable marriage problem. By showing that argumentation can be viewed as a special form of logic programming with negation as failure, we introduce a general logic-programming-based method for generating meta-interpreters for argumentation systems, a method very much similar to the compiler-compiler idea in conventional programming. Keyword: Argumentation; Nonmonotonic reasoning; Logic programming; n-person games; The stable marriage problem}}
@ARTICLE{Ghanavati_2011,title={A systematic review of goal-oriented requirements management frameworks for business process compliance},year={2011},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton and Liam Peyton},doi={10.1109/relaw.2011.6050270},pmid={null},pmcid={null},mag_id={2106560733},journal={null},abstract={Legal compliance has been an active topic in Software Engineering and Information Systems for many years. However, business analysts and others recently started exploiting Requirements Engineering techniques, and in particular goal-oriented approaches, to model and reason about legal documents in system design and business process management. Many contributions involve extracting legal requirements, providing law-compliant business processes, as well as managing and maintaining compliance. In this paper, we report on a systematic literature review focusing on goal-oriented legal compliance of business processes. 88 papers were selected out of nearly 800 unique papers extracted from five search engines, with manual additions from the Requirements Engineering Journal and four relevant conferences. We grouped these papers in eight categories based on a set of criteria and then highlight their main contributions. We found that the main areas for contributions have been in extracting legal requirements, modeling them with goal modeling languages, and integrating them with business processes. We identify gaps and opportunities for future work in areas related to prioritization to improve compliance, templates for generating law-compliant processes, general links between legal requirements, goal models, and business processes, and semi-automation of legal compliance and analysis.}}
@ARTICLE{Bench‐Capon_2003,title={Persuasion in Practical Argument Using Value-based Argumentation Frameworks},year={2003},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1093/logcom/13.3.429},pmid={null},pmcid={null},mag_id={2116246756},journal={Journal of Logic and Computation},abstract={In many cases of disagreement, particularly in situations involving practical reasoning, it is impossible to demonstrate conclusively that either party is wrong. The role of argument in such cases is to persuade rather than to prove, demonstrate or refute. Following Perelman, we argue that persuasion in such cases relies on a recognition that the strength of an argument depends on the social values that it advances, and that whether the attack of one argument on another succeeds depends on the comparative strength of the values advanced by the arguments concerned. To model this we extend the standard notion of Argumentation Frameworks (AFs) to Value-based Argumentation Frameworks (VAFs). After defining VAFs we explore their properties, and show how they can provide a rational basis for the acceptance or rejection of arguments, even where this would appear to be a matter of choice in a standard AF .I n particular we show that in a VAF certain arguments can be shown to be acceptable however the relative strengths of the values involved are assessed. This means that disputants can concur on the acceptance of arguments, even when they differ as to which values are more important, and hence that we can identify points for which persuasion should be possible. We illustrate the above using an example moral debate. We then show how factual considerations can be admitted to our framework and discuss the possibility of persuasion in the face of uncertainty and disagreement as to values.}}
@ARTICLE{Chorley_2005,title={AGATHA: using heuristic search to automate the construction of case law theories},year={2005},author={Alison Chorley and Alison Chorley and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-006-9004-2},pmid={null},pmcid={null},mag_id={2134211747},journal={Artificial Intelligence and Law},abstract={In this paper we describe AGATHA, a program designed to automate the process of theory construction in case based domains. Given a seed case and a number of precedent cases, the program uses a set of argument moves to generate a search space for a dialogue between the parties to the dispute. Each move is associated with a set of theory constructors, and thus each point in the space can be associated with a theory intended to explain the seed case and the other cases in the domain. The space is large and so an heuristic search method is needed. This paper describes two methods based on A* and alpha/beta pruning and also a series of experiments designed to explore the appropriateness of different evaluation functions, the most useful precedents to use as seed cases and the quality of the resulting theories.}}
@ARTICLE{Prakken_null,title={null},year={null},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1023/a:1008278309945},pmid={null},pmcid={null},mag_id={4250868552},journal={Artificial Intelligence and Law},abstract={null}}
@ARTICLE{Walton_2008,title={Argumentation Schemes: Acknowledgments},year={2008},author={Douglas Walton and Douglas Walton and Christopher A. Reed and Chris Reed and Fabrizio Macagno and Fabrizio Macagno},doi={10.1017/cbo9780511802034},pmid={null},pmcid={null},mag_id={581684831},journal={null},abstract={This book provides a systematic analysis of many common argumentation schemes and a compendium of 96 schemes. The study of these schemes, or forms of argument that capture stereotypical patterns of human reasoning, is at the core of argumentation research. Surveying all aspects of argumentation schemes from the ground up, the book takes the reader from the elementary exposition in the first chapter to the latest state of the art in the research efforts to formalize and classify the schemes, outlined in the last chapter. It provides a systematic and comprehensive account, with notation suitable for computational applications that increasingly make use of argumentation schemes.}}
@ARTICLE{Yin_1984,title={Case Study Research: Design and Methods},year={1984},author={Robert K. Yin and Robert K. Yin},doi={null},pmid={null},pmcid={null},mag_id={1527311855},journal={null},abstract={Foreword, by Donald T. Campbell Preface 1. INTRODUCTION: How to Know Whether and When to Use Case Studies as a Research Method The Case Study as a Research Method Comparing Case Studies With Other Research Methods in the Social Sciences Different Kinds of Case Studies, But a Common Definition Summary 2. DESIGNING CASE STUDIES: Identifying Your Case(s) and Establishing the Logic of Your Case Study General Approach to Designing Case Studies Criteria for Judging the Quality of Research Designs Case Study Designs Modest Advice in Selecting Case Study Designs 3. PREPARING TO COLLECT CASE STUDY EVIDENCE: What You Need to Do Before Starting to Collect Case Study Data The Case Study Investigator: Desired Skills Preparation and Training for a Specific Case Study The Case Study Protocol Screening the Candidate "Cases" for Your Case Study The Pilot Case Study Summary 4. COLLECTING CASE STUDY EVIDENCE: The Principles You Should Follow in Working With Six Sources of Evidence Six Sources of Evidence Three Principles of Data Collection Summary 5. ANALYZING CASE STUDY EVIDENCE: How to Start Your Analysis, Your Analytic Choices, and How They Work An Analytic Strategy: More Than Familiarity With Analytic Tools Five Analytic Techniques Pressing for a High-Quality Analysis Summary 6. REPORTING CASE STUDIES: How and What to Compose Targeting Case Study Reports Case Study Reprots as Part of Larger, Mixed Methods Studies Illustrative Structures for Case Study Compositions Procedures in Doing a Case Study Report What Makes an Examplary Case Study? References Author Index Subject Index About the Author}}
@ARTICLE{Becker_1949,title={An Introduction to Legal Reasoning},year={1949},author={Walter Becker and Edward Hirsch Levi and Chad Perry and Max Rheinstein and Edward H. Levi},doi={null},pmid={null},pmcid={null},mag_id={1605421440},journal={null},abstract={This volume will be of interest and value to students of logic, ethics, and political philosophy, as well as to members of the legal profession and to everyone concerned with problems of government and jurisprudence. By citing a large number of cases, the author makes his presentation of the proceses of judicial interpretation particularly lucid.}}
@ARTICLE{Gordon_2009,title={Proof Burdens and Standards},year={2009},author={Thomas F. Gordon and Thomas F. Gordon and Douglas Walton and Douglas Walton},doi={10.1007/978-0-387-98197-0_12},pmid={null},pmcid={null},mag_id={1636613648},journal={null},abstract={This chapter explains the role of proof burdens and standards in argumentation, illustrates them using legal procedures, and surveys the history of research on computational models of these concepts. It also presents an original computational model which aims to integrate the features of these prior systems. The ‘mainstream’ conception of argumentation in the field of artificial intelligence is monological and deductive [6]. Argumentation is viewed as taking place against the background of an inconsistent knowledge base, where the knowledge base is a set of propositions represented in some formal logic. Argumentation in this conception is a method for deducing warranted propositions from an inconsistent knowledge base. Which statements are warranted depends on attack relations among the arguments [10] which can be constructed from the knowledge base. The notions of proof standards and burden of proof become relevant only when argumentation is viewed as a dialogical process for making justified decisions. The input to the process is an initial claim or issue. The goal of the process is to clarify and decide the issues, and produce a justification of the decision which can withstand a critical evaluation by a particular audience. The role of the audience could be played by the respondent or a neutral-third party, depending on the type of dialogue. The output of this process consists of: 1) a set of claims, 2) the decision to accept or reject each claim, 3) a theory of the generalizations of the domain and the facts of the particular case, and 4) a proof justifying the decision of each issues, showing how the decision is supported by the theory. Notice that a theory or knowledge-base is part of the output of argumentation dialogues, not, as in the deductive conception, its input. This is because, as has been}}
@ARTICLE{Gordon_1994,title={The Pleadings Game: an exercise in computational dialectics},year={1994},author={Thomas F. Gordon and Thomas F. Gordon},doi={10.1007/bf00871972},pmid={null},pmcid={null},mag_id={1879898357},journal={Artificial Intelligence and Law},abstract={The Pleadings Game is a normative formalization and computational model of civil pleading, founded in Roberty Alexy's discourse theory of legal argumentation. The consequences of arguments and counterarguments are modelled using Geffner and Pearl's nonmonotonic logic,conditional entailment. Discourse in focussed using the concepts of issue and relevance. Conflicts between arguments can be resolved by arguing about the validity and priority of rules, at any level. The computational model is fully implemented and has been tested using examples from Article Nine of the Uniform Commercial Code.}}
@ARTICLE{Gordon_2012,title={Reconciling multi-jurisdictional legal requirements: A case study in requirements water marking},year={2012},author={David G. Gordon and David G. Gordon and Travis D. Breaux and Travis D. Breaux},doi={10.1109/re.2012.6345843},pmid={null},pmcid={null},mag_id={1963515996},journal={null},abstract={Companies that own, license, or maintain personal information face a daunting number of privacy and security regulations. Companies are subject to new regulations from one or more governing bodies, when companies introduce new or existing products into a jurisdiction, when regulations change, or when data is transferred across political borders. To address this problem, we developed a framework called “requirements water marking” that business analysts can use to align and reconcile requirements from multiple jurisdictions (municipalities, provinces, nations) to produce a single high or low standard of care. We evaluate the framework in an empirical case study conducted over a subset of U.S. data breach notification laws that require companies to secure their data and notify consumers in the event of data loss or theft. In this study, applying our framework reduced the number of requirements a company must comply with by 76% across 8 jurisdictions. We show how the framework surfaces critical requirements trade-offs and potential regulatory conflicts that companies must address during the reconciliation process. We summarize our results, including surveys of information technology law experts to contextualize our empirical results in legal practice.}}
@ARTICLE{Bench‐Capon_1987,title={Logic programming for large scale applications in law: A formalisation of supplementary benefit legislation},year={1987},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and G. O. Robinson and G. O. Robinson and Tom Routen and T. W. Routen and Marek Sergot and Marek Sergot},doi={10.1145/41735.41757},pmid={null},pmcid={null},mag_id={1965774555},journal={null},abstract={Article Free Access Share on Logic programming for large scale applications in law: A formalisation of supplementary benefit legislation Authors: T. J. M. Bench-Capon Imperial College, London, England, UK Imperial College, London, England, UKView Profile , G. O. Robinson Imperial College, London, England, UK Imperial College, London, England, UKView Profile , T. W. Routen Imperial College, London, England, UK Imperial College, London, England, UKView Profile , M. J. Sergot Imperial College, London, England, UK Imperial College, London, England, UKView Profile Authors Info & Claims ICAIL '87: Proceedings of the 1st international conference on Artificial intelligence and lawDecember 1987 Pages 190–198https://doi.org/10.1145/41735.41757Online:01 December 1987Publication History 57citation578DownloadsMetricsTotal Citations57Total Downloads578Last 12 Months75Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Atkinson_2007,title={Practical reasoning as presumptive argumentation using action based alternating transition systems},year={2007},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1016/j.artint.2007.04.009},pmid={null},pmcid={null},mag_id={2003066662},journal={Artificial Intelligence},abstract={In this paper we describe an approach to practical reasoning, reasoning about what it is best for a particular agent to do in a given situation, based on presumptive justifications of action through the instantiation of an argument scheme, which is then subject to examination through a series of critical questions. We identify three particular aspects of practical reasoning which distinguish it from theoretical reasoning. We next provide an argument scheme and an associated set of critical questions which is able to capture these features. In order that both the argument scheme and the critical questions can be given precise interpretations we use the semantic structure of an Action-Based Alternating Transition System as the basis for their definition. We then work through a detailed example to show how this approach to practical reasoning can be applied to a problem solving situation, and briefly describe some other previous applications of the general approach. In a second example we relate our account to the social laws paradigm for co-ordinating multi-agent systems. The contribution of the paper is to provide firm foundations for an approach to practical reasoning based on presumptive argument in terms of a well-known model for representing the effects of actions of a group of agents.}}
@ARTICLE{Bench‐Capon_2012,title={Representing Popov v Hayashi with dimensions and factors},year={2012},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-012-9118-7},pmid={null},pmcid={null},mag_id={2016919642},journal={Artificial Intelligence and Law},abstract={Modelling reasoning with legal cases has been a central concern of AI and Law since the 1980s. The approach which represents cases as factors and dimensions has been a central part of that work. In this paper I consider how several varieties of the approach can be applied to the interesting case of Popov v Hayashi. After briefly reviewing some of the key landmarks of the approach, the case is represented in terms of factors and dimensions, and further explored using theory construction and argumentation schemes approaches.}}
@ARTICLE{Massey_2009,title={Prioritizing Legal Requirements},year={2009},author={Aaron K. Massey and Aaron K. Massey and Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1109/relaw.2009.9},pmid={null},pmcid={null},mag_id={2018357323},journal={null},abstract={Requirements prioritization is used in the early phases of software development to determine the order in which requirements should be implemented. Requirements are not all equally important to the final software system because time constraints, expense, and design can each raise the urgency of implementing some requirements before others. Laws and regulations can make requirements prioritization particularly challenging due to the high costs of noncompliance and the substantial amount of domain knowledge needed to make prioritization decisions. In the context of legal requirements, implementation order ideally should be influenced by the laws and regulations governing a given software system. In this paper, we present a prioritization technique for legal requirements. We apply our technique on a set of 63 functional requirements for an open-source electronic health records system that must comply with the U.S. Health Insurance Portability and Accountability Act.}}
@ARTICLE{Prakken_1996,title={A dialectical model of assessing conflicting arguments in legal reasoning},year={1996},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1007/bf00118496},pmid={null},pmcid={null},mag_id={2024102269},journal={Artificial Intelligence and Law},abstract={Inspired by legal reasoning, this paper presents a formal framework for assessing conflicting arguments. Its use is illustrated with applications to realistic legal examples, and the potential for implementation is discussed. The framework has the form of a logical system for defeasible argumentation. Its language, which is of a logic-programming-like nature, has both weak and explicit negation, and conflicts between arguments are decided with the help of priorities on the rules. An important feature of the system is that these priorities are not fixed, but are themselves defeasibly derived as conclusions within the system. Thus debates on the choice between conflicting arguments can also be modelled.

The proof theory of the system is stated in dialectical style, where a proof takes the form of a dialogue between a proponent and an opponent of an argument. An argument is shown to be justified if the proponent can make the opponent run out of moves in whatever way the opponent attacks. Despite this dialectical form, the system reflects a `declarative', or `relational' approach to modelling legal argument. A basic assumption of this paper is that this approach complements two other lines of research in AI and Law, investigations of precedent-based reasoning and the development of `procedural', or `dialectical' models of legal argument.}}
@ARTICLE{Marshall_1989,title={Representing the structure of a legal argument},year={1989},author={Cathy Marshall and Catherine C. Marshall},doi={10.1145/74014.74031},pmid={null},pmcid={null},mag_id={2024729236},journal={null},abstract={The investigation described in this paper is part of a larger project to characterize and develop computational tools to help people formulate, record, and present arguments and rationale in diverse domains such as law, policy, and design where argumentation and decision-making are fundamental processes. To build such tools, it is necessary first to design a uniform representation for the structure of arguments; law has provided us with a good starting point for understanding this structure. Since arguments are important legal artifacts, law maintains a recorded institutional memory of them in forms such as casebooks, databases, and courtroom transcripts. This analysis is primarily concerned with the arguments that occur in two excerpts from Supreme Court oral argument transcripts; it is directed toward developing a system of semi-formal representations of the structure of these arguments in hypertext. A system of representations of argument structure, coupled with an understanding of the argumentation process, can be used to form the basis for tools for authoring, fault-detection, and other activities associated with formulating and presenting rationale.}}
@ARTICLE{Rissland_1996,title={BankXX: Supporting legal arguments through heuristic retrieval},year={1996},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak and Menahem Friedman and M. Timur Friedman},doi={10.1007/bf00123994},pmid={null},pmcid={null},mag_id={2037262533},journal={Artificial Intelligence and Law},abstract={The BankXX system models the process of perusing and gathering information for argument as a heuristic best-first search for relevant cases, theories, and other domain-specific information. As BankXX searches its heterogeneous and highly interconnected network of domain knowledge, information is incrementally analyzed and amalgamated into a dozen desirable ingredients for argument (called argument pieces), such as citations to cases, applications of legal theories, and references to prototypical factual scenarios. At the conclusion of the search, BankXX outputs the set of argument pieces filled with harvested material relevant to the input problem situation.

This research explores the appropriateness of the search paradigm as a framework for harvesting and mining information needed to make legal arguments. In this article, we describe how legal research fits the heuristic search framework and detail how this model is used in BankXX. We describe the BankXX program with emphasis on its representation of legal knowledge and legal argument. We describe the heuristic search mechanism and evaluation functions that drive the program. We give an extended example of the processing of BankXX on the facts of an actual legal case in BankXX's application domain -- the good faith question of Chapter 13 personal bankruptcy law. We discuss closely related research on legal knowledge representation and retrieval and the use of search for case retrieval or tasks related to argument creation. Finally we review what we believe are the contributions of this research to the understanding of the diverse disciplines it addresses.}}
@ARTICLE{Antón_2003,title={Precluding incongruous behavior by aligning software requirements with security and privacy policies},year={2003},author={Annie I. Antón and Annie I. Antón and Julia B. Earp and Julia B. Earp and Ryan A. Carter and Ryan A. Carter},doi={10.1016/s0950-5849(03)00095-8},pmid={null},pmcid={null},mag_id={2061202190},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Grabmair_2011,title={Facilitating case comparison using value judgments and intermediate legal concepts},year={2011},author={Matthias Grabmair and Matthias Grabmair and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/2018358.2018382},pmid={null},pmcid={null},mag_id={2065776138},journal={null},abstract={This paper explains and illustrates in an example context how case comparison in legal case-based reasoning can be modeled in the value judgment formalism. It presents a set of argument schemes corresponding to typical moves in case-based reasoning which make use of intermediate legal concepts and their impact on the applicable values.}}
@ARTICLE{Barth_2006,title={Privacy and contextual integrity: framework and applications},year={2006},author={Adam Barth and Adam Barth and Anupam Datta and Anupam Datta and John C. Mitchell and John C. Mitchell and Helen Nissenbaum and Helen Nissenbaum},doi={10.1109/sp.2006.32},pmid={null},pmcid={null},mag_id={2066266646},journal={null},abstract={Contextual integrity is a conceptual framework for understanding privacy expectations and their implications developed in the literature on law, public policy, and political philosophy. We formalize some aspects of contextual integrity in a logical framework for expressing and reasoning about norms of transmission of personal information. In comparison with access control and privacy policy frameworks such as RBAC, EPAL, and P3P, these norms focus on who personal information is about, how it is transmitted, and past and future actions by both the subject and the users of the information. Norms can be positive or negative depending on whether they refer to actions that are allowed or disallowed. Our model is expressive enough to capture naturally many notions of privacy found in legislation, including those found in HIPAA, COPPA, and GLBA. A number of important problems regarding compliance with privacy norms, future requirements associated with specific actions, and relations between policies and legal standards reduce to standard decision procedures for temporal logic.}}
@ARTICLE{Gordon_1993,title={The pleadings game: formalizing procedural justice},year={1993},author={Thomas F. Gordon and Thomas F. Gordon},doi={10.1145/158976.158978},pmid={null},pmcid={null},mag_id={2068144010},journal={null},abstract={The Pleadings Game is a normative formalization and computational model of civil pleading, founded in Robert Alexy's discourse theory of legal argumentation. The consequences of arguments and counterarguments are modelled using Geffner and Pearl's non-monotonic logic,  conditional entailment . Discourse is focussed using the concepts of issue and relevance. Conflicts between arguments can be resolved by arguing about the validity and priority rules, at any level. The computational model is fully implemented and has been tested using examples from Article Nine of the Uniform Commercial Code.}}
@ARTICLE{Modgil_2014,title={The ASPIC+ framework for structured argumentation: a tutorial},year={2014},author={Sanjay Modgil and Sanjay Modgil and Henry Prakken and Henry Prakken and Hendrik Prakken},doi={10.1080/19462166.2013.869766},pmid={null},pmcid={null},mag_id={2088166168},journal={Argument & Computation},abstract={This article gives a tutorial introduction to the ASPIC+ framework for structured argumentation. The philosophical and conceptual underpinnings of ASPIC+ are discussed, the main definitions are illustrated with examples and several ways are discussed to instantiate the framework and to reconstruct other approaches as special cases of the framework. The ASPIC+ framework is based on two ideas: the first is that conflicts between arguments are often resolved with explicit preferences, and the second is that arguments are built with two kinds of inference rules: strict, or deductive rules, whose premises guarantee their conclusion, and defeasible rules, whose premises only create a presumption in favour of their conclusion. Accordingly, arguments can in ASPIC+ be attacked in three ways: on their uncertain premises, or on their defeasible inferences, or on the conclusions of their defeasible inferences. ASPIC+ is not a system but a framework for specifying systems. A main objective of the study of the ASPIC+ f...}}
@ARTICLE{Prakken_1995,title={From logic to dialectics in legal argument},year={1995},author={Henry Prakken and Henry Prakken},doi={10.1145/222092.222230},pmid={null},pmcid={null},mag_id={2089679326},journal={null},abstract={Article Free Access Share on From logic to dialectics in legal argument Author: Henry Prakken Computer/Law Institute, Faculty of Law, Free University, Amsterdam Computer/Law Institute, Faculty of Law, Free University, AmsterdamView Profile Authors Info & Claims ICAIL '95: Proceedings of the 5th international conference on Artificial intelligence and lawMay 1995 Pages 165–174https://doi.org/10.1145/222092.222230Online:24 May 1995Publication History 24citation459DownloadsMetricsTotal Citations24Total Downloads459Last 12 Months26Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Prakken_2006,title={Formal systems for persuasion dialogue},year={2006},author={Henry Prakken and Henry Prakken},doi={10.1017/s0269888906000865},pmid={null},pmcid={null},mag_id={2100758964},journal={Knowledge Engineering Review},abstract={This article reviews formal systems that regulate persuasion dialogues. In such dialogues two or more participants aim to resolve a difference of opinion, each trying to persuade the other participants to adopt their point of view. Systems for persuasion dialogue have found application in various fields of computer science, such as non-monotonic logic, artificial intelligence and law, multi-agent systems, intelligent tutoring and computer-supported collaborative argumentation. Taking a game-theoretic view on dialogue systems, this review proposes a formal specification of the main elements of dialogue systems for persuasion and then uses it to critically review some of the main formal systems for persuasion. The focus of this review will be on regulating the interaction between agents rather than on the design and behaviour of individual agents within a dialogue.}}
@ARTICLE{Gordon_2007,title={The Carneades model of argument and burden of proof},year={2007},author={Thomas F. Gordon and Thomas F. Gordon and Henry Prakken and Henry Prakken and Douglas Walton and Douglas Walton},doi={10.1016/j.artint.2007.04.010},pmid={null},pmcid={null},mag_id={2109409153},journal={Artificial Intelligence},abstract={We present a formal, mathematical model of argument structure and evaluation, taking seriously the procedural and dialogical aspects of argumentation. The model applies proof standards to determine the acceptability of statements on an issue-by-issue basis. The model uses different types of premises (ordinary premises, assumptions and exceptions) and information about the dialectical status of statements (stated, questioned, accepted or rejected) to allow the burden of proof to be allocated to the proponent or the respondent, as appropriate, for each premise separately. Our approach allows the burden of proof for a premise to be assigned to a different party than the one who has the burden of proving the conclusion of the argument, and also to change the burden of proof or applicable proof standard as the dialogue progresses from stage to stage. Useful for modeling legal dialogues, the burden of production and burden of persuasion can be handled separately, with a different responsible party and applicable proof standard for each. Carneades enables critical questions of argumentation schemes to be modeled as additional premises, using premise types to capture the varying effect on the burden of proof of different kinds of questions.}}
@ARTICLE{Walton_1995,title={Argumentation Schemes for Presumptive Reasoning},year={1995},author={Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={2129213188},journal={null},abstract={Contents: Preface. Introduction. Presumptive Reasoning. The Argumentation Schemes. Argument from Ignorance. Ignoring Qualifications. Argument from Consequences.}}
@ARTICLE{Massacci_2005,title={Using a security requirements engineering methodology in practice: The compliance with the Italian data protection legislation},year={2005},author={Fabio Massacci and Fabio Massacci and Marco Prest and Marco Prest and Nicola Zannone and Nicola Zannone},doi={10.1016/j.csi.2005.01.003},pmid={null},pmcid={null},mag_id={2129527513},journal={Computer Standards & Interfaces},abstract={Extending Requirements Engineering modelling and formal analysis methodologies to cope with Security Requirements has been a major effort in the past decade. Yet, only few works describe complex case studies that show the ability of the informal and formal approaches to cope with the level complexity required by compliance with ISO-17799 security management requirements. In this paper we present a comprehensive case study of the application of the Secure Tropos RE methodology for compliance to the Italian legislation on Privacy and Data Protection by the University of Trento, leading to the definition and analysis of a ISO-17799-like security management scheme.}}
@ARTICLE{Breaux_2008,title={Legal Requirements, Compliance and Practice: An Industry Case Study in Accessibility},year={2008},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Annie I. Antón and Kenneth M. Boucher and K. Boucher and Merlin Dorfman and M. Dorfman},doi={10.1109/re.2008.36},pmid={null},pmcid={null},mag_id={2134271348},journal={null},abstract={U.S. laws and regulations are designed to support broad societal goals, such as accessibility, privacy and safety. To demonstrate that a product complies with these goals, businesses need to identify and refine legal requirements into product requirements and integrate the product requirements into their ongoing product design and testing processes. We report on an industry case study in which product requirements were specified to comply with Section 508 of the U.S. Rehabilitation Act Amendments of 1998. This study sought to identify: limitations in existing requirements-acquisition methods; compliance gaps between previously specified product requirements and Section 508; and additional sources of knowledge that are necessary to refine legal requirements into product requirements to comply with the law. Our study reveals the need for a community of practice and generalizable techniques that can reduce ambiguity, complexity and redundancy in legal and product requirements and manage innovation in product requirements. We present these findings with several examples from Section 508 regulations and actual product requirements that are implemented in Cisco products.}}
@ARTICLE{Johnston_2003,title={Induction of defeasible logic theories in the legal domain},year={2003},author={Benjamin Johnston and Benjamin Johnston and Guido Governatori and Guido Governatori},doi={10.1145/1047788.1047834},pmid={null},pmcid={null},mag_id={2145464309},journal={null},abstract={Defeasible Logic is a promising representation for legal knowledge that appears to overcome many of the deficiencies of previous approaches to representing legal knowledge. Unfortunately, an immediate application of technology to the challenges of generating theories in the legal domain is an expensive and computationally intractable problem. So, in light of the potential benefits, we seek to find a practical algorithm that uses heuristics to discover an approximate solution. As an outcome of this work, we have developed an algorithm that integrates defeasible logic into a decision support system by automatically deriving its knowledge from databases of precedents. Experiments with the new algorithm are very promising -- delivering results comparable to and exceeding other approaches.}}
@ARTICLE{Atkinson_2005,title={Legal case-based reasoning as practical reasoning},year={2005},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-006-9003-3},pmid={null},pmcid={null},mag_id={2145918162},journal={Artificial Intelligence and Law},abstract={In this paper we apply a general account of practical reasoning to arguing about legal cases. In particular, we provide a reconstruction of the reasoning of the majority and dissenting opinions for a particular well-known case from property law. This is done through the use of Belief-Desire-Intention (BDI) agents to replicate the contrasting views involved in the actual decision. This reconstruction suggests that the reasoning involved can be separated into three distinct levels: factual and normative levels and a level connecting the two, with conclusions at one level forming premises at the next. We begin by summarising our general approach, which uses instantiations of an argumentation scheme to provide presumptive justifications for actions, and critical questions to identify arguments which attack these justifications. These arguments and attacks are organised into argumentation frameworks to identify the status of individual arguments. We then discuss the levels of reasoning that occur in this reconstruction and the properties and significance of each of these levels. We illustrate the different levels with short examples and also include a discussion of the role of precedents within these levels of reasoning.}}
@ARTICLE{Greenwood_2003,title={Towards a computational account of persuasion in law},year={2003},author={Katie Greenwood and Katie Greenwood and Trevor Bench‐Capon and Trevor J. M. Bench Capon and Peter McBurney and Peter McBurney},doi={10.1145/1047788.1047792},pmid={null},pmcid={null},mag_id={2145957733},journal={null},abstract={In this paper we attempt to give an account of reasoning with legal cases contextualised within a general theory of persuasion in practical reasoning. We begin by presenting our general theory, concentrating on the variety of ways in which a particular position can be attacked. We then apply our theory to the legal domain, illustrating our approach by a case study based on the well known CATO system. From this we conclude that it is possible to see reasoning with legal cases as a particular instantiation of our general theory. We identify some points of interest for discussion, and conclude by stating our intended directions for future work.}}
@ARTICLE{Siena_2009,title={A Meta-Model for Modelling Law-Compliant Requirements},year={2009},author={Alberto Siena and Alberto Siena and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi and John Mylopoulos and John Mylopoulos},doi={10.1109/relaw.2009.1},pmid={null},pmcid={null},mag_id={2146077699},journal={null},abstract={While new laws and regulations address organisations, with their processes and information systems, the problem of defining suitable methods and techniques to support the design of law-compliant systems is getting increasing attention. We proposed a novel requirements engineering framework that includes a systematic process to derive law-compliant system requirements taking into account laws and strategic goals of stakeholders of a given domain. In this paper, we focus on the conceptual meta-model this framework rests on, defining it and discussing its use.}}
@ARTICLE{Biagioli_1987,title={Esplex: A rule and conceptual model for representing statutes},year={1987},author={Carlo Biagioli and Carlo Biagioli and C. Biagioli and Philip Mariani and Paola Mariani and P. Mariani and Daniela Tiscornia and D. Tiscornia and Daniela Tiscornia},doi={10.1145/41735.41762},pmid={null},pmcid={null},mag_id={2159228603},journal={null},abstract={The characteristics of the ESPLEX system which may be defined as a “rule and conceptual based model” are illustrated, together with the possibilities for its utilization, its similarities with other existing projects, and the requisites of the knowledge representation language.  The methodology and the theoretical propositions that have led to the definition of the representation language are therefore explained. The characteristics of the system which manages the knowledge base are also described and a brief comment is made regarding future development.}}
@ARTICLE{Prakken_2015,title={A formalization of argumentation schemes for legal case-based reasoning in ASPIC+},year={2015},author={Henry Prakken and Henry Prakken and Adam Wyner and Adam Wyner and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson},doi={10.1093/logcom/ext010},pmid={null},pmcid={null},mag_id={2167543484},journal={Journal of Logic and Computation},abstract={In this article we offer a formal account of reasoning with legal cases in terms of argumentation schemes. These schemes, and undercutting attacks associated with them, are formalized as defeasible rules of inference within the ASPIC+ framework. We begin by modelling the style of reasoning with cases developed by Aleven and Ashley in the CATO project, which describes cases using factors, and then extend the account to accommodate the dimensions used in Rissland and Ashley's earlier HYPO project. Some additional scope for argumentation is then identified and formalized.}}
@ARTICLE{Hohfeld_null,title={Some Fundamental Legal Conceptions as Applied in Judicial Reasoning},year={null},author={Wesley Newcomb Hohfeld and Wesley Newcomb Hohfeld},doi={10.2307/785533},pmid={null},pmcid={null},mag_id={4250371488},journal={Yale Law Journal},abstract={null}}
@ARTICLE{Petersen_2008,title={Systematic mapping studies in software engineering},year={2008},author={Kai Petersen and Kai Petersen and Robert Feldt and Robert Feldt and Shahid Mujtaba and Shahid Mujtaba and Michael Mattsson and Michael Mattsson},doi={10.14236/ewic/ease2008.8},pmid={null},pmcid={null},mag_id={4214443},journal={null},abstract={BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions.

OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.

METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.

RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined.

CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).}}
@ARTICLE{Rissland_1983,title={Examples in legal reasoning: legal hypotheticals},year={1983},author={Edwina L. Rissland and Edwina L. Rissland},doi={null},pmid={null},pmcid={null},mag_id={24106767},journal={null},abstract={This paper discusses examples, particularly hypotheticals, their use and generation, in legal reasoning. It examines the use of sequences of hypotheticals.}}
@ARTICLE{Brewka_2010,title={Carneades and Abstract Dialectical Frameworks: A Reconstruction},year={2010},author={Gerhard Brewka and Gerhard Brewka and Thomas F. Gordon and Thomas F. Gordon},doi={null},pmid={null},pmcid={null},mag_id={106283100},journal={null},abstract={Carneades is a rather general framework for argumentation. Unlike many other approaches, Carneades captures a number of aspects, like proof burdens, proof standards etc., which are of central importance, in particular in legal argumentation.

In this paper we show how Carneades argument evaluation structures can be reconstructed as abstract dialectical frameworks (ADFs), a recently proposed generalization of Dung argumentation frameworks (AFs). This not only provides at least an indirect link between Carneades and AFs, it also allows us to handle arbitrary argument cycles, thus lifting a restriction of Carneades. At the same time it provides strong evidence for the usefulness of ADFs as analytical/semantical tools in argumentation.}}
@ARTICLE{Bex_2011,title={Arguments, Stories and Criminal Evidence: A Formal Hybrid Theory},year={2011},author={Bex and Floris Bex and Bex and Floris Bex},doi={null},pmid={null},pmcid={null},mag_id={586152901},journal={null},abstract={Preface.- Chapter 1. Introduction.- Chapter 2. Reasoning with criminal evidence.- Chapter 3. Two approaches to reasoning with evidence: arguments and stories.- Chapter 4. A hybrid theory of stories and arguments.- Chapter 5. A formal logical hybrid theory of argumentation and explanation.- Chapter 6. Case study: Murder in Anjum.- Chapter 7. Related research on reasoning with criminal evidence.- Chapter 8. Conclusions. - References.- Index.}}
@ARTICLE{Rigoni_2015,title={An improved factor based approach to precedential constraint},year={2015},author={Adam Rigoni and Adam Rigoni},doi={10.1007/s10506-015-9166-x},pmid={null},pmcid={null},mag_id={832967940},journal={Artificial Intelligence and Law},abstract={In this article I argue for rule-based, non-monotonic theories of common law judicial reasoning and improve upon one such theory offered by Horty and Bench-Capon. The improvements reveal some of the interconnections between formal theories of judicial reasoning and traditional issues within jurisprudence regarding the notions of the ratio decidendi and obiter dicta. Though I do not purport to resolve the long-standing jurisprudential issues here, it is beneficial for theorists both of legal philosophy and formalizing legal reasoning to see where the two projects interact.}}
@ARTICLE{Hafner_2002,title={The role of context in case-based legal reasoning: teleological, temporal, and procedural},year={2002},author={Carole D. Hafner and Carole D. Hafner and Donald H. Berman and Donald H. Berman},doi={10.1023/a:1019516031847},pmid={null},pmcid={null},mag_id={1486267044},journal={Artificial Intelligence and Law},abstract={Computational models of relevance in case-based legal reasoning have traditionallybeen based on algorithms for comparing the facts and substantive legal issues of aprior case to those of a new case. In this paper we argue that robust models ofcase-based legal reasoning must also consider the broader social and jurisprudentialcontext in which legal precedents are decided. We analyze three aspects of legalcontext: the teleological relations that connect legal precedents to the socialvalues and policies they serve, the temporal relations between prior andsubsequent cases in a legal domain, and the procedural posture of legal cases,which defines the scope of their precedential relevance. Using real examples drawnfrom appellate courts of New York and Massachusetts, we show with the courts' ownarguments that the doctrine of stare decisis (i.e., similar facts should lead to similar results) is subject to contextual constraints and influences. For each of the three aspects of legal context, we outline an expanded computational framework for case-based legal reasoning that encompasses the reasoning of the examples, and provides a foundation for generating a more robust set of legal arguments.}}
@ARTICLE{Bench‐Capon_1999,title={Specification and Implementation of Toulmin Dialogue Game},year={1999},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={1492436717},journal={null},abstract={In this paper I describe the specification and implementation of a dialogue game based on the argument schema of Toulmin. It is argued that this schema is particularly suited to legal dialogues, and the need for the additional expressiveness provided by a game based on it, rather than logic alone, is illustrated by a sample dialogue. A full specification is given and an implementation, allowing the game to be played by human players, is described. Issues relating to the design of a computer program to play the game are discussed, and some concluding remarks are made.}}
@ARTICLE{Basili_1992,title={Software modeling and measurement: the Goal/Question/Metric paradigm},year={1992},author={Victor R. Basili and Victor R. Basili},doi={null},pmid={null},pmcid={null},mag_id={1494946506},journal={null},abstract={null}}
@ARTICLE{Siena_2009,title={Designing Law-Compliant Software Requirements},year={2009},author={Alberto Siena and Alberto Siena and John Mylopoulos and John Mylopoulos and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi},doi={10.1007/978-3-642-04840-1_35},pmid={null},pmcid={null},mag_id={1536084207},journal={null},abstract={New laws, such as HIPAA and SOX, are increasingly impacting the design of software systems, as business organisations strive to comply. This paper studies the problem of generating a set of requirements for a new system which comply with a given law. Specifically, the paper proposes a systematic process for generating law-compliant requirements by using a taxonomy of legal concepts and a set of primitives to describe stakeholders and their strategic goals. Given a model of law and a model of stakeholders goals, legal alternatives are identified and explored. Strategic goals that can realise legal prescriptions are systematically analysed, and alternative ways of fulfilling a law are evaluated. The approach is demonstrated by means of a case study. This work is part of the Nomos framework, intended to support the design of law-compliant requirements models.}}
@ARTICLE{Olson_2008,title={Advanced Data Mining Techniques},year={2008},author={David L. Olson and Dursun Delen},doi={null},pmid={null},pmcid={null},mag_id={1565050238},journal={null},abstract={This book covers the fundamental concepts of data mining, to demonstrate the potential of gathering large sets of data, and analyzing these data sets to gain useful business understanding. The book is organized in three parts. Part I introduces concepts. Part II describes and demonstrates basic data mining algorithms. It also contains chapters on a number of different techniques often used in data mining. Part III focusses on business applications of data mining.Methods are presented with simple examples, applications are reviewed, and relativ advantages are evaluated.}}
@ARTICLE{Sartor_2002,title={Teleological arguments and theory-based dialectics},year={2002},author={Giovanni Sartor and Giovanni Sartor},doi={10.1023/a:1019589831118},pmid={null},pmcid={null},mag_id={1566598272},journal={Artificial Intelligence and Law},abstract={This paper proposes to model legal reasoning asdialectical theory-constructiondirected by teleology. Precedents are viewed asevidence to be explained throughtheories. So, given a background of factors andvalues, the parties in a case canbuild their theories by using a set of operators,which are called theory constructors.The objective of each party is to provide theoriesthat both explain the evidence (theprecedents) and support the decision wished by thatparty. This leads to theory-basedargumentation, i.e., a dialectical exchange ofcompeting theories, which support opposedoutcomes by explaining the same evidence and appealingto the same values. The winneris the party that can reply with a more coherent theoryto all theories of its adversary.}}
@ARTICLE{Wyner_2007,title={Argument Schemes for Legal Case-based Reasoning},year={2007},author={Adam Wyner and Adam Wyner and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={1647597836},journal={null},abstract={In this paper we use the notion of argument schemes to analyse some leading approaches to case-based reasoning in Law. We identify a set of argument schemes that can express the argument provided by such systems and draw attention to some important differences between the various approaches.}}
@ARTICLE{Kiyavitskaya_2008,title={Automating the Extraction of Rights and Obligations for Regulatory Compliance},year={2008},author={Nadzeya Kiyavitskaya and Nadzeya Kiyavitskaya and Nicola Zeni and Nicola Zeni and Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Annie I. Antón and James R. Cordy and James R. Cordy and Luisa Mich and Luisa Mich and John Mylopoulos and John Mylopoulos},doi={10.1007/978-3-540-87877-3_13},pmid={null},pmcid={null},mag_id={1825943944},journal={null},abstract={Government regulations are increasingly affecting the security, privacy and governance of information systems in the United States, Europe and elsewhere. Consequently, companies and software developers are required to ensure that their software systems comply with relevant regulations, either through design or re-engineering. We previously proposed a methodology for extracting stakeholder requirements, called rights and obligations, from regulations. In this paper, we examine the challenges to developing tool support for this methodology using the Cerno framework for textual semantic annotation. We present the results from two empirical evaluations of a tool called "Gaius T." that is implemented using the Cerno framework and that extracts a conceptual model from regulatory texts. The evaluation, carried out on the U.S. HIPAA Privacy Rule and the Italian accessibility law, measures the quality of the produced models and the tool's effectiveness in reducing the human effort to derive requirements from regulations.}}
@ARTICLE{Prakken_2005,title={A study of accrual of arguments, with applications to evidential reasoning},year={2005},author={Henry Prakken and Henry Prakken},doi={10.1145/1165485.1165500},pmid={null},pmcid={null},mag_id={1968567610},journal={null},abstract={This paper presents a logical formalisation of accrual of arguments as a form of inference. The formalisation is given within the logical framework of Dung as instantiated by Pollock, and is shown to satisfy three principles that any treatment of accrual should satisfy. The formalisation of accrual as inference is contrasted to knowledge-representation treatments of accrual. Also, the formalisation is applied to some concepts from the theory of evidential legal reasoning.}}
@ARTICLE{Zhang_2007,title={Semantics-based legal citation network},year={2007},author={Paul Zhang and Paul Zhang and Lavanya Koppaka and Lavanya Koppaka},doi={10.1145/1276318.1276342},pmid={null},pmcid={null},mag_id={1968817294},journal={null},abstract={We describe and discuss the use of semantics-based citation networks in a new legal research tool. Such networks are generated based on citation relations between cases found in legal corpora as well as legal issues being discussed with these citations. Unlike traditional tools, the System allows legal professionals to efficiently study legal issues without having to go through whole cases or tedious manual citation search. This shift of focus from cases to individual issues within cases would greatly reduce time required for attorneys and legal scholars who have specific research problems in mind.   The Systems User Interface (UI) allows users to easily navigate in the citation networks and study how citations are interrelated and how legal issues have evolved in the past. Various forms of natural language processing (NLP) technologies are used in building the metadata behind the prototype. Formal evaluation confirmed the Systems capability of accurately identifying citations relevant to given legal issues.}}
@ARTICLE{Bench‐Capon_2011,title={Argument schemes for two-phase democratic deliberation},year={2011},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Henry Prakken and Henry Prakken and Wietske Visser and Wietske Visser},doi={10.1145/2018358.2018361},pmid={null},pmcid={null},mag_id={1969776745},journal={null},abstract={A formal two-phase model of democratic policy deliberation is presented, in which in the first phase sufficient and necessary criteria for proposals to be accepted are determined (the `acceptable' criteria) and in the second phase proposals are made and evaluated in light of the acceptable criteria resulting from the first phase. Such a separation gives the discussion a clear structure and prevents time and resources from being wasted on evaluating arguments for proposals based on unacceptable criteria. Argument schemes for both phases are defined and formalised in a logical framework for structured argumentation. The process of deliberation is abstracted from and it is assumed that both deliberation phases result in a set of arguments and attack and defeat relations between them. The acceptability status of criteria and proposals within the resulting argumentation framework is then evaluated using preferred semantics. For cases where preferences are required to choose between proposals, inference rules for deriving preferences between sets from an ordering of their elements are given.}}
@ARTICLE{Maxwell_2012,title={A legal cross-references taxonomy for reasoning about compliance requirements},year={2012},author={Jeremy C. Maxwell and Jeremy C. Maxwell and Annie I. Antón and Annie I. Antón and Peter Swire and Peter Swire and Muhammad Mohsin Riaz and Maria Riaz and Christopher M. McCraw and Christopher M. McCraw},doi={10.1007/s00766-012-0152-5},pmid={null},pmcid={null},mag_id={1969861018},journal={Requirements Engineering},abstract={Companies must ensure their software complies with relevant laws and regulations to avoid the risk of costly penalties, lost reputation, and brand damage resulting from non-compliance. Laws and regulations contain internal cross-references to portions of the same legal text, as well as cross-references to external legal texts. These cross-references introduce ambiguities, exceptions, as well as other challenges to regulatory compliance. Requirements engineers need guidance as to how to address cross-references in order to comply with the requirements of the law. Herein, we analyze each external cross-reference within the U.S. Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule, the Gramm–Leach–Bliley Act (GLBA), and the GLBA Financial Privacy Rule to determine whether a cross-reference either introduces a conflicting requirement, a conflicting definition, or refines an existing requirement. Herein, we propose a legal cross-reference taxonomy to aid requirements engineers in classifying cross-references as they specify compliance requirements. Analyzing cross-references enables us to address conflicting requirements that may otherwise thwart legal compliance. We identify five sets of conflicting compliance requirements and recommend strategies for resolving these conflicts.}}
@ARTICLE{Maxwell_2012,title={Managing changing compliance requirements by predicting regulatory evolution},year={2012},author={Jeremy C. Maxwell and Jeremy C. Maxwell and Annie I. Antón and Annie I. Antón and Peter Swire and Peter Swire},doi={10.1109/re.2012.6345793},pmid={null},pmcid={null},mag_id={1974488143},journal={null},abstract={Over time, laws change to meet evolving social needs. Requirements engineers that develop software for regulated domains, such as healthcare or finance, must adapt their software as laws change to maintain legal compliance. In the United States, regulatory agencies will almost always release a proposed regulation, or rule, and accept comments from the public. The agency then considers these comments when drafting a final rule that will be binding on the regulated domain. Herein, we examine how these proposed rules evolve into final rules, and propose an Adaptability Framework. This framework can aid software engineers in predicting what areas of a proposed rule are most likely to evolve, allowing engineers to begin building towards the more stable sections of the rule. We develop the framework through a formative study using the Health Insurance Portability and Accountability (HIPAA) Security Rule and apply it in a summative study on the Health Information Technology: Initial Set of Standards, Implementation Specifications, and Certification Criteria for Electronic Health Record Technology.}}
@ARTICLE{Bex_2003,title={Towards a formal account of reasoning about evidence: argumentation schemes and generalisations},year={2003},author={Floris Bex and Floris Bex and Henry Prakken and Henry Prakken and Chris Reed and Chris Reed and Douglas Walton and Douglas Walton},doi={10.1023/b:arti.0000046007.11806.9a},pmid={null},pmcid={null},mag_id={1982329760},journal={Artificial Intelligence and Law},abstract={This paper studies the modelling of legal reasoning about evidence within general theories of defeasible reasoning and argumentation. In particular, Wigmore's method for charting evidence and its use by modern legal evidence scholars is studied in order to give a formal underpinning in terms of logics for defeasible argumentation. Two notions turn out to be crucial, viz. argumentation schemes and empirical generalisations.}}
@ARTICLE{Sherman_1987,title={A Prolog model of the income tax act of Canada},year={1987},author={David M. Sherman and D. M. Sherman},doi={10.1145/41735.41750},pmid={null},pmcid={null},mag_id={1984954092},journal={null},abstract={The use of computers in Canadian tax planning has until now been concentrated on numerical analysis. The computer is indeed an excellent tool for calculating tax effects where the legal results of transactions are known. However, I maintain that it can be equally useful as a tool for analysing transactions to determine what those legal results are. The complexities of the Canadian tax system are such that a given transaction could have unintended tax results, where the facts resulting from the transaction fit the words of a rule which may have been designed for, and is perceived as applying to, different circumstances altogether.  In this paper I consider the structure and form of the Income Tax Act and show how it is well-designed for computerization. I then review the primary design considerations for a computer-based tax planning system. Finally, I describe the implementation of a partial tax analysis system which I have programmed in Prolog, and review its deficiencies and the extensions which would be required to make it a useful planning tool for tax practitioners.  NOTE: this paper is a greatly condensed version of a Master of Laws (LL.M.) thesis, “Blueprint for a Computer-Based Model of the Income Tax Act of Canada”, York University, September 1986. Important detail, references and examples have been omitted due to lack of space. Interested readers are invited to contact the author for a copy of the thesis.}}
@ARTICLE{Atkinson_2012,title={Introduction to special issue on modelling Popov v. Hayashi},year={2012},author={Katie Atkinson and Katie Atkinson},doi={10.1007/s10506-012-9122-y},pmid={null},pmcid={null},mag_id={1985723339},journal={Artificial Intelligence and Law},abstract={null}}
@ARTICLE{Jureta_2013,title={Toward benchmarks to assess advancement in legal requirements modeling},year={2013},author={Ivan Jureta and Ivan Jureta and Travis D. Breaux and Travis D. Breaux and Alberto Siena and Alberto Siena and David G. Gordon and David G. Gordon},doi={10.1109/relaw.2013.6671343},pmid={null},pmcid={null},mag_id={1992097055},journal={null},abstract={As software engineers create and evolve information systems to support business practices, these engineers need to address constraints imposed by laws, regulations and policies that govern those business practices. Requirements modeling can be used to extract important legal constraints from laws, and decide how, and evaluate if an information system design complies to applicable laws. To advance research on evaluating requirements modeling formalisms for the representation of legal information, we propose several benchmarks that we believe represent important challenges in modeling laws and requirements governing information systems, and evaluating the compliance of these requirements with laws. While incomplete, the proposed set of benchmarks covers a range of challenges in modeling laws and requirements that we observed in privacy and security law: from the possibility to trace model fragments to law fragments, to the ability to distinguish modalities in law, and to model relations between requirements and law fragments, needed when evaluating compliance. Benchmarks can be used as a checklist when designing and discussing requirements formalisms that support legal requirements modeling. Each benchmark is motivated by related work, a brief legal excerpt, and our experience in modeling regulations.}}
@ARTICLE{Loui_1995,title={Rationales and argument moves},year={1995},author={Ronald P. Loui and Ronald P. Loui and Jeff Norman and Jeff Norman},doi={10.1007/bf00872529},pmid={null},pmcid={null},mag_id={2003805585},journal={Artificial Intelligence and Law},abstract={We discuss five kinds of representations of rationales and provide a formal account of how they can alter disputation. The formal model of disputation is derived from recent work in argument. The five kinds of rationales are compilation rationales, which can be represented without assuming domain-knowledge (such as utilities) beyond that normally required for argument. The principal thesis is that such rationales can be analyzed in a framework of argument not too different from what AI already has. The result is a formal understanding of rationales, a partial taxonomy, and a foundation for computer programs that represent and reason with rationales.

The five kinds of rationales are as follows: (c)ompression and (s)pecialization, which yield rules, and (d)isputation, which yields a decision. These are modeled as potentially changing the focus of the dispute. Then there are (f)it, a rationale for rules, and (r)esolution, a rationale for decisions. These cannot be modeled as simply; they force disputation to a meta-level, at least temporarily.

The paper first discusses each kind of rationale in the abstract. Then it produces a model of dispute in which the simpler rationales can be analyzed. Formal examples are given. The model is augmented to allow analysis of the more difficult rationales. Examples are again given. The discussion is not intended to be strictly mathematical; rather, it aims to use formal methods to illuminate and provide framework for future interpretation and implementation.}}
@ARTICLE{Bench‐Capon_2012,title={A history of AI and Law in 50 papers: 25 years of the international conference on AI and Law},year={2012},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Michał Araszkiewicz and Michał Araszkiewicz and Kevin D. Ashley and Kevin D. Ashley and Katie Atkinson and Katie Atkinson and Floris Bex and Floris Bex and Filipe Borges and Filipe Borges and Danièle Bourcier and Danièle Bourcier and Danièle Bourcier and Paul Bourgine and Paul Bourgine and Paul Bourgine and Jack G. Conrad and Jack G. Conrad and Enrico Francesconi and Enrico Francesconi and Thomas F. Gordon and Thomas F. Gordon and Guido Governatori and Guido Governatori and Jochen L. Leidner and Jochen L. Leidner and David Lewis and David D. Lewis and David Lewis and David D. Lewis and Ronald P. Loui and Ronald P. Loui and L. Thorne McCarty and L. Thorne McCarty and Henry Prakken and Henry Prakken and Frank Schilder and Frank Schilder and Erich Schweighofer and Erich Schweighofer and Paul Thompson and Paul Thompson and Alex Tyrrell and Alex Tyrrell and Bart Verheij and Bart Verheij and Douglas Walton and Douglas Walton and Adam Wyner and Adam Wyner},doi={10.1007/s10506-012-9131-x},pmid={null},pmcid={null},mag_id={2005949547},journal={Artificial Intelligence and Law},abstract={We provide a retrospective of 25 years of the International Conference on AI and Law, which was first held in 1987. Fifty papers have been selected from the thirteen conferences and each of them is described in a short subsection individually written by one of the 24 authors. These subsections attempt to place the paper discussed in the context of the development of AI and Law, while often offering some personal reactions and reflections. As a whole, the subsections build into a history of the last quarter century of the field, and provide some insights into where it has come from, where it is now, and where it might go.}}
@ARTICLE{Berman_1991,title={Developer's choice in the legal domain: the Sisyphean journey with DBR or down hill with rules (a working paper for the case-rules panel at the third international conference of artificial intelligence and law)},year={1991},author={Donald H. Berman and Donald H. Berman},doi={10.1145/112646.112685},pmid={null},pmcid={null},mag_id={2006435516},journal={null},abstract={Article Free Access Share on Developer's choice in the legal domain: the Sisyphean journey with DBR or down hill with rules (a working paper for the case-rules panel at the third international conference of artificial intelligence and law) Author: Donald H. Berman Northeastern University School of Law, Co-Director, Center for Law and Computer Science Northeastern University School of Law, Co-Director, Center for Law and Computer ScienceView Profile Authors Info & Claims ICAIL '91: Proceedings of the 3rd international conference on Artificial intelligence and lawMay 1991 Pages 307–309https://doi.org/10.1145/112646.112685Online:01 May 1991Publication History 10citation440DownloadsMetricsTotal Citations10Total Downloads440Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Prakken_2010,title={An abstract framework for argumentation with structured arguments},year={2010},author={Henry Prakken and Henry Prakken},doi={10.1080/19462160903564592},pmid={null},pmcid={null},mag_id={2006559750},journal={Argument & Computation},abstract={An abstract framework for structured arguments is presented, which instantiates Dung's (‘On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming, and n-Person Games’, Artificial Intelligence, 77, 321–357) abstract argumentation frameworks. Arguments are defined as inference trees formed by applying two kinds of inference rules: strict and defeasible rules. This naturally leads to three ways of attacking an argument: attacking a premise, attacking a conclusion and attacking an inference. To resolve such attacks, preferences may be used, which leads to three corresponding kinds of defeat: undermining, rebutting and undercutting defeats. The nature of the inference rules, the structure of the logical language on which they operate and the origin of the preferences are, apart from some basic assumptions, left unspecified. The resulting framework integrates work of Pollock, Vreeswijk and others on the structure of arguments and the nature of defeat and extends it...}}
@ARTICLE{Lutomski_1989,title={The design of an attorney's statistical consultant},year={1989},author={L. S. Lutomski and L. S. Lutomski},doi={10.1145/74014.74043},pmid={null},pmcid={null},mag_id={2018543100},journal={null},abstract={null}}
@ARTICLE{Massey_2014,title={Identifying and classifying ambiguity for regulatory requirements},year={2014},author={Aaron K. Massey and Aaron K. Massey and Richard L. Rutledge and Richard L. Rutledge and Annie I. Antón and Annie I. Antón and Peter Swire and Peter Swire},doi={10.1109/re.2014.6912250},pmid={null},pmcid={null},mag_id={2021064897},journal={null},abstract={Software engineers build software systems in increasingly regulated environments, and must therefore ensure that software requirements accurately represent obligations described in laws and regulations. Prior research has shown that graduate-level software engineering students are not able to reliably determine whether software requirements meet or exceed their legal obligations and that professional software engineers are unable to accurately classify cross-references in legal texts. However, no research has determined whether software engineers are able to identify and classify important ambiguities in laws and regulations. Ambiguities in legal texts can make the difference between requirements compliance and non-compliance. Herein, we develop a ambiguity taxonomy based on software engineering, legal, and linguistic understandings of ambiguity. We examine how 17 technologists and policy analysts in a graduate-level course use this taxonomy to identify ambiguity in a legal text. We also examine the types of ambiguities they found and whether they believe those ambiguities should prevent software engineers from implementing software that complies with the legal text. Our research suggests that ambiguity is prevalent in legal texts. In 50 minutes of examination, participants in our case study identified on average 33.47 ambiguities in 104 lines of legal text using our ambiguity taxonomy as a guideline. Our analysis suggests (a) that participants used the taxonomy as intended: as a guide and (b) that the taxonomy provides adequate coverage (97.5%) of the ambiguities found in the legal text.}}
@ARTICLE{Hage_1993,title={Hard cases: a procedural approach},year={1993},author={Jaap Hage and Jaap Hage and Ronald Leenes and Ronald Leenes and A.R. Lodder and Arno R. Lodder},doi={10.1007/bf00871759},pmid={null},pmcid={null},mag_id={2022257388},journal={Artificial Intelligence and Law},abstract={Much work on legal knowledge systems treats legal reasoning as arguments that lead from a description of the law and the facts of a case, to the legal conclusion for the case. The reasoning steps of the inference engine parallel the logical steps by means of which the legal conclusion is derived from the factual and legal premises. In short, the relation between the input and the output of a legal inference engine is a logical one. The truth of the conclusion only depends on the premises, and is independent of the argument that leads to the conclusion.

This paper opposes the logical approach, and defends a procedural approach to legal reasoning. Legal conclusions are not true or false independent of the reasoning process that ended in these conclusions. In critical cases this reasoning process consists of an adversarial procedure in which several parties are involved. The course of the argument determines whether the conclusion is true or false. The phenomenon of hard cases is used to demonstrate this essential procedural nature of legal reasoning.

Dialogical Reason Based Logic offers a framework that makes it possible to model legal dialogues. We use Dialogical Reason Based Logic to specify hard cases in dialogical terms. Moreover, we analyse an actual Dutch hard case in terms of Dialogical Reason Based Logic, to demonstrate both the possibilities and the shortcomings of this approach.

It turns out that there is no one set of rational dialogue rules. There are many concurring sets of rules that govern particular types of dialogues. The rules for legal procedures are as much part of the law as the more substantial rules. As a consequence, it is not possible to offer an universal set of dialogue rules. Dialogical Reason Based Logic rather provides a framework which can be filled with dialogue rules that determine which dialogues are valid and which ones are invalid.}}
@ARTICLE{Zeni_2015,title={GaiusT: supporting the extraction of rights and obligations for regulatory compliance},year={2015},author={Nicola Zeni and Nicola Zeni and Nadzeya Kiyavitskaya and Nadzeya Kiyavitskaya and Luisa Mich and Luisa Mich and James R. Cordy and James R. Cordy and John Mylopoulos and John Mylopoulos},doi={10.1007/s00766-013-0181-8},pmid={null},pmcid={null},mag_id={2030573703},journal={Requirements Engineering},abstract={Ensuring compliance of software systems with government regulations, policies, and laws is a complex problem. Generally speaking, solutions to the problem first identify rights and obligations defined in the law and then treat these as requirements for the system under design. This work examines the challenge of developing tool support for extracting such requirements from legal documents. To address this challenge, we have developed a tool called GaiusT. The tool is founded on a framework for textual semantic annotation. It semiautomatically generates elements of requirements models, including actors, rights, and obligations. We present the complexities of annotating prescriptive text, the architecture of GaiusT, and the process by which annotation is accomplished. We also present experimental results from two case studies to illustrate the application of the tool and its effectiveness relative to manual efforts. The first case study is based on the US Health Insurance Portability and Accountability Act, while the second analyzes the Italian accessibility law for information technology instruments.}}
@ARTICLE{McCarty_1995,title={An implementation of Eisner v. Macomber},year={1995},author={L. Thorne McCarty and L. Thorne McCarty},doi={10.1145/222092.222258},pmid={null},pmcid={null},mag_id={2033787843},journal={null},abstract={Eisner v. Macomber , 252 U.S. 189 (1920), a corporate tax case, was the principal illustration of a theory of legal reasoning and legal argumentation proposed more than ten years ago. Although the theory was described in some detail, using the vocabulary of prototypes and deformations, it was never fully implemented. There were two main problems: (1) the knowledge represen- tation languages available at the time were not suf- ciently expressive, and (2) as a result, the central concept of a prototype was never suciently formal- ized. These problems have been remedied by subse- quent work, and the present paper describes an im- plementation (in PROLOG) of the original theory. A study of the implemented system provides a rational reconstruction of the arguments of Justice Pitney and Justice Brandeis in this seminal corporate tax case.}}
@ARTICLE{Aleven_1995,title={Doing things with factors},year={1995},author={Vincent Aleven and Vincent Aleven and Vincent Aleven and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/222092.222106},pmid={null},pmcid={null},mag_id={2040000954},journal={null},abstract={We conducted an experiment to investigate whether a human tutor could employ the CATO model and instructional program to teach legal research and argumentation skills to beginning law stu&nts. The CATO model covem arguments comparing and contrasting casesin terms of factors, abstractionsof facts that tend to strengthen or weaken a party’s position on a legal claim. At the time of the experimen~ the CATO program comprised tools and resources that help apply the CATO model to specific problems, most importantly, a case database and tools for retrieving, displaying, and comparing casesin terms of factors. We compmed human-led itulruction with CATO against more traditional classroom instruction designed to teach the same skills, without the use of the CATO model or tools. The subjects were 17 fmtsemester students from the University of Pittsburgh Law School. We found that human-guided instruction with CATO was as good as classroom instruction, We also found that answers generated by the CATO program were scored higher than the students’ answers, suggestingthat the model can potentially be employed even more effectively to teach students. Examples drawn from protocols of CATO sessionsih.trate that students can use the CATO model to guide and facilitate the construction of arguments and often go beyond the model’s limitations, at least under the guidance of a human tutor.}}
@ARTICLE{Reed_2004,title={ARAUCARIA: SOFTWARE FOR ARGUMENT ANALYSIS, DIAGRAMMING AND REPRESENTATION},year={2004},author={Chris Reed and Chris Reed and Glenn Rowe and Glenn Rowe},doi={10.1142/s0218213004001922},pmid={null},pmcid={null},mag_id={2041139729},journal={International Journal on Artificial Intelligence Tools},abstract={Argumentation theory involves the analysis of naturally occurring argument, and one key tool employed to this end both in the academic community and in teaching critical thinking skills to undergraduates is argument diagramming. By identifying the structure of an argument in terms of its constituents and the relationships between them, it becomes easier to critically evaluate each part of an argument in turn. The task of analysis and diagramming, however, is labor intensive and often idiosyncratic, which can make academic exchange difficult. The Araucaria system provides an interface which supports the diagramming process, and then saves the result using AML, an open standard, designed in XML, for describing argument structure. Araucaria aims to be of use not only in pedagogical situations, but also in support of research activity. As a result, it has been designed from the outset to handle more advanced argumentation theoretic concepts such as schemes, which capture stereotypical patterns of reasoning. The software is also designed to be compatible with a number of applications under development, including dialogic interaction and online corpus provision. Together, these features, combined with its platform independence and ease of use, have the potential to make Araucaria a valuable resource for the academic community.}}
@ARTICLE{Gordon_2012,title={A Carneades reconstruction of Popov v Hayashi},year={2012},author={Thomas F. Gordon and Thomas F. Gordon and Douglas Walton and Douglas Walton},doi={10.1007/s10506-012-9120-0},pmid={null},pmcid={null},mag_id={2046836002},journal={Artificial Intelligence and Law},abstract={Carneades is an open source argument mapping application and a programming library for building argumentation support tools. In this paper, Carneades' support for argument reconstruction, evaluation and visualization is illustrated by modeling most of the factual and legal arguments in Popov v Hayashi.}}
@ARTICLE{Ashley_2009,title={Automatically classifying case texts and predicting outcomes},year={2009},author={Kevin D. Ashley and Kevin D. Ashley and Stefanie Brüninghaus and Stefanie Brüninghaus},doi={10.1007/s10506-009-9077-9},pmid={null},pmcid={null},mag_id={2050223480},journal={Artificial Intelligence and Law},abstract={Work on a computer program called SMILE + IBP (SMart Index Learner Plus Issue-Based Prediction) bridges case-based reasoning and extracting information from texts. The program addresses a technologically challenging task that is also very relevant from a legal viewpoint: to extract information from textual descriptions of the facts of decided cases and apply that information to predict the outcomes of new cases. The program attempts to automatically classify textual descriptions of the facts of legal problems in terms of Factors, a set of classification concepts that capture stereotypical fact patterns that effect the strength of a legal claim, here trade secret misappropriation. Using these classifications, the program can evaluate and explain predictions about a problem's outcome given a database of previously classified cases. This paper provides an extended example illustrating both functions, prediction by IBP and text classification by SMILE, and reports empirical evaluations of each. While IBP's results are quite strong, and SMILE's much weaker, SMILE + IBP still has some success predicting and explaining the outcomes of case scenarios input as texts. It marks the first time to our knowledge that a program can reason automatically about legal case texts.}}
@ARTICLE{Berman_1995,title={Understanding precedents in a temporal context of evolving legal doctrine},year={1995},author={Donald H. Berman and Donald H. Berman and Carole D. Hafner and Carole D. Hafner},doi={10.1145/222092.222116},pmid={null},pmcid={null},mag_id={2051784609},journal={null},abstract={null}}
@ARTICLE{Bench‐Capon_1987,title={Support for policy makers: formulating legislation with the aid of logical models},year={1987},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/41735.41756},pmid={null},pmcid={null},mag_id={2053600678},journal={null},abstract={null}}
@ARTICLE{Bench‐Capon_2000,title={A method for the computational modelling of dialectical argument with dialogue games},year={2000},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and T. Geldard and T. Geldard and Paul Leng and P. H. Leng},doi={10.1023/a:1008308627745},pmid={null},pmcid={null},mag_id={2055647644},journal={Artificial Intelligence and Law},abstract={In this paper we describe a method for the specification of computationalmodels of argument using dialogue games. The method, which consists ofsupplying a set of semantic definitions for the performatives making upthe game, together with a state transition diagram, is described in full.Its use is illustrated by some examples of varying complexity, includingtwo complete specifications of particular dialogue games, Mackenzie's DC,and the authors' own TDG. The latter is also illustrated by a fully workedexample illustrating all the features of the game.}}
@ARTICLE{Gijzel_2012,title={Relating Carneades with abstract argumentation via the ASPIC + framework for structured argumentation},year={2012},author={Bas van Gijzel and Bas van Gijzel and Henry Prakken and Henry Prakken},doi={10.1080/19462166.2012.661766},pmid={null},pmcid={null},mag_id={2056630576},journal={Argument & Computation},abstract={Carneades is a recently proposed formalism for structured argumentation with varying proof standards, inspired by legal reasoning, but more generally applicable. Its distinctive feature is that each statement can be given its own proof standard, which is claimed to allow a more natural account of reasoning under burden of proof than existing formalisms for structured argumentation, in which proof standards are defined globally. In this article, the two formalisms are formally related by translating Carneades into the ASPIC+ framework for structured argumentation. Since ASPIC+ is defined to generate Dung-style abstract argumentation frameworks, this in effect translates Carneades graphs into abstract argumentation frameworks. For this translation, we prove a formal correspondence and show that certain rationality postulates hold. It is furthermore proved that Carneades always induces a unique Dung extension, which is the same in all of Dung's semantics, allowing us to generalise Carneades to cycle-containi...}}
@ARTICLE{Farley_1995,title={Burden of proof in legal argumentation},year={1995},author={Arthur M. Farley and Arthur M. Farley and Kathleen P. Freeman and Kathleen Freeman and Kathleen Freeman},doi={10.1145/222092.222227},pmid={null},pmcid={null},mag_id={2058767821},journal={null},abstract={We present a computational model of dialectical argumentation that could serve as a basis for studying elements of legal reasoning. Argumentation is well-suited to decisionmaking in the legal domain, where knowledge is incomplete, uncertain, and inconsistent, We model an argument both as information structure, i.e., argument units connecting claims with supporting data, and as dialectical process, i.e., an alternating series of moves made by opposing sides. Inspired by the legal domain, our model includes burden of proof as a key element, indicating the level of support that must be achieved by a particular side to an argument. Burden of proof acts as a move filter and termination criterion during argumentation and determines the eventual winner. We demonstrate our model by considering two examples that have been discussed previously in the artificial intelligence and legal reasoning literature. INTRODUCTION As the artificial intelligence (AI) and legal reasoning communities are well aware, most decisions are reached against a background of incomplete, uncertain, and inconsistent knowledge (i.e., weak theory domains; Porter, et. al., 1990). The most widely used AI methods for reasoning under uncertainty either rely on an absence of outright contradictions (e. g., probabilistic reasoning; Pearl, 1987) or are unable to support motivated decision-making in the face of inconsistent information (e.g., default reasoning; Ginsberg, 1987). Both solutions put the problem of deciding what to believe outside their respective domains of discourse. Choosing the proposition with highest Permission to copywithoutfee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appea, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republisb, requires a fee andlor specific permission.}}
@ARTICLE{Biagioli_2005,title={Automatic semantics extraction in law documents},year={2005},author={Carlo Biagioli and C. Biagioli and Carlo Biagioli and C. Biagioli and Enrico Francesconi and Enrico Francesconi and Andrea Passerini and Andrea Passerini and Simonetta Montemagni and Simonetta Montemagni and Claudia Soria and Claudia Soria},doi={10.1145/1165485.1165506},pmid={null},pmcid={null},mag_id={2063112000},journal={null},abstract={Normative texts can be viewed as composed by formal partitions (articles, paragraphs, etc.) or by semantic units containing fragments of a regulation (provisions). Provisions can be described according to a metadata scheme which consists of provision types and their arguments. This semantic annotation of a normative text can make the retrieval of norms easier. The detection and description of the provisions according to the established metadata scheme is an analytic intellectual activity aiming at classifying portions of a normative text into provision types and to extract their arguments. Automatic facilities supporting this intellectual activity are desirable. Particularly, in this paper, two modules able to qualify fragments of a normative text in terms of provision types and to extract their arguments are presented.}}
@ARTICLE{Bench‐Capon_2010,title={Using argument schemes for hypothetical reasoning in law},year={2010},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Henry Prakken and Henry Prakken},doi={10.1007/s10506-010-9094-8},pmid={null},pmcid={null},mag_id={2063764015},journal={Artificial Intelligence and Law},abstract={This paper studies the use of hypothetical and value-based reasoning in US Supreme-Court cases concerning the United States Fourth Amendment. Drawing upon formal AI & Law models of legal argument a semi-formal reconstruction is given of parts of the Carney case, which has been studied previously in AI & law research on case-based reasoning. As part of the reconstruction, a semiformal proposal is made for extending the formal AI & Law models with forms of metalevel reasoning in several argument schemes. The result is compared with Rissland's (1989) analysis in terms of dimensions and Ashley's (2008) analysis in terms of his process model of legal argument with hypotheticals.}}
@ARTICLE{Ashley_2009,title={Ontological requirements for analogical, teleological, and hypothetical legal reasoning},year={2009},author={Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/1568234.1568236},pmid={null},pmcid={null},mag_id={2069321861},journal={null},abstract={In 1993, Berman and Hafner criticized case-based models of legal reasoning for not modeling analogical and teleological elements. Another lesson learned since then is the role of ontologies in representing domain knowledge so that a legal reasoning system can represent and solve problems. If the reasoning involves drawing abstract analogies, reasoning teleologically about rules for deciding a case, and posing hypothetical cases to test decision rules, however, it is not clear what requirements the ontology should satisfy. This paper presents an extended example of such legal reasoning to illustrate what an ontology for case-based legal reasoning should provide. The example centers on a microworld of legal discourse, an ensemble of real legal cases, hypothetical examples, concepts, factors, principles and policies. Beginning with any case in the microworld, the system's goal is to generate arguments that a law professor and students might reasonably make in discussing the legal case in class. The example illustrates three roles the ontology should play in providing representational support for the system, distills the ontological requirements, and suggests an incremental approach to making good on Berman's and Hafner's challenge.}}
@ARTICLE{Ghanavati_2014,title={Goal-oriented compliance with multiple regulations},year={2014},author={Sepideh Ghanavati and Sepideh Ghanavati and André Rifaut and André Rifaut and Éric Dubois and Eric Dubois and Daniel Amyot and Daniel Amyot},doi={10.1109/re.2014.6912249},pmid={null},pmcid={null},mag_id={2081396812},journal={null},abstract={Most systems and business processes in organizations need to comply with more than one law or regulation. Different regulations can partially overlap (e.g., one can be more detailed than the other) or even conflict with each other. In addition, one regulation can permit an action whereas the same action in another regulation might be mandatory or forbidden. In each of these cases, an organization needs to take different strategies. This paper presents an approach to handle different situations when comparing and attempting to comply with multiple regulations as part of a goal-oriented modeling framework named LEGAL-URN. This framework helps organizations find suitable trade-offs and priorities when complying with multiple regulations while at the same time trying to meet their own business objectives. The approach is illustrated with a case study involving a Canadian health care organization that must comply with four laws related to privacy, quality of care, freedom of information, and care consent.}}
@ARTICLE{Lodder_1995,title={DiaLaw: a dialogical framework for modeling legal reasoning},year={1995},author={A.R. Lodder and Arno R. Lodder and Aimée Herczog and Aimée Herczog},doi={10.1145/222092.222225},pmid={null},pmcid={null},mag_id={2082016936},journal={null},abstract={Article Free Access Share on DiaLaw: a dialogical framework for modeling legal reasoning Authors: Arno R. Lodder University of Limburg, Faculty of law, P.O. Box 616; 6200 MD Maastricht; The Netherlands University of Limburg, Faculty of law, P.O. Box 616; 6200 MD Maastricht; The NetherlandsView Profile , Aimée Herczog University of Nijmegen, Faculty of Mathematics, P.O. Box 9010, 6500 GL Nijmegen, The Netherlands University of Nijmegen, Faculty of Mathematics, P.O. Box 9010, 6500 GL Nijmegen, The NetherlandsView Profile Authors Info & Claims ICAIL '95: Proceedings of the 5th international conference on Artificial intelligence and lawMay 1995 Pages 146–155https://doi.org/10.1145/222092.222225Published:24 May 1995Publication History 13citation271DownloadsMetricsTotal Citations13Total Downloads271Last 12 Months25Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Horty_2011,title={Reasons and precedent},year={2011},author={John F. Horty and John F. Horty},doi={10.1145/2018358.2018363},pmid={null},pmcid={null},mag_id={2091136898},journal={null},abstract={This paper describes a precise reason model of precedent, based on the general idea that a later court is constrained to reach a decision that is consistent with an earlier court's assessment of the balance of reasons. The account draws on recent work in legal theory as well as concepts developed within the field of artificial intelligence and law.}}
@ARTICLE{Amyot_2010,title={Evaluating goal models within the goal-oriented requirement language},year={2010},author={Daniel Amyot and Daniel Amyot and Sepideh Ghanavati and Sepideh Ghanavati and Jennifer Horkoff and Jennifer Horkoff and Gunter Mussbacher and Gunter Mussbacher and Liam Peyton and Liam Peyton and Liam Peyton and Eric Yu and Eric Yu},doi={10.1002/int.v25:8},pmid={null},pmcid={null},mag_id={2102069200},journal={International Journal of Intelligent Systems},abstract={In this article, we introduce the application of rigorous analysis procedures to goal models to provide several benefits beyond the initial act of modeling. Such analysis can allow modelers to assess the satisfaction of goals, facilitate evaluation of high-level design alternatives, help analysts decide on the high-level requirements and design of the system, test the sanity of a model, and support communication and learning. The analysis of goal models can be done in very different ways depending on the nature of the model and the purpose of the analysis. In our work, we use the Goal-oriented Requirement Language (GRL), which is part of the User Requirements Notation (URN). URN, a new Recommendation of the International Telecommunications Union, provides the first standard goal-oriented language. Using GRL, we develop an approach to analysis that can be done by evaluating qualitative or quantitative satisfaction levels of the actors and intentional elements (e.g., goals and tasks) composing the model. Initial satisfaction levels for some of the intentional elements are provided in a strategy and then propagated to the other intentional elements of the model through the various links that connect them. The results allow for an assessment of the relative effectiveness of design alternatives at the requirements level. Although no specific propagation algorithm is imposed in the URN standard, different criteria for defining evaluation mechanisms are described. We provide three algorithms (quantitative, qualitative, and hybrid) as examples, which satisfy the constraints imposed by the standard. These algorithms have been implemented in the open-source jUCMNav tool, an Eclipse-based editor for URN models. The algorithms are presented and compared with the help of a telecommunication system example. © 2010 Wiley Periodicals, Inc.}}
@ARTICLE{Branting_1991,title={Reasoning with portions of precedents},year={1991},author={L. Karl Branting and L. Karl Branting},doi={10.1145/112646.112664},pmid={null},pmcid={null},mag_id={2102224868},journal={null},abstract={null}}
@ARTICLE{Breaux_2009,title={Exercising Due Diligence in Legal Requirements Acquisition: A Tool-supported, Frame-Based Approach},year={2009},author={Travis D. Breaux and Travis D. Breaux},doi={10.1109/re.2009.46},pmid={null},pmcid={null},mag_id={2103705380},journal={null},abstract={Government laws and organizational policies introduce critical legal requirements that govern information systems. Unlike traditional requirements elicited from stakeholders, legal requirements have unique characteristics that software engineers must address to ensure that their systems are demonstrably compliant with relevant laws and policies. This paper presents important terminology for developing legally compliant software systems and a methodology consisting of procedures and models for acquiring, representing and analyzing phenomena in legal documents, which constitute rich sources of legal requirements. Based on a grounded theory, the method has been validated through a mixed-methods approach consisting of multiple, descriptive case studies. This paper presents a human subject experiment that tests a fundamental part of the theory to understand the efficacy of multiple users applying the method to a sample regulation text.}}
@ARTICLE{Bench‐Capon_2002,title={The missing link revisited: The role of teleology in representing legal argument},year={2002},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1023/a:1019501830692},pmid={null},pmcid={null},mag_id={2104802723},journal={Artificial Intelligence and Law},abstract={In this paper I recapitulate the ideas of Berman and Hafner (1993) regarding the role of teleology in legal argument. I show how these ideas can be used to address some issues arising from more recent work on legal argument, and how this relates to ideas associated with the "New Rhetoric" of Perelman. I illustrate the points with a discussion of the classic problem of which vehicles should be allowed in parks.}}
@ARTICLE{Siena_2008,title={From Laws to Requirements},year={2008},author={Alberto Siena and Alberto Siena and John Mylopoulos and John Mylopoulos and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi},doi={10.1109/relaw.2008.6},pmid={null},pmcid={null},mag_id={2108469728},journal={null},abstract={Legal prescriptions are increasingly impacting on information systems and on organisations that must comply with them in order to avoid to be prosecuted or fined. Addressing law compliance in early phases of the requirements analysis helps in improving the alignment of information systems with the law. In this paper, we point out ontological differences between legal concepts and requirements and set the basis for a systematic process able to support decision making about requirements for law compliant systems.}}
@ARTICLE{Chorley_2005,title={An empirical investigation of reasoning with legal cases through theory construction and application},year={2005},author={Alison Chorley and Alison Chorley and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-006-9016-y},pmid={null},pmcid={null},mag_id={2117781069},journal={Artificial Intelligence and Law},abstract={In recent years several proposals to view reasoning with legal cases as theory construction have been advanced. The most detailed of these is that of Bench-Capon and Sartor, which uses facts, rules, values and preferences to build a theory designed to explain the decisions in a set of cases. In this paper we describe CATE (CAse Theory Editor), a tool intended to support the construction of theories as described by Bench-Capon and Sartor, and which produces executable code corresponding to a theory. CATE has been used in a series of experiments intended to explore a number of issues relating to such theories, including how the theories should be constructed, how sets of values should be compared, and the representation of cases using structured values as opposed to factors.}}
@ARTICLE{Atkinson_2011,title={Semantic models for policy deliberation},year={2011},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and D. Cartwright and Dan Cartwright and Adam Wyner and Adam Wyner},doi={10.1145/2018358.2018369},pmid={null},pmcid={null},mag_id={2117858142},journal={null},abstract={Semantic models have received little attention in recent years, much of their role having been taken over by developments in ontologies. Ontologies, however, are static, and so have only a limited role in reasoning about domains in which change matters. In this paper, we focus on the domain of policy deliberation, where policy decisions are designed to change things to realise particular social values. We explore how a particular kind of state transition system can be constructed to serve as a semantic model to support reasoning about alternative policy decisions. The policy making process includes stages that support the construction of a model, which can then be exploited in reasoning. The reasoning itself will be driven by a particular argumentation scheme for practical reasoning, and the ways in which arguments based on this scheme can be attacked and evaluated. The evaluation provides alternative policy positions. The semantics underpin a current web-based implementation, designed to solicit structured feedback on policy proposals.}}
@ARTICLE{Kharbili_2011,title={CoReL: Policy-Based and Model-Driven Regulatory Compliance Management},year={2011},author={Marwane El Kharbili and Marwane El Kharbili and Qin Ma and Qin Ma and Pierre Kelsen and Pierre Kelsen and Elke Pulvermueller and Elke Pulvermueller},doi={10.1109/edoc.2011.23},pmid={null},pmcid={null},mag_id={2122372361},journal={null},abstract={Regulatory compliance management is now widely recognized as one of the main challenges still to be efficiently dealt with in information systems. In the discipline of business process management in particular, compliance is considered as an important driver of the efficiency, reliability and market value of companies. It consists of ensuring that enterprise systems behave according to some guidance provided in the form of regulations. This paper gives a definition of the research problem of regulatory compliance. We show why we expect a formal policy-based and model-driven approach to provide significant advantages in allowing enterprises to flexibly manage decision-making related to regulatory compliance. For this purpose, we contribute CoReL, a domain-specific modeling language for representing compliance requirements that has a graphical concrete syntax. Informal semantics of CoReL are introduced and its use is illustrated on an example. CoReL allows to leverage business process compliance modeling and checking, enhancing it with regard to, among other dimensions, user-friendliness, genericity, and traceability.}}
@ARTICLE{Ishikawa_2009,title={Modeling, Analyzing and Weaving Legal Interpretations in Goal-Oriented Requirements Engineering},year={2009},author={Fuyuki Ishikawa and Fuyuki Ishikawa and Rihoko Inoue and Rihoko Inoue and Shinichi Honiden and Shinichi Honiden},doi={10.1109/relaw.2009.8},pmid={null},pmcid={null},mag_id={2123058306},journal={null},abstract={As many laws are concerned with information systems, each organization needs to carefully examine a law and determine technical requirements as well as its operational policies with sufficient concreteness. Legal interpretations essentially affect decisions made in refining goals, or making goals more concrete, that are directly or indirectly established by the law. As common and conventional interpretations are gradually identified and become widespread through appearances of cases and guidelines, goal refinement relationships need to be constantly managed and adjusted to the latest legal interpretations. This paper discusses an initial approach to management of legal interpretations together with goal-oriented requirements engineering, through a motivating example of a Japanese law on trade secrets. Specifically, this study proposes a metamodel for legal interpretation. In the metamodel, legal interpretations are modeled as refinement relationships between definitions of concepts or terms and then analyzed by matching with instances. Legal interpretations can then be woven into goal trees as criteria of goal refinement.}}
@ARTICLE{Gordon_2013,title={A cross-domain empirical study and legal evaluation of the requirements water marking method},year={2013},author={David G. Gordon and David G. Gordon and Travis D. Breaux and Travis D. Breaux},doi={10.1007/s00766-013-0167-6},pmid={null},pmcid={null},mag_id={2123513092},journal={Requirements Engineering},abstract={Companies that own, license, or maintain personal information face a daunting number of privacy and security regulations. Companies are subject to new regulations from one or more governing bodies, when companies introduce new or existing products into a jurisdiction, when regulations change, or when data are transferred across political borders. To address this problem, we developed a framework called "requirements water marking" that business analysts can use to align and reconcile requirements from multiple jurisdictions (municipalities, provinces, nations) to produce a single high or low standard of care. We evaluate the framework in two empirical case studies covering a subset of U.S. data breach notification laws and medical record retention laws. In these studies, applying our framework reduced the number of requirements a company must comply with by 76 % across 8 jurisdictions and 15 % across 4 jurisdictions, respectively. We show how the framework surfaces critical requirements trade-offs and potential regulatory conflicts that companies must address during the reconciliation process. We summarize our results, including surveys of information technology law experts to contextualize our empirical results in legal practice.}}
@ARTICLE{Bench‐Capon_1995,title={PLAID: proactive legal assistance},year={1995},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Geof Staniford and Geof Staniford},doi={10.1145/222092.222142},pmid={null},pmcid={null},mag_id={2131052787},journal={null},abstract={Article PLAID: proactive legal assistance Share on Authors: T. J. M. Bench-Capon Department of Computer Science, The University of Liverpool, Liverpool, England Department of Computer Science, The University of Liverpool, Liverpool, EnglandView Profile , G. Staniford Department of Computer Science, The University of Liverpool, Liverpool, England Department of Computer Science, The University of Liverpool, Liverpool, EnglandView Profile Authors Info & Claims ICAIL '95: Proceedings of the 5th international conference on Artificial intelligence and lawMay 1995 Pages 81–88https://doi.org/10.1145/222092.222142Published:24 May 1995 15citation212DownloadsMetricsTotal Citations15Total Downloads212Last 12 Months2Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access}}
@ARTICLE{Prakken_2002,title={An exercise in formalising teleological case-based reasoning},year={2002},author={Henry Prakken and Henry Prakken},doi={10.1023/a:1019536206548},pmid={null},pmcid={null},mag_id={2131795851},journal={Artificial Intelligence and Law},abstract={This paper takes up Berman and Hafner's (1993) challenge to model legal case-based reasoning not just in terms of factual similarities and differences but also in terms of the values that are at stake. The formal framework of Prakken and Sartor (1998) is applied to examples of case-based reasoning involving values, and a method for formalising such examples is proposed. The method makes it possible to express that a case should be decided in a certain way because that advances certain values. The method also supports the comparison of conflicting precedents in terms of values, and it supports debates on the relevance of distinctions in terms of values.}}
@ARTICLE{Atkinson_2006,title={Computational Representation of Practical Argument},year={2006},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Peter McBurney and Peter McBurney},doi={10.1007/s11229-005-3488-2},pmid={null},pmcid={null},mag_id={2142001056},journal={Synthese},abstract={In this paper we consider persuasion in the context of practical reasoning, and discuss the problems associated with construing reasoning about actions in a manner similar to reasoning about beliefs. We propose a perspective on practical reasoning as presumptive justification of a course of action, along with critical questions of this justification, building on the account of Walton. From this perspective, we articulate an interaction protocol, which we call PARMA, for dialogues over proposed actions based on this theory. We outline an axiomatic semantics for the PARMA Protocol, and discuss two implementations which use this protocol to mediate a discussion between humans. We then show how our proposal can be made computational within the framework of agents based on the Belief-Desire-Intention model, and illustrate this proposal with an example debate within a multi agent system.}}
@ARTICLE{Wardeh_2009,title={PADUA: a protocol for argumentation dialogue using association rules},year={2009},author={Maya Wardeh and Maya Wardeh and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Frans Coenen and Frans Coenen},doi={10.1007/s10506-009-9078-8},pmid={null},pmcid={null},mag_id={2146591455},journal={Artificial Intelligence and Law},abstract={We describe PADUA, a protocol designed to support two agents debating a classification by offering arguments based on association rules mined from individual datasets. We motivate the style of argumentation supported by PADUA, and describe the protocol. We discuss the strategies and tactics that can be employed by agents participating in a PADUA dialogue. PADUA is applied to a typical problem in the classification of routine claims for a hypothetical welfare benefit. We particularly address the problems that arise from the extensive number of misclassified examples typically found in such domains, where the high error rate is a widely recognised problem. We give examples of the use of PADUA in this domain, and explore in particular the effect of intermediate predicates. We have also done a large scale evaluation designed to test the effectiveness of using PADUA to detect misclassified examples, and to provide a comparison with other classification systems.}}
@ARTICLE{Breaux_2005,title={Analyzing goal semantics for rights, permissions, and obligations},year={2005},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Annie I. Antón},doi={10.1109/re.2005.12},pmid={null},pmcid={null},mag_id={2146989366},journal={null},abstract={Software requirements, rights, permissions, obligations, and operations of policy enforcing systems are often misaligned. Our goal is to develop tools and techniques that help requirements engineers and policy makers bring policies and system requirements into better alignment. Goals from requirements engineering are useful for distilling natural language policy statements into structured descriptions of these interactions; however, they are limited in that they are not easy to compare with one another despite sharing common semantic features. In this paper, we describe a process called semantic parameterization that we use to derive semantic models from goals mined from privacy policy documents. We present example semantic models that enable comparing policy statements and present a template method for generating natural language policy statements (and ultimately requirements) from unique semantic models. The semantic models are described by a context-free grammar called KTL that has been validated within the context of the most frequently expressed goals in over 100 Internet privacy policy documents. KTL is supported by a policy analysis tool that supports queries and policy statement generation.}}
@ARTICLE{Herold_2003,title={The practical guide to HIPAA privacy and security compliance},year={2003},author={Rebecca Herold and Rebecca Herold and Kevin M. Beaver and Kevin Beaver},doi={null},pmid={null},pmcid={null},mag_id={2148066263},journal={null},abstract={HIPAA ESSENTIALS Introduction to HIPAA How HIPAA Came to Be What HIPAA Covers Organizations that Must Comply with HIPAA Compliance Deadlines HIPAA Penalties and Enforcement Insight into the Electronic Transactions and Code Sets Rule Summary Chapter 1: Practical Checklist Preparing for the HIPAA Changes Background Managing Change Creating the Mindset It's Up to You Chapter 2: Practical Checklist HIPAA Cost Considerations Background Privacy Implementation Costs Privacy Ongoing Maintenance Costs Costs Related to Providing Access to PHI Privacy Officer Costs Security Implementation Costs Security Ongoing Maintenance Costs Security Officer Costs Chapter 3: Practical Checklist The Relationship Between Security and Privacy Background Privacy Rule and Security Rule Overlaps Conclusion Chapter 4: Practical Checklist Section 1 Quiz HIPAA PRIVACY RULE HIPAA Privacy Rule Requirements Overview Background Uses and Disclosures Incidental Uses and Disclosures Minimum Necessary De-Identification Business Associates Marketing Notice of Privacy Practices for PHI Individual Rights to Request Privacy Protection for PHI Individual Access to PHI Amendment of PHI Accounting Disclosures of PHI PHI Restrictions Requests Administrative Requirements Personal Representatives Minors Transition Provisions Compliance Dates and Penalties Looking Forward Performing a Privacy Rule Gap Analysis and Risk Analysis Gap Analysis and Risk Analysis Chapter 6: Practical Checklist Writing Effective Privacy Policies Notice of Privacy Practices Example NPP Organizational Privacy Policies Chapter 7: Practical Checklist State Preemption What is Contrary? Preemption Criteria Exceptions to Preemption Preemption Analysis Conclusion Chapter 8: Practical Checklist Crafting a Privacy Implementation Plan Some Points to Keep in Mind Conclusion Chapter 9: Practical Checklist Privacy Rule Compliance Checklist HIPAA SECURITY RULE Security Rule Requirements Overview Introduction to the Security Rule What's New in the Final Security Rule General Rules for Security Rule Compliance Required versus Addressable Insight Into the Security Rule Other Organizational Requirements Reasons to Get Started on Security Rule Initiatives Chapter 11: Practical Checklist Performing a Security Rule Risk Analysis Background Risk Analysis Requirements According to HIPAA Risk Analysis Essentials Stepping Through the Process Calculating Risk Managing Risks Going Forward Chapter 12: Practical Checklist Writing Effective Information Security Policies Introduction to Security Policies Critical Elements of Security Policies Sample Security Policy Framework Security Policies You May Need for HIPAA Security Rule Compliance Managing Your Security Policies Chapter 13: Practical Checklist Crafting a Security Implementation Plan Background Some Points to Keep In Mind Conclusion Chapter 14: Practical Checklist Security Rule Compliance Checklist COVERED ENTITY ISSUES Healthcare Provider Issues Background Privacy Notices Fees for Record Review Mitigation Measures Fax Use Sign-In Sheets Patient Charts Business Associates Authorizations Chapter 16: Practical Checklist Healthcare Clearinghouse Issues Background Requirements Transactions Financial Institutions Conclusion Chapter 17: Practical Checklist Health Plan Issues What is a Health Plan? What is a Small Health Plan? Health Plan Requirements Marketing Issues Notice of Privacy Practices Types of Insurance Plans Excluded from HIPAA Communications Government and Law Enforcement Chapter 18: Practical Checklist Employer Issues Background "Small" and "Large" Employers Health Benefits Enforcement and Penalties Organizational Requirements Health Information Medical Surveillance Workers' Compensation Training Resources Conclusion Chapter 19: Practical Checklist Business Associate Issues Is Your Organization a Business Associate? Business Associate Requirements What You Can Expect to See or Hear from Covered Entities Issues to Consider Moving Forward Chapter 20: Practical Checklist HIPAA TECHNOLOGY CONSIDERATIONS Building a HIPAA Compliant Technology Infrastructure Overview Areas of Technology to Focus On Looking Deeper into Specific Technologies Mobile Computing Concerns Summary Chapter 21: Practical Checklist Crafting Security Incident Procedures and Contingency Plans Background Handling Security Incidents Security Incident Procedure Essentials Basics of Contingency Planning Moving Forward Chapter 22: Practical Checklist Outsourcing Information Technology Services Background Reasons to Consider Outsourcing What Functions to Outsource What to Look for in Outsourcing Firms Common Outsourcing Mistakes Chapter 23: Practical Checklist MANAGING ONGOING HIPAA COMPLIANCE HIPAA Training, Education, and Awareness Creating an Effective Awareness Program Identify Awareness and Training Groups Training Training Design and Development Awareness Options Document Training and Awareness Activities Get Support Measure Effectiveness Conclusion Chapter 24: Practical Checklist Performing Ongoing HIPAA Compliance Reviews and Audits Background Privacy Issues Security Issues Making Audits Work Chapter 25: Practical Checklist APPENDIXES HIPAA Case Studies: Experiences Shared Sample Documents HIPAA Resources Answers to Chapter Quizzes HIPAA Glossary}}
@ARTICLE{Gordon_2013,title={Assessing regulatory change through legal requirements coverage modeling},year={2013},author={David G. Gordon and David G. Gordon and Travis D. Breaux and Travis D. Breaux},doi={10.1109/re.2013.6636714},pmid={null},pmcid={null},mag_id={2157582701},journal={null},abstract={Developing global markets offer companies new opportunities to manufacture and sell information technology (IT) products in ways unforeseen by current laws and regulations. This innovation leads to changing requirements due to changes in product features, laws, or the locality where the product is sold or manufactured. To help developers rationalize these changes, we introduce a preliminary framework and method that can be used by requirements engineers and their legal teams to identify relevant legal requirements and trace changes in requirements coverage. The framework includes a method to translate IT regulations into a legal requirements coverage model used to make coverage assertions about existing or planned IT systems. We evaluated the framework in a case study using three IT laws: California's Confidentiality of Medical Records Act, the U.S. Health Information Portability and Accountability Act (HIPAA) and amendments from the Health Information Technology for Economic and Clinical Health (HITECH) Act, and the India 2011 Information Technology Rules. Further, we demonstrate the framework using three scenarios: new product features are proposed; product-related services are outsourced abroad; and regulations change to address changes in the market.}}
@ARTICLE{Otto_2007,title={The ChoicePoint Dilemma: How Data Brokers Should Handle the Privacy of Personal Information},year={2007},author={Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón and David L. Baumer and David L. Baumer},doi={10.1109/msp.2007.126},pmid={null},pmcid={null},mag_id={2158114612},journal={null},abstract={Before 2005, data broker ChoicePoint suffered fraudulent access to its databases that exposed thousands of customers' personal information. We examine Choice-Point's data breach, explore what went wrong from the perspective of consumers, executives, policy, and IT systems, and offer recommendations for the future.}}
@ARTICLE{Massey_2011,title={Assessing the accuracy of legal implementation readiness decisions},year={2011},author={Aaron K. Massey and Aaron K. Massey and Ben Smith and Ben Smith and Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1109/re.2011.6051661},pmid={null},pmcid={null},mag_id={2164880965},journal={null},abstract={Software engineers regularly build systems that are required to comply with laws and regulations. To this end, software engineers must determine which requirements have met or exceeded their legal obligations and which requirements have not. Requirements that have met or exceeded their legal obligations are legally implementation ready, whereas requirements that have not met or exceeded their legal obligations need further refinement. Research is needed to better understand how to support software engineers in making these determinations. In this paper, we describe a case study in which we asked graduate-level software engineering students to assess whether a set of software requirements for an electronic health record system met or exceeded their corresponding legal obligations as expressed in regulations created pursuant to the U.S. Health Insurance Portability and Accountability Act (HIPAA). We compare the assessment made by graduate students with an assessment made by HIPAA compliance subject matter experts. Additionally, we contrast these results with those generated by a legal requirements triage algorithm. Our findings suggest that the average graduate-level software engineering student is ill-prepared to write legally compliant software with any confidence and that domain experts are an absolute necessity. Our findings also indicate the potential utility of legal requirements metrics in aiding software engineers as they make legal compliance decisions.}}
@ARTICLE{Kitchenham_2004,title={Procedures for Performing Systematic Reviews},year={2004},author={Barbara Kitchenham and Barbara Kitchenham},doi={null},pmid={null},pmcid={null},mag_id={2168894761},journal={null},abstract={null}}
@ARTICLE{Lindahl_2008,title={Intermediaries and intervenients in normative systems},year={2008},author={Lars Lindahl and Lars Lindahl and Jan Odelstad and Jan Odelstad},doi={10.1016/j.jal.2007.06.010},pmid={null},pmcid={null},mag_id={2171753860},journal={Journal of Applied Logic},abstract={Many concepts in legal texts are “intermediaries”, in the sense that they serve as links between statements of legal grounds, on one hand, and of legal consequences, on the other. In our paper, an  ...}}
@ARTICLE{Massey_2010,title={Triage for legal requirements},year={2010},author={Aaron K. Massey and Aaron K. Massey and Annie I. Antón and Ana I. Anton},doi={null},pmid={null},pmcid={null},mag_id={2342151364},journal={null},abstract={The high cost of non-compliance with laws and regulations that govern software systems makes legal requirements prioritization crucial. In addition, software design, expense, and time constraints all inuence how requirements are prioritized. Prioritizing requirements derived from laws and regulations can be untenable using traditional pairwise requirements prioritization techniques because the number of pairwise comparisons grows quadratically with the number of requirements. In this paper, we introduce legal requirements triage{{a technique used to subdivide a requirements set into three subsets: (a) implementation ready legal requirements; (b) legal requirements that require further renement; and (c) non-legal requirements. Legal requirements triage supplements a traditional pairwise requirements prioritization by focusing analysts on implementation ready legal requirements to reduce the number of pairwise comparisons. Herein, we discuss a case study in which we applied these techniques to prioritize 75 functional requirements for the iTrust Medical Records System, an open-source electronic health records system that must comply with the U.S. Health Insurance Portability and Accountability Act (HIPAA). Our study shows that we were able to reduce signicantly the number of pairwise comparisons.}}
@ARTICLE{Bench‐Capon_2011,title={Relating Values in a Series of Supreme Court Decisions.},year={2011},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2396038514},journal={null},abstract={In recent years it has become quite usual to view legal decisions in terms of consideration of the values affected by deciding the case for or against a particular party. Often deciding for, say, the plaintiff will promote one value at the expense of another. Precedents are then supposed to guide the way in which this conflict is resolved. In this paper we will consider a series of cases exploring the so-called automobile exception to the requirement of the Fourth Amendment protecting against unreasonable search of persons, houses, papers, and effects. These cases highlight a conflict between the value of law enforcement and the value of privacy as protected by the Fourth Amendment, and will be used to illuminate questions about the treatment of value conflicts arising from previous work in AI and Law.}}
@ARTICLE{Atkinson_2013,title={Argumentation schemes for reasoning about factors with dimensions},year={2013},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Henry Prakken and Henry Prakken and Adam Wyner and Adam Wyner},doi={null},pmid={null},pmcid={null},mag_id={2406050175},journal={null},abstract={In previous work we presented argumentation schemes to capture the CATO and value based theory construction approaches to reasoning with legal cases with factors. We formalised the schemes with ASPIC+, a formal representation of instantiated argumentation. In ASPIC+ the premises of a scheme may either be a factor provided in a knowledge base or established using a further argumentation scheme. Thus far we have taken the factors associated with cases to be given in the knowledge base. While this is adequate for expressing factor based reasoning, we can further investigate the justifications for the relationship between factors and facts or evidence. In this paper we examine how dimensions as used in the HYPO system can provide grounds on which to argue about which factors should apply to a case. By making this element of the reasoning explicit and subject to argument, we advance our overall account of reasoning with legal cases and make it more robust.}}
@ARTICLE{李幼升_1989,title={ph},year={1989},author={李幼升 and F. G. J. Hayhoe},doi={null},pmid={null},pmcid={null},mag_id={3031066211},journal={null},abstract={null}}
@ARTICLE{Wyner_2007,title={Arguments, Values and Baseballs: Representation of Popov v. Hayashi},year={2007},author={Adam Wyner and Adam Wyner and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson},doi={null},pmid={null},pmcid={null},mag_id={7221777},journal={null},abstract={In this paper, we model a recent legal case as presented in a court of first instance using argument schemes and an argumentation framework, providing a formal analysis of the case and how the outcome was determined. The paper contributes to the body of literature that formally analyses legal cases in terms of arguments and argument schemes. It is novel in that we analyse a case in a court of first instance, so we have arguments about facts, qualifications of intermediate predicates, and the application of legal rules. We show the importance of undercutters in relating principles to the specific case.}}
@ARTICLE{Rissland_1989,title={Combining case-based and rule-based reasoning: a heuristic approach},year={1989},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak},doi={null},pmid={null},pmcid={null},mag_id={26655238},journal={null},abstract={In this paper we discuss a heuristically controlled approach to combining reasoning with cases and reasoning with rules. Our task is interpretation of under-defined terms that occur in legal statutes (like the Internal Revenue Code) where certain terms must be applied to particular cases even though their meanings are not defined by the statute and the statutory rules are unclear as to scope and meaning. We describe this problem, known as statutory interpretation, provide examples of it, describe the need for melding case-based and rule-based reasoning, and discuss heuristics used in guiding reasoning on such problems. We conclude with a discussion of our on-going work to model this mode of expert reasoning.}}
@ARTICLE{McCarty_1977,title={Reflections on "Taxman": An Experiment in Artificial Intelligence and Legal Reasoning},year={1977},author={L. Thorne McCarty and L. Thorne McCarty},doi={10.2307/1340132},pmid={null},pmcid={null},mag_id={50182414},journal={Harvard Law Review},abstract={null}}
@ARTICLE{Siena_2010,title={Engineering Law-Compliant Requirements: the Nomos Framework},year={2010},author={Alberto Siena and Alberto Siena},doi={null},pmid={null},pmcid={null},mag_id={80044280},journal={null},abstract={In modern societies, both business and private life are deeply pervaded by software and information systems.
Using software has extended human capabilities, allowing information to cross physical and ethical barriers.
To handle misuse dangers, governments are increasingly laying down new laws and introducing obligations, rights and responsibilities concerned with the use of software.
As a consequence, laws are assuming a steering role in the specification of software requirements, which must be compliant to avoid fines and penalties.

This work proposes a model-based approach to the problem of law compliance of software requirements. It aims at extending state-of-the-art goal-oriented requirements engineering techniques with the capability to argue about compliance, through the use and analysis of models. It is based on a language for modelling legal prescriptions. Upon the language, compliance can be defined as a condition that depends on a set of properties. Such a condition is achieved through an iterative modelling process.

Specifically, we investigated the nature of legal prescription to capture their conceptual language. From jurisprudence literature, we adopted a taxonomy of legal concepts, which has been elaborated and translated into a conceptual meta-model. Moreover, this metamodel was integrated with the meta-model of a goal-oriented modelling language for requirements engineering, in order to provide a common legal-intentional meta-model.

Requirements models built with the proposed language consist of graphs, which ultimately can be verified automatically. Compliance amounts then in a set of properties the graph must have.

The compliance condition gains relevance in two cases. Firstly, when a requirements model has already been developed, and it needs to be reconciled with a set of laws. Secondly, when requirements have to be modelled from scratch, and they are need to be compliant. In both cases, compliance results from a design process.

The proposed modelling language, as well as the compliance condition and the corresponding design process, have been applied to two case studies.
The obtained results confirm the validity of the approach, and point out interesting research directions for the future.}}
@ARTICLE{Cleven_2009,title={Regulatory Compliance in Information Systems Research – Literature Analysis and Research Agenda},year={2009},author={Anne Cleven and Anne Cleven and Robert Winter and Robert Winter},doi={10.1007/978-3-642-01862-6_15},pmid={null},pmcid={null},mag_id={102455850},journal={null},abstract={After a period of little regulation, many companies are now facing a growing number and an increasing complexity of new laws, regulations, and standards. This has a huge impact on how organizations conduct their daily business and involves various changes in organizational and governance structures, software systems and data flows as well as corporate culture, organizational power and communication. We argue that the implementation of a holistic compliance cannot be divided into isolated projects, but instead requires a thorough analysis of relevant components as well as an integrated design of the very same. This paper examines the state-of-the-art of compliance research in the field of information systems (IS) by means of a comprehensive literature analysis. For the systemization of our results we apply a holistic framework for enterprise analysis and design. The framework allows us to both point out “focus areas” as well as “less travelled roads” and derive a future research agenda for compliance research.}}
@ARTICLE{Modgil_2010,title={Integrating Dialectical and Accrual Modes of Argumentation},year={2010},author={Sanjay Modgil and Sanjay Modgil and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={120174898},journal={null},abstract={This paper argues that accrual should be modelled in terms of reasoning about the application of preferences to sets of arguments, and shows how such reasoning can be formalised within metalevel argumentation frameworks. These frameworks adopt the same machinery and level of abstraction as Dung's argumentation framework. We thus provide a dialectical argumentation semantics that integrates accrual, and illustrate our approach by instantiating our framework with the arguments and attacks defined by an object level formalism that accommodates reasoning about priorities over sets of rules.}}
@ARTICLE{Smith_1987,title={The Application of Expert Systems Technology to Case-Based Law.},year={1987},author={J. C. Smith and J. C. Smith and Cal Deedman and Cal Deedman},doi={null},pmid={null},pmcid={null},mag_id={141049445},journal={null},abstract={null}}
@ARTICLE{Breaux_2013,title={Regulatory requirements traceability and analysis using semi-formal specifications},year={2013},author={Travis D. Breaux and Travis D. Breaux and David G. Gordon and David G. Gordon},doi={10.1007/978-3-642-37422-7_11},pmid={null},pmcid={null},mag_id={187147204},journal={null},abstract={Information systems are increasingly distributed and pervasive, enabling organizations to deliver remote services and share personal information, worldwide. However, developers face significant challenges in managing the many laws that govern their systems in this multi-jurisdictional environment. In this paper, we report on a computational requirements document expressible using a legal requirements specification language (LRSL). The purpose is to make legal requirements open and available to policy makers, business analysts and software developers, alike. We show how requirements engineers can codify policy and law using the LRSL and design, debug, analyze, trace, and visualize relationships among regulatory requirements. The LRSL provides new constructs for expressing distributed constraints, making regulatory specification patterns visually salient, and enabling metrics to quantitatively measure different styles for writing legal and policy documents. We discovered and validated the LRSL using thirteen U.S. state data breach notification laws.}}
@ARTICLE{Hamou-Lhadj_2007,title={Towards a compliance support framework for global software companies},year={2007},author={Abdelwahab Hamou-Lhadj and AbdelKrim Hamou-Lhadj and Abdelwahab Hamou-Lhadj},doi={null},pmid={null},pmcid={null},mag_id={202414668},journal={null},abstract={Regulated companies are required to comply with the laws and regulations that apply to their industries. An important aspect of these authoritative rules is directly related to the way by which software systems, used by the regulated companies, are built, tested, and maintained. As a result, many regulated companies have turned to their software vendors to request their support in the compliance efforts. For most global software vendors, this new situation represents a significant challenge. From the technological standpoint, the complexity and sheer volume of typical authoritative rules poses a serious obstacle to implementing effective compliance support strategies. From the organizational perspective, the delivery of compliance support activities requires efficient business processes, skilled and valued employees, and a strong governance model with commitment at all management levels. To address these issues, we present a compliance support framework that aims to facilitate the linkage between compliance requirements, software development practices, and business process management. We believe that, if implemented properly, this framework can significantly improve the way software companies handle the increasing customer demand for compliance support. It can turn compliance support into a revenue-generating activity, and possibly a competitive advantage.}}
@ARTICLE{Bench‐Capon_2015,title={Using Argumentation to Structure E-Participation in Policy Making},year={2015},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson and Adam Wyner and Adam Wyner},doi={10.1007/978-3-662-46485-4_1},pmid={null},pmcid={null},mag_id={341917907},journal={null},abstract={Tools for e-participation are becoming increasingly important. In this paper we argue that existing tools exhibit a number of limitations, and that these can be addressed by basing tools on developments in the field of computational argumentation. After discussing the limitations, we present an argumentation scheme which can be used to justify policy proposals, and a way of modelling the domain so that arguments using this scheme and attacks upon them can be automatically generated. We then present two prototype tools: one to present justifications and receive criticism, and the other to elicit justifications of user-proposed policies and critique them. We use a running example of a genuine policy debate to illustrate the various aspects.}}
@ARTICLE{Hage_2005,title={Studies in Legal Logic},year={2005},author={Jaap Hage and Jaap Hage},doi={null},pmid={null},pmcid={null},mag_id={561993517},journal={null},abstract={Law and Defeasibility.- Law and Coherence.- Reason-Based Logic.- Comparing Alternatives.- Rule Consistency.- What is a Norm?.- Legal Statics and Legal Dynamics.- Dialectical Models in Artificial Intelligence and Law.- Legal Reasoning and Legal Integration.}}
@ARTICLE{Niblett_1980,title={Computer Science and Law},year={1980},author={Brian Niblett and Brian Niblett},doi={null},pmid={null},pmcid={null},mag_id={636778590},journal={null},abstract={null}}
@ARTICLE{Al-Abdulkarim_2013,title={From Oral Hearing to Opinion in the U.S. Supreme Court},year={2013},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={791393294},journal={null},abstract={This paper provide a structured analysis of US Supreme Court Oral Hearings to enable identification of the relevant issues, factors and facts that can be used to construct a test to resolve a case. Our analysis involves the production of what we term ‘argument component trees’ (ACTs) in which the issues, facts and factors, and the relationship between these, are made explicit. We show how such ACTs can be constructed by identifying the speech acts that are used by the counsel and Justices within their dialogue. We illustrate the application of our analysis by applying it to the oral hearing for the case of Carney v. California, and we relate the majority and minority opinions delivered in that case to our ACTs. The aim of the work is to provide a formal framework that addresses a particular aspect of case-based reasoning: enabling the identification and representation of the components that are used to form a test to resolve a case and guide future behaviour.}}
@ARTICLE{Eskridge_1990,title={Statutory Interpretation as Practical Reasoning},year={1990},author={William N. Eskridge and William N. Eskridge and Philip J. Frickey and Philip J. Frickey},doi={10.2307/1228963},pmid={null},pmcid={null},mag_id={1482125362},journal={Stanford Law Review},abstract={In the last decade, statutory interpretation has reemerged as an important topic of academic theory and discussion.' This development is welcome, since few topics are more relevant to legal craft and education than the interpretation of statutes, now our primary source of law. The recent theoretical views, however, contrast with practicing lawyers' strategies of statutory interpretation. When practitioners give advice to clients about what a statute means, their approach is usually eclectic: They look at the text of the relevant statutory provisions, any legislative history that is available, the context in which the legislation was enacted, the overall legal landscape, and the lessons of common sense and good policy. But when law professors talk about statutory interpretation, they tend to posit a more abstract, "grand" theory that privileges one or another of these approaches as "foundational."2 The commentators' grand theories contrast with the more ad hoc, factbased reasoning of the practicing lawyer. How do judges interpret statutes? How should they? Many commentators argue that judicial interpretation is, or at least ought to be, inspired by grand theory. We think these commentators are wrong, both descriptively and normatively: Judges' approaches to statutory interpretation are generally eclectic, not inspired by any grand theory,}}
@ARTICLE{Perelman_1971,title={The New Rhetoric: A Treatise on Argumentation},year={1971},author={Chaïm Perelman and Chaïm Perelman and Lucie Olbrechts-Tyteca and Lucie Olbrechts-Tyteca and John Wilkinson and John Wilkinson and Purcell Weaver and Purcell Weaver},doi={null},pmid={null},pmcid={null},mag_id={1488505771},journal={null},abstract={null}}
@ARTICLE{Ghanavati_2007,title={A Requirements Management Framework for Privacy Compliance.},year={2007},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton and Liam Peyton},doi={null},pmid={null},pmcid={null},mag_id={1490050818},journal={null},abstract={Compliance with privacy legislation is a primary concern for health care institutions that are building information systems support for their business processes. This paper describes a requirements management framework that enables health information custodians (HIC) to document and track compliance with privacy legislation. A metamodel is defined for our framework to define compliance tracking links between separate User Requirements Notation models of the HIC and privacy legislation. Using examples from a case study at a major teaching hospital, we show how this framework can be used to manage change and ensure compliance when privacy legislation is amended or the business processes evolved.}}
@ARTICLE{Freeman_1993,title={Toward formalizing dialectical argumentation},year={1993},author={Kathleen Freeman and Kathleen Freeman},doi={null},pmid={null},pmcid={null},mag_id={1506028345},journal={null},abstract={The construction of arguments has long been viewed as a paradigmatic example of human reasoning and, as such, is an important ability for computer programs that attempt to model intelligent behavior. We explore the use of argumentation for deriving and justifying claims in domains where knowledge is incomplete, uncertain, or inconsistent, i.e., weak theory domains. Argumentation supports a notion of proof appropriate for reasoning in weak theory domains, e.g., a claim is proved if there is plausible, irrefutable support for the claim, and there is no such support for any counter-claim.
We present elements of a theory of argumentation involving two senses of argument, argument as supporting explanation and argument as dialectical process. For argument as supporting explanation, we create argument structures that organize relevant, available support for both a claim and its negation. In dialectical argument, the format of a two-sided argument process is used to intertwine the strengths and weaknesses of support for competing claims, so arguments can be refuted and directly compared. Our account of dialectical argumentation includes a catalog of argument moves and a set of heuristics for selecting moves and thereby controlling argument generation.
This model, which has been implemented in a computer program, is a flexible environment for exploring the representation and generation of arguments. We show how the program generates reasonable arguments for a set of example problems. We give an analysis of the program, including limits of the current model of argumentation.
For artificial intelligence programs, the ability to generate arguments provides a useful technique for reasoning in real world contexts. For argumentation researchers, artificial intelligence methodology offers a new way for evaluating theories of argumentation.}}
@ARTICLE{Glaser_1967,title={The Discovery of Grounded Theory},year={1967},author={Barney G. Glaser and Anselm L. Strauss and Barney G. Glaser},doi={null},pmid={null},pmcid={null},mag_id={1515587369},journal={null},abstract={The discovery of grounded theory , The discovery of grounded theory , کتابخانه مرکزی دانشگاه علوم پزشکی تهران}}
@ARTICLE{Vreeswijk_2000,title={Credulous and Sceptical Argument Games for Preferred Semantics},year={2000},author={Gerard A. W. Vreeswijk and Gerard A. W. Vreeswijk and Henry Prakken and Henry Prakken},doi={10.1007/3-540-40006-0_17},pmid={null},pmcid={null},mag_id={1521433304},journal={Lecture Notes in Computer Science},abstract={This paper presents dialectical proof theories for Dung's preferred semantics of defeasible argumentation. The proof theories have the form of argument games for testing membership of some (credulous reasoning) or all preferred extensions (sceptical reasoning). The credulous proof theory is for the general case, while the sceptical version is for the case where preferred semantics coincides with stable semantics. The development of these argument games is especially motivated by applications of argumentation in automated negotiation, mediation of collective discussion and decision making, and intelligent tutoring.}}
@ARTICLE{Arnold_1996,title={Software Change Impact Analysis},year={1996},author={Robert S. Arnold and Robert S. Arnold},doi={null},pmid={null},pmcid={null},mag_id={1548254758},journal={null},abstract={From the Publisher:
As software systems become increasingly large and complex, the need increases to predict and control the effects of software changes. Software Change Impact Analysis captures the latest information on the science and art of determining what software parts affect each other. It provides a battery of ideas for doing impact analysis better, presents a framework for the field, and focuses attention on important results. You will gain a healthy respect for the strengths and limitations of impact analysis technology and a solid background that will prove valuable for years to come. The book identifies key impact analysis definitions and themes and illustrates the important themes to give you a solid understanding for tackling impact analysis problems. It includes reports on software source code dependency analysis and software traceability analysis and shows how results from both areas can more effectively support impact analysis in software engineering repositories. It also describes why impact representation and determination techniques are at the heart of both source dependency analysis and traceability analysis.}}
@ARTICLE{Sadiq_2007,title={Modeling control objectives for business process compliance},year={2007},author={Shazia Sadiq and Shazia Sadiq and Guido Governatori and Guido Governatori and Kioumars Namiri and Kioumars Namiri},doi={10.1007/978-3-540-75183-0_12},pmid={null},pmcid={null},mag_id={1556540894},journal={null},abstract={Business process design is primarily driven by process improvementobjectives. However, the role of control objectives stemming from regulationsand standards is becoming increasingly important for businesses in light ofrecent events that led to some of the largest scandals in corporate history. Asorganizations strive to meet compliance agendas, there is an evident need toprovide systematic approaches that assist in the understanding of the interplaybetween (often conflicting) business and control objectives during businessprocess design. In this paper, our objective is twofold. We will firstly present aresearch agenda in the space of business process compliance, identifying majortechnical and organizational challenges. We then tackle a part of the overallproblem space, which deals with the effective modeling of control objectivesand subsequently their propagation onto business process models. Controlobjective modeling is proposed through a specialized modal logic based onnormative systems theory, and the visualization of control objectives onbusiness process models is achieved procedurally. The proposed approach isdemonstrated in the context of a purchase-to-pay scenario.}}
@ARTICLE{Ashley_2008,title={A Process Model of Legal Argument with Hypotheticals},year={2008},author={Kevin D. Ashley and Kevin D. Ashley and Collin F. Lynch and Collin Lynch and Niels Pinkwart and Niels Pinkwart and Vincent Aleven and Vincent Aleven},doi={null},pmid={null},pmcid={null},mag_id={1573032755},journal={null},abstract={This paper presents a process model of arguing with hypotheticals and uses it to explain examples of oral arguments before the U.S. Supreme Court that are like those employed in Socratic law teaching. The process model has been partially implemented in the LARGO (Legal ARgument Graph Observer) intelligent tutoring system. The program supports students in diagramming oral argument examples; its feedback on students' diagrammatic reconstructions of the examples enforces the expectations of the process model. The paper presents empirical evidence that features of the argument diagrams made with LARGO are correlated with independent measures of argumentation ability. The examples and empirical results support the model's explanatory and diagnostic utility.}}
@ARTICLE{Rissland_1995,title={Evaluating a Legal Argument Progra: The BankXX Experiments},year={1995},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak and Menahem Friedman and M. T Friedman},doi={10.1023/a:1008215000938},pmid={null},pmcid={null},mag_id={1584634947},journal={Artificial Intelligence and Law},abstract={In this article we evaluate the BankXX program from several perspectives. BankXX is a case-based legal argument program that retrieves cases and other legal knowledge pertinent to a legal argument through a combination of heuristic search and knowledge-based indexing. The program is described in detail in a companion article in (this issue of) the Journal of Artificial Intelligence and Law. Three perspectives are used to evaluate BankXX: (1) classical information retrieval measures of precision and recall applied against a hand-coded baseline; (2) knowledge- representation and case-based reasoning perspectives, where the baseline is provided by the functionality of a well-known case-based argument program, HYPO [Ashley, 1990]; and (3) search perspective, in which the performance of BankXX run with various parameter settings, for instance, resource limits, is compared. In this article we report on an extensive series of experiments performed to evaluate the program. We also describe two brief experiments on ancillary questions regarding the program''s search behavior and knowledge representation. Finally we offer some general conclusions that might be drawn from these particular experiments.}}
@ARTICLE{Lodder_1999,title={DiaLaw: On Legal Justification and Dialogical Models of Argumentation},year={1999},author={A.R. Lodder and Arno R. Lodder},doi={null},pmid={null},pmcid={null},mag_id={1593671444},journal={null},abstract={Preface. 1. Introduction. 2. From law to DiaLaw. 3. DiaLaw. Framework and general rules. 4. DiaLaw. Special rules for communication. 5. DiaLaw in action. 6. Dialogical models of argumentation. 7. What is an argument? Properties of procedural models of argumentation. 8. In conclusion. Appendix- Prolog code of DiaLaw. References. Index of names. Index of subjects.}}
@ARTICLE{Ericsson_2006,title={The Cambridge Handbook of Expertise and Expert Performance},year={2006},author={K. Anders Ericsson and K. Anders Ericsson and Neil Charness and Neil Charness and Paul J. Feltovich and Paul J. Feltovich and Robert R. Hoffman and Robert R. Hoffman},doi={10.1017/cbo9780511816796},pmid={null},pmcid={null},mag_id={1602376808},journal={null},abstract={Introduction and perspective -- An introduction to Cambridge handbook of expertise and expert performance : its development, organization, and content / K. Anders Ericsson -- Two approaches to the study of experts' characteristics / Michelene T.H. Chi -- Expertise, talent, and social encouragement / Earl Hunt -- Overview of approaches to the study of expertise : brief historical accounts of theories and methods -- Studies of expertise from psychological perspectives / Paul J. Feltovich, Michael J. Prietula & K. Anders Ericsson -- Educators and expertise : a brief history of theories and models / Ray J. Amirault & Robert K. Branson -- Expert systems : a perspective from computer science / Bruce G. Buchanan, Randall Davis, & Edward A. Feigenbaum -- Professionalization, scientific expertise, and elitism : a sociological perspective / Julia Evetts, Harald A. Mieg, & Ulrike Felt -- Methods for studying the structure of expertise --^}}
@ARTICLE{Shamsaei_2010,title={Business Process Compliance Tracking Using Key Performance Indicators},year={2010},author={Azalia Shamsaei and Azalia Shamsaei and Alireza Pourshahid and Alireza Pourshahid and Daniel Amyot and Daniel Amyot},doi={10.1007/978-3-642-20511-8_7},pmid={null},pmcid={null},mag_id={1605405386},journal={null},abstract={Compliance of business processes with authoritative rules is significantly important to avoid financial penalties, efficiency problems, and reputation damages. However, finding the right measures to evaluate and track compliance is very challenging. We propose a novel method to model the context and measure compliance using the User Requirements Notation (URN). We mainly use Key Performance Indicator (KPI) extensions of URN to measure the level of compliance to rules. Such KPIs have been used in the past to measure the satisfaction level of goals and the performance of business processes. Yet, they have never been used for measuring compliance. Our method highlights the non-compliant policies and rules on a quadrant map based on their importance and compliance levels. Furthermore, we suggest a new method for importance calculation in this context. We use a human resource policy example to illustrate our method.}}
@ARTICLE{Rissland_2002,title={A note on dimensions and factors},year={2002},author={Edwina L. Rissland and Edwina L. Rissland and Kevin D. Ashley and Kevin D. Ashley},doi={10.1023/a:1019543817123},pmid={null},pmcid={null},mag_id={1686779877},journal={Artificial Intelligence and Law},abstract={In this short note, we discuss several aspectsof "dimensions" and the related constructof "factors". We concentrate on those aspectsthat are relevant to articles in this specialissue, especially those dealing with the analysisof the wild animal cases discussed inBerman and Hafner's 1993 ICAIL article. We reviewthe basic ideas about dimensions,as used in HYPO, and point out differences withfactors, as used in subsequent systemslike CATO. Our goal is to correct certainmisconceptions that have arisen over the years.}}
@ARTICLE{Hoekstra_2007,title={The LKIF core ontology of basic legal concepts},year={2007},author={Rinke Hoekstra and Rinke Hoekstra and Joost Breuker and Joost Breuker and Marcello Di Bello and M. di Bello and Alexander Boer and Alexander Boer},doi={null},pmid={null},pmcid={null},mag_id={1723826121},journal={null},abstract={In this paper we describe a legal core ontology that is part of a generic architecture for legal knowledge systems, which will enable the interchange of knowledge between existing legal knowledge systems. This Legal Knowledge Interchange Format, is under development in the Estrella project and has two main roles: 1) the translation of legal knowledge bases written in different representation formats and formalisms and 2) a knowledge representation formalism that is part of a larger architecture for developing legal knowledge systems. A legal (core) ontology can play an important role in the translation of existing legal knowledge bases to other representation formats, in particular into LKIF as the basis for articulate knowledge serving. We describe the methodology underlying the LKIF core ontology, introduce the concepts it defines, and discuss its use in the formalisation of an EU directive.}}
@ARTICLE{Shanteau_1999,title={Decision Making by Experts: The GNAHM Effect},year={1999},author={James Shanteau and James Shanteau},doi={10.1007/978-1-4615-5089-1_7},pmid={null},pmcid={null},mag_id={1914065239},journal={null},abstract={Psychological studies involving experts date back to the earliest days of experimental psychology. Research on domain experts has also been a fundamental part of the history of judgment and decision making (JDM). The purpose of this chapter is to look at how domain experts have been viewed in the decision making literature. The focus will be on an unappreciated historical bias derived from a misinterpretation of the foundations of experimental psychology.}}
@ARTICLE{Pollock_1995,title={Cognitive Carpentry: A Blueprint for How to Build a Person},year={1995},author={John L. Pollock and John L. Pollock},doi={null},pmid={null},pmcid={null},mag_id={1936596455},journal={null},abstract={From the Publisher:
In his groundbreaking new book, John Pollock establishes an outpost at the crossroads where artificial intelligence meets philosophy. Specifically, he proposes a general theory of rationality and then describes its implementation in OSCAR, an architecture for an autonomous rational agent he claims is the "first AI system capable of performing reasoning that philosophers would regard as epistemically sophisticated."
A sequel to Pollock's How to Build a Person, this volume builds upon that theoretical groundwork for the implementation of rationality through artificial intelligence. Pollock argues that progress in AI has stalled because of its creators' reliance upon unformulated intuitions about rationality. Instead, he bases the OSCAR architecture upon an explicit philosophical theory of rationality, encompassing principles of practical cognition, epistemic cognition, and defeasible reasoning. One of the results is the world's first automated defeasible reasoner capable of reasoning in a rich, logical environment.
Underlying Pollock's thesis is a conviction that the tenets of artifical intelligence and those of philosophy can be complementary and mutually beneficial. And, while members of both camps have in recent years grown skeptical of the very possibility of "symbol processing" AI, Cognitive Carpentry establishes that such an approach to AI can be successful. 
A Bradford Book}}
@ARTICLE{Amgoud_1998,title={On the acceptability of arguments in preference-based argumentation},year={1998},author={Leïla Amgoud and Leila Amgoud and Claudette Cayrol and Claudette Cayrol},doi={null},pmid={null},pmcid={null},mag_id={1961668815},journal={null},abstract={Argumentation is a promising model for reasoning with uncertain and inconsistent knowledge. The key concept of acceptability enables to differentiate arguments and defeaters: The certainty of a proposition can then be evaluated through the most acceptable arguments for that proposition. In this paper, we investigate different complementary points of view: an acceptability based on the existence of direct defeaters and an acceptability based on the existence of defenders. Pursuing previous work on preference-based argumentation principles, we enforce both points of view by taking into account preference orderings for comparing arguments. Our approach is illustrated in the context of reasoning with stratified knowledge bases.}}
@ARTICLE{Breaux_2011,title={Legally reasonable security requirements: A 10-year FTC retrospective},year={2011},author={Travis D. Breaux and Travis D. Breaux and David L. Baumer and David L. Baumer},doi={10.1016/j.cose.2010.11.003},pmid={null},pmcid={null},mag_id={1966182711},journal={Computers & Security},abstract={Growth in electronic commerce has enabled businesses to reduce costs and expand markets by deploying information technology through new and existing business practices. However, government laws and regulations require businesses to employ reasonable security measures to thwart risks associated with this technology. Because many security vulnerabilities are only discovered after attacker exploitation, regulators update their interpretation of reasonable security to stay current with emerging threats. With a focus on determining what businesses must do to comply with these changing interpretations of the law, we conducted an empirical, multi-case study to discover and measure the meaning and evolution of ''reasonable'' security by examining 19 regulatory enforcement actions by the U.S. Federal Trade Commission (FTC) over a 10 year period. The results reveal trends in FTC enforcement actions that are institutionalizing security knowledge as evidenced by 39 security requirements that mitigate 110 legal security vulnerabilities.}}
@ARTICLE{Rissland_1991,title={CABARET: rule interpretation in a hybrid architecture},year={1991},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak},doi={10.1016/0020-7373(91)90013-w},pmid={null},pmcid={null},mag_id={1966379407},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={null}}
@ARTICLE{Breuker_1991,title={Separating world and regulation knowledge: where is the logic},year={1991},author={Joost Breuker and Joost Breuker and Nienke den Haan and Nienke den Haan},doi={10.1145/112646.112658},pmid={null},pmcid={null},mag_id={1966626839},journal={null},abstract={null}}
@ARTICLE{Braun_2012,title={Drafting and modeling of regulations: Is it being done backwards?},year={2012},author={Edna Braun and Edna Braun and Nick Cartwright and Nick Cartwright and Azalia Shamsaei and Azalia Shamsaei and Saeed Ahmadi Behnam and Saeed Ahmadi Behnam and Gregory Richards and Greg Richards and Gunter Mussbacher and Gunter Mussbacher and Mohammad Alhaj and Mohammad Alhaj and Rasha Tawhid and Rasha Tawhid},doi={10.1109/relaw.2012.6347802},pmid={null},pmcid={null},mag_id={1966752000},journal={null},abstract={The performance modeling of regulations is a relatively recent innovation. However, as regulators in many domains increasingly look to move from prescriptive regulations towards more outcome-based regulations, the use of performance modeling will become more common place. The major difference of outcome-based regulations over prescriptive regulations is that the main interest lies in specifying clear objectives of the regulations and measuring whether regulated parties achieve these objectives, while leaving much freedom to the regulated party on how to meet these objectives. Recently, we have found that the use of performance modeling provides benefits such as revealing inconsistencies and lack of clarity in existing regulatory language. In this paper, we report on these experiences, summarize guidelines for the modeling of regulations, and examine whether the current drafting processes for regulations are optimized to take advantage of these additional benefits. We explore the advantages and disadvantages of various ways of augmenting the current approach with goal-oriented modeling of regulations. Based on our experience with Aviation Security regulations, we believe it is time for modeling to play a new role in helping to guide the drafting of regulations.}}
@ARTICLE{Modgil_2009,title={Reasoning about preferences in argumentation frameworks},year={2009},author={Sanjay Modgil and Sanjay Modgil},doi={10.1016/j.artint.2009.02.001},pmid={null},pmcid={null},mag_id={1966873207},journal={Artificial Intelligence},abstract={The abstract nature of Dung's seminal theory of argumentation accounts for its widespread application as a general framework for various species of non-monotonic reasoning, and, more generally, reasoning in the presence of conflict. A Dung argumentation framework is instantiated by arguments and a binary conflict based attack relation, defined by some underlying logical theory. The justified arguments under different extensional semantics are then evaluated, and the claims of these arguments define the inferences of the underlying theory. To determine a unique set of justified arguments often requires a preference relation on arguments to determine the success of attacks between arguments. However, preference information is often itself defeasible, conflicting and so subject to argumentation. Hence, in this paper we extend Dung's theory to accommodate arguments that claim preferences between other arguments, thus incorporating meta-level argumentation based reasoning about preferences in the object level. We then define and study application of the full range of Dung's extensional semantics to the extended framework, and study special classes of the extended framework. The extended theory preserves the abstract nature of Dung's approach, thus aiming at a general framework for non-monotonic formalisms that accommodate defeasible reasoning about as well as with preference information. We illustrate by formalising argument based logic programming with defeasible priorities in the extended theory.}}
@ARTICLE{Ashley_1989,title={Toward a computational theory of arguing with precedents},year={1989},author={Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/74014.74028},pmid={null},pmcid={null},mag_id={1966954430},journal={null},abstract={This paper presents a partial theory of arguing with precedents in law and illustrates how that theory supports multiple interpretations of a precedent. The theory provides succinct computational definitions of (1) the most persuasive precedents to cite in the principal argument roles and (2) the most salient aspects of the precedents to emphasize when citing them in those roles. An extended example, drawn from the output of the HYPO program, illustrates the range of different descriptions of the same precedent that are supported by the theory. Each description focuses on different salient aspects of the case depending on the argument context.}}
@ARTICLE{Fleiss_1971,title={Measuring nominal scale agreement among many raters.},year={1971},author={Joseph L. Fleiss and Joseph L. Fleiss and Joseph L. Fleiss},doi={10.1037/h0031619},pmid={null},pmcid={null},mag_id={1975879668},journal={Psychological Bulletin},abstract={null}}
@ARTICLE{Bench‐Capon_1991,title={Argument-based explanation of logic programs},year={1991},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and D. Lowes and D. Lowes and Duncan Lowes and Tony McEnery and A. M. McEnery},doi={10.1016/0950-7051(91)90007-o},pmid={null},pmcid={null},mag_id={1976424220},journal={Knowledge Based Systems},abstract={null}}
@ARTICLE{Horty_2012,title={A factor-based definition of precedential constraint},year={2012},author={John F. Horty and John F. Horty and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-012-9125-8},pmid={null},pmcid={null},mag_id={1976902801},journal={Artificial Intelligence and Law},abstract={This paper describes one way in which a precise reason model of precedent could be developed, based on the general idea that courts are constrained to reach a decision that is consistent with the assessment of the balance of reasons made in relevant earlier decisions. The account provided here has the additional advantage of showing how this reason model can be reconciled with the traditional idea that precedential constraint involves rules, as long as these rules are taken to be defeasible. The account presented is firmly based on a body of work that has emerged in AI and Law. This work is discussed, and there is a particular discussion of approaches based on theory construction, and how that work relates to the model described in this paper.}}
@ARTICLE{Bench‐Capon_1999,title={Some observations on modelling case based reasoning with formal argument models},year={1999},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/323706.323713},pmid={null},pmcid={null},mag_id={1977898142},journal={null},abstract={In this paper I shall explore the modelling of case based reasoning using a formal model of argument, taking the approach of Prakken and Sartor as my starting point. I first consider their method of representing cases, and describe how — if we restrict ourselves to independent boolean factors — we can fruitfully model the domain as a partial order on rules. I then consider the issues relating to quantifiable factors, as used in HYPO, and factor hierarchies, as used in CATO. The former presents some difficulties for modelling as a partial order, and, coupled with the latter, forces us to recognise two different kinds of reasoning used in concept application which have different implications for representing the domain. I then present some conclusions arising from the discussion.}}
@ARTICLE{Gordon_1997,title={The Zeno argumentation framework},year={1997},author={Thomas F. Gordon and Thomas F. Gordon and Nikos Karacapilidis and Nikos Karacapilidis},doi={10.1145/261618.261622},pmid={null},pmcid={null},mag_id={1979320841},journal={null},abstract={The Zeno Argumentation Framework is a formal model of argumentation based on the informal models of Toulmin and Rittel. Its main feature is a labelling function using arguments to compute heuristic information about the relative quality of the alternative positions proposed as solutions for some practical issue. The Zeno Argumentation Framework was designed to be used in mediation systems, an advanced kind of electronic discussion forum with special support for argumentation, negotiation and other structured forms of group decision-making.}}
@ARTICLE{Bench‐Capon_2005,title={Persuasion and Value in Legal Argument},year={2005},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson and Alison Chorley and Alison Chorley},doi={10.1093/logcom/exi058},pmid={null},pmcid={null},mag_id={1982993548},journal={Journal of Logic and Computation},abstract={In this paper we consider legal reasoning as a species of practical reasoning. As such it is important both that arguments are considered in the context of competing, attacking and supporting arguments, and that the possibility of rational disagreement is accommodated. We present two formal frameworks for considering systems of arguments: the standard framework of Dung, and an extension which relates arguments to values allowing for rational disagreement. We apply these frameworks to modelling a body of case law, explain how the frameworks can be generated to reconstruct legal reasoning in particular cases, and describe some tools to support the extraction of the value related knowledge required from a set of precedent cases.}}
@ARTICLE{Bench‐Capon_1993,title={Argument‐based explanation of the British nationality act as a logic program},year={1993},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Frans Coenen and Frans Coenen and Paul A. Orton and Paul Orton},doi={10.1080/13600834.1993.9965668},pmid={null},pmcid={null},mag_id={1986545354},journal={Information & Communications Technology Law},abstract={Abstract Explanations are a significant component of any knowledge‐based system in the legal domain. We have previously proposed a method by which explanations can be improved by making use of annotations on program clauses as to the role of the clause, and organising the explanation according to an argument schema based on that of Stephen Toulmin. In this paper we describe an application of this approach to a part of the British Nationality Act. This serves to illustrate both the practicality of making the required annotations on a legal logic program, and the gains in terms of explanation that can be achieved.}}
@ARTICLE{Yin_2013,title={Eros: an approach for ensuring regulatory compliance of process outcomes},year={2013},author={Quanjun Yin and Quanjun Yin and Nazim H. Madhavji and Nazim H. Madhavji and Mahesh Pattani and Mahesh Pattani},doi={10.1109/relaw.2013.6671342},pmid={null},pmcid={null},mag_id={1987911881},journal={null},abstract={In the service industry, such as healthcare, catering, tourism, and others, there exist regulations that require organisations to provide service outcomes that comply with the regulations. More and more regulations in the service sector are, or are aimed to be, outcome-focused regulations. An outcome prescribed in the regulation is what users should experience or achieve when the regulated business processes are compliant. Service providers need to proactively ensure that the outcomes specified in the regulations have been achieved prior to conducting the relevant part of the business or prior to inspectors discovering noncompliance. Published approaches check requirements or business processes, not outcomes, against regulations and thus this still leaves uncertain as to whether what the users actually experience is what is prescribed in the regulations. In this research preview paper., we propose a method for assessing the compliance of process outcomes. Actual outcomes from business processes are checked against prescribed outcomes from the regulations. The method is illustrated by an example from the U.K.'s CQC regulations in the care-home environment. The key contribution of this paper is a preliminary approach for proactively checking the compliance of process outcomes.}}
@ARTICLE{Prakken_2007,title={Formalising Arguments About the Burden of Persuasion},year={2007},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1145/1276318.1276338},pmid={null},pmcid={null},mag_id={1990379426},journal={null},abstract={This paper presents an argument-based logic for reasoning about allocations of the burden of persuasion. The logic extends the system of Prakken (2001), which in turn modified the system of Prakken & Sartor (1996) with the possibility to distribute the burden of proof over both sides in an argument game. First the (2001) system is put in the context of a distinction of three types of proof burdens and it is argued that the proof burdens of that system are in fact burdens of persuasion. Then the (2001) system is modified to allow for defeasible reasoning about allocations of such burdens within the logic. The usefulness of the resulting system is illustrated with applications to real legal cases.}}
@ARTICLE{Amyot_2003,title={Introduction to the user requirements notation: learning by example},year={2003},author={Daniel Amyot and Daniel Amyot},doi={10.1016/s1389-1286(03)00244-5},pmid={null},pmcid={null},mag_id={1994092777},journal={Computer Networks},abstract={null}}
@ARTICLE{Rissland_1993,title={BankXX: a program to generate argument through case-base search},year={1993},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak and Menahem Friedman and M. Timur Friedman},doi={10.1145/158976.158991},pmid={null},pmcid={null},mag_id={1995700735},journal={null},abstract={In this paper we describe a system, called BankXX, which generates arguments by performing a heuristic best-first search of a highly interconnected network of legal knowledge. The legal knowledge includes cases represented from a variety of points of view—cases as collections of facts, cases as dimensionally-analyzed fact situations, cases as bundles of citations, and cases as prototypical factual scripts—as well as legal theories represented in terms of domain factors. BankXX performs its search for useful information using one of three evaluation functions encoded at different levels of abstraction: the domain level, an “argument-piece” level, and the overall argument level. Evaluation at the domain level uses easily accessible information about the nodes,  such  as their type; evaluation at the argument-piece level uses information about generally useful components of case-based argument, such as best cases and supporting legal theories; evaluation at the overall-argument level uses factors, called argument dimensions, which address the overall substance and quality of an argument, such as the centrality of its supporting cases or the success record of its best theory. BankXX is instantiated in the area of personal bankruptcy governed by Chapter 13 of the U.S. Bankruptcy Code, which permits a debtor to be discharged from debts through completion of a court-approved payment plan. In particular, our system addresses the requirement that such Chapter 13 plans be “proposed in good faith.”}}
@ARTICLE{Karlsson_2007,title={Requirements engineering challenges in market-driven software development - An interview study with practitioners},year={2007},author={Linda Karlsson and Lena Karlsson and sa G. Dahlstedt and sa G. Dahlstedt and Björn Regnell and Björn Regnell and Johan Natt och Dag and Johan Natt och Dag and Anne Persson and Anne Persson},doi={10.1016/j.infsof.2007.02.008},pmid={null},pmcid={null},mag_id={2000574603},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Schank_1977,title={Scripts, plans, goals and understanding: an inquiry into human knowledge structures},year={1977},author={Roger C. Schank and Roger C. Schank and Robert P. Abelson and Robert P. Abelson},doi={null},pmid={null},pmcid={null},mag_id={2000900121},journal={null},abstract={For both people and machines, each in their own way, there is a serious problem in common of making sense out of what they hear, see, or are told about the world. The conceptual apparatus necessary to perform even a partial feat of understanding is formidable and fascinating. Our analysis of this apparatus is what this book is about. Roger C. Schank and Robert P. Abelson from the Introduction (http://www.psypress.com/scripts-plans-goals-and-understanding-9780898591385)}}
@ARTICLE{Johnson_1991,title={Legislative knowledge base systems for public administration: some practical issues},year={1991},author={Peter D. Johnson and Peter Johnson and David Mead and David Mead},doi={10.1145/112646.112660},pmid={null},pmcid={null},mag_id={2004260949},journal={null},abstract={The first part of the paper suggests that knowledge base systems can be viewed simply as a means of changing the medium in which legislation is presented to administrative decision-makers for their consideration. When viewed as a delivery vehicle for a body of rules, rather than as a reasoning decision-making tool, knowledge base systems which reliably model legislation offer major benefits in the administration of complex legislation.}}
@ARTICLE{Sapkota_2012,title={Extracting meaningful entities from regulatory text: Towards automating regulatory compliance},year={2012},author={Krishna Sapkota and Krishna Sapkota and Arantza Aldea and Arantza Aldea and Muhammad Younas and Muhammad Younas and David A. Duce and David A. Duce and René Bañares‐Alcántara and René Bañares-Alcántara},doi={10.1109/relaw.2012.6347798},pmid={null},pmcid={null},mag_id={2005285066},journal={null},abstract={Extracting essential meaning from the regulatory text helps in the automation of the Compliance Management (CM) process. CM is a process where organizations assure that the processes are run according to requirements and expectations. However, extraction of meaningful text from regulatory guidelines comes with many research challenges such as dealing with different document-format, implicit document-structure, textual ambiguity and complexity. In this paper, the extended version of the Semantic-ART framework is described, which focuses on tackling the challenges of document-structure identification and regulatory-entity extraction. An initial result has shown an inspirational result as compared to the previous version of the framework.}}
@ARTICLE{Rubin_1991,title={Interpreting Statutes: A Comparative Study},year={1991},author={Edward L. Rubin and D. Neil MacCormick and D. N. MacCormick and Robert S. Summers and Robert S. Summers},doi={null},pmid={null},pmcid={null},mag_id={2009122572},journal={null},abstract={Contents: Introduction, Robert Summers Ithaca on method and methodology, Zenon Bankowski, Neil MacCormick and Jerzy Wroblewski Statutory Interpretation in Argentina, Enrique Zuleta-Puceiro Statutory interpretation in the Federal Republic of Germany, Robert Alexy and Ralf Drier Statutory interpretation in Finland, Aulis Aarnio Statutory Interpretation In France, Michel Troper, Christophe Grzegorczyk and Jean Louis Gardes Statutory Interpretation in Italy, Massimo La Torre, Enrico Pattaro and Michele Taruffo Statutory Interpretation in Poland, Jerzy Wroblewski Statutory Interpretation in Sweden, Alexsander Peczenik and Gunnar Bergholtz Statutory Interpretation in the United Kingdom, Zenon Bankowski and Neil MacCormick Statutory Interpretation in the United States, Robert S. Summers Interpretation and Comparative Analysis, Robert S. Summers and Michele Taruffo Interpretation and Justification, Neil MacCormick and Robert S. Summers Appendix Index.}}
@ARTICLE{Aleven_2003,title={Using background knowledge in case-based legal reasoning: a computational model and an intelligent learning environment},year={2003},author={Vincent Aleven and Vincent Aleven},doi={10.1016/s0004-3702(03)00105-x},pmid={null},pmcid={null},mag_id={2009299421},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Atkinson_2006,title={PARMENIDES: facilitating deliberation in democracies},year={2006},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Peter McBurney and Peter McBurney},doi={10.1007/s10506-006-9001-5},pmid={null},pmcid={null},mag_id={2011097580},journal={Artificial Intelligence and Law},abstract={Governments and other groups interested in the views of citizens require the means to present justifications of proposed actions, and the means to solicit public opinion concerning these justifications. Although Internet technologies provide the means for such dialogues, system designers usually face a choice between allowing unstructured dialogues, through, for example, bulletin boards, or requiring citizens to acquire a knowledge of some argumentation schema or theory, as in, for example, ZENO. Both of these options present usability problems. In this paper, we describe an implemented system called PARMENIDES which allows structured argument over a proposed course of action, without requiring knowledge of the underlying argumentation theory.}}
@ARTICLE{Smith_1997,title={The use of lexicons in information retrieval in legal databases},year={1997},author={J. C. Smith and J. C. Smith},doi={10.1145/261618.261625},pmid={null},pmcid={null},mag_id={2011103481},journal={null},abstract={This paper reports on the mid-beta stage of development of the Flexicon system and descrii how a 1exicaUy designed search engine in a domain specific database such as a large database of legal cases, can provide better relevance ranking than best match search engines, and at the same time permit an information need to be formulated using multiple word concepts, phrases and items, which is the one great advantage that an exact match Boolean search engine has to offer over best match engines. In addition the paper sets out in detail the lexical structure of the Flexicon system. The first tests on a legal problem, comparing the Flexicon system with two best match systems, indicate that a lexkally designed search engine and database has the potential for a substantially higher level of precision than best match search engines.}}
@ARTICLE{Shanteau_2002,title={Performance-based assessment of expertise: How to decide if someone is an expert or not},year={2002},author={James Shanteau and James Shanteau and David J. Weiss and David J. Weiss and Rick P. Thomas and Rickey P. Thomas and Julia C Pounds and Julia Pounds},doi={10.1016/s0377-2217(01)00113-8},pmid={null},pmcid={null},mag_id={2024711376},journal={European Journal of Operational Research},abstract={null}}
@ARTICLE{Aleven_1997,title={Evaluating a learning environment for case-based argumentation skills},year={1997},author={Vincent Aleven and Vincent Aleven and Vincent Aleven and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/261618.261650},pmid={null},pmcid={null},mag_id={2032556714},journal={null},abstract={CAT0 is an intelligent learning environment designed to help &@ting law students learn basic skills of making arguments with cases, through practice in theory-testing and argumentation tasks. CAT0 models ways in which experts compare and contrast cases, assess the significance of similarities and differences between cases in light of general domain knowledge, and use the same general knowledge to organize multi-case arguments by issues. CAT0 communicates its model to students by presenting dynamically-generated argumentation examples and reifying (i.e., making visible) argument structure. Also, the CAT0 Tools reduce some of the distracting complexity of the students’ task. gal education seeks to teach. But they are difficult to learn and time-consuming to teach. If computer-based instruction can help students get some extra practice in basic skills and free up instructor’s time for more advanced topics, much would be gained. We evaluated CAT0 in the context of a second-semester Iegal writing course taught at the University of Pittsburgh School of Law. We found that 7.5 hours of CAT0 instruction led to a statistically significant improvement in students’ basic argumentation skills, comparable to that achieved, in the same amount of time, by an experienced legal writing instructor teaching small groups of students in a more traditional way. On a more advanced memo-writing assignment, meant to explore the “frontier” of the CAT0 instruction, students taught by the legal writing instructor did better, indicating that more is needed if CAT0 is to help students to improve their memo-writing skills. CAT0 employs a computational model of case-based argumentation that addresses eight basic argument moves and more elaborate multi-case arguments. The model includes a “Factor Hierarchy,” which represents more nbstract., but still domain-specific, legal knowledge about the meaning of the factors used to represent cases. CAT0 uses the Factor Hierarchy for a number of purposes, among them to organize multi-case arguments by issues and to make arguments about the significance of distinctions. To generate the latter, CAT0 strategically selects alternative interpretations of cases to elaborate a “deeper” (or more. abstract) contrast or parallel between cases.}}
@ARTICLE{Geffner_1992,title={Conditional entailment: bridging two approaches to default reasoning},year={1992},author={Héctor Geffner and Hector Geffner and Judea Pearl and Judea Pearl},doi={10.1016/0004-3702(92)90071-5},pmid={null},pmcid={null},mag_id={2035416838},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Bench‐Capon_1997,title={Ontologies in legal information systems; the need for explicit specifications of domain conceptualisations},year={1997},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Pepijn R. S. Visser and Pepijn R. S. Visser},doi={10.1145/261618.261646},pmid={null},pmcid={null},mag_id={2035770827},journal={null},abstract={null}}
@ARTICLE{Hart_1961,title={The Concept of Law},year={1961},author={Herbert Lionel Adolphus Hart and H. L. A. Hart},doi={null},pmid={null},pmcid={null},mag_id={2035902442},journal={null},abstract={The Concept of Law is the most important and original work of legal philosophy written this century. First published in 1961, it is considered the masterpiece of H.L.A. Hart's enormous contribution to the study of jurisprudence and legal philosophy. Its elegant language and balanced arguments have sparked wide debate and unprecedented growth in the quantity and quality of scholarship in this area--much of it devoted to attacking or defending Hart's theories. Principal among Hart's critics is renowned lawyer and political philosopher Ronald Dworkin who in the 1970s and 80s mounted a series of challenges to Hart's Concept of Law. It seemed that Hart let these challenges go unanswered until, after his death in 1992, his answer to Dworkin's criticism was discovered among his papers. In this valuable and long-awaited new edition Hart presents an Epilogue in which he answers Dworkin and some of his other most influential critics including Fuller and Finnis. Written with the same clarity and candor for which the first edition is famous, the Epilogue offers a sharper interpretation of Hart's own views, rebuffs the arguments of critics like Dworkin, and powerfully asserts that they have based their criticisms on a faulty understanding of Hart's work. Hart demonstrates that Dworkin's views are in fact strikingly similar to his own. In a final analysis, Hart's response leaves Dworkin's criticisms considerably weakened and his positions largely in question. Containing Hart's final and powerful response to Dworkin in addition to the revised text of the original Concept of Law, this thought-provoking and persuasively argued volume is essential reading for lawyers and philosophers throughout the world.}}
@ARTICLE{Schweighofer_2001,title={Automatic text representation, classification and labeling in European law},year={2001},author={Erich Schweighofer and Erich Schweighofer and Andreas Rauber and Andreas Rauber and Michael Dittenbach and Michael Dittenbach},doi={10.1145/383535.383544},pmid={null},pmcid={null},mag_id={2038480249},journal={null},abstract={The huge text archives and retrieval systems of legal information have not achieved yet the representation in the well-known subject-oriented structure of legal commentaries. Content-based classification and text analysis remains a high priority research topic. In the joint KONTERM, SOM and LabelSOM projects, learning techniques of neural networks are used to achieve similar high compression rates of classification and analysis like in manual legal indexing. The produced maps of legal text corpora cluster related documents in units that are described with automatically selected descriptors. Extensive tests with text corpora in European case law have shown the feasibility of this approach. Classification and labeling proved very helpful for legal research. The Growing Hierarchical Self-Organizing Map represents very interesting generalities and specialties of legal text corpora. The segmentation into document parts improved very much the quality of labeling. The next challenge would be a change from tf × idf vector representation to a modified vector representation taking into account thesauri or ontologies considering learned properties of legal text corpora.}}
@ARTICLE{Ingolfo_2013,title={Arguing regulatory compliance of software requirements},year={2013},author={Silvia Ingolfo and Silvia Ingolfo and Alberto Siena and Alberto Siena and John Mylopoulos and John Mylopoulos and Angelo Susi and Angelo Susi and Angelo Susi and Angelo Susi and Anna Perini and Anna Perini},doi={10.1016/j.datak.2012.12.004},pmid={null},pmcid={null},mag_id={2038625040},journal={null},abstract={null}}
@ARTICLE{Gordon_1987,title={Oblog-2: A hybrid knowledge representation system for defeasible reasoning},year={1987},author={Thomas F. Gordon and T. F. Gordon},doi={10.1145/41735.41761},pmid={null},pmcid={null},mag_id={2039274370},journal={null},abstract={Oblog-2 is a hybrid knowledge representation system comparable to Krypton and KL-TWO. It combines a terminological reasoner with a Prolog-like inference mechanism. The terminological component supports the description of type and attribute taxonomies. Entities are instances of a set of types. Procedures for determining the values of attributes are Horn clause rules indexed by type. The known types of an entity determine its set of applicable rules, which changes as our knowledge about the types of the entity is refined, supporting a form of defeasible reasoning. Oblog-2 has been designed for modeling legal domains. Laws can be represented as general rules with exceptions, a technique traditionally used in the law, together with burden of proof rules, for reaching decisions when less than perfect information is available.}}
@ARTICLE{Bench‐Capon_1993,title={Neural networks and open texture},year={1993},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/158976.159012},pmid={null},pmcid={null},mag_id={2039686381},journal={null},abstract={In this paper some experiments designed to explore the suitability of using neural nets to tackle problems of open texture in law are described. Three key questions are investigated: can a net classify cases successfully; can an acceptable rationale be uncovered by an examination of the net; and can we derive rules describing the problem from an examination of the net?}}
@ARTICLE{Prakken_1991,title={A tool in modelling disagreement in law: preferring the most specific argument},year={1991},author={Henry Prakken and Henry Prakken},doi={10.1145/112646.112666},pmid={null},pmcid={null},mag_id={2041154952},journal={null},abstract={Article Free Access Share on A tool in modelling disagreement in law: preferring the most specific argument Author: Henry Prakken Computer/Law Institute, Vrije Universiteit Amsterdam Computer/Law Institute, Vrije Universiteit AmsterdamView Profile Authors Info & Claims ICAIL '91: Proceedings of the 3rd international conference on Artificial intelligence and lawMay 1991 Pages 165–174https://doi.org/10.1145/112646.112666Online:01 May 1991Publication History 25citation112DownloadsMetricsTotal Citations25Total Downloads112Last 12 Months7Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Al-Abdulkarim_2015,title={Factors, issues and values: revisiting reasoning with cases},year={2015},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/2746090.2746103},pmid={null},pmcid={null},mag_id={2042702269},journal={null},abstract={In this paper we revisit reasoning with legal cases, with a view to articulating the relationships between issues, factors, facts and values, and to identifying areas for future work on these topics. We start from the different ways in which attempts have been made to go beyond a fortori reasoning from the precedent base, so that conclusions not fully justified by the precedents can be drawn. We then use a particular example domain taken from the literature to illustrate our preferred approach and to relate factors and values. From this we observe that much current work depends critically on the ascription of factors to cases in a Boolean manner, while in practice there are compelling reasons to see the presence of factors as a matter of degree. On the basis of our observations we make suggestions for the directions of future work on this topic.}}
@ARTICLE{Verheij_2001,title={Legal decision making as dialectical theory construction with argumentation schemes},year={2001},author={Bart Verheij and Bart Verheij},doi={10.1145/383535.383565},pmid={null},pmcid={null},mag_id={2043417657},journal={null},abstract={No abstract available.}}
@ARTICLE{Zeleznikow_1995,title={The split-up system: integrating neural networks and rule-based reasoning in the legal domain},year={1995},author={John Zeleznikow and John Zeleznikow and Andrew Stranieri and Andrew Stranieri},doi={10.1145/222092.222235},pmid={null},pmcid={null},mag_id={2044851278},journal={null},abstract={Article Free Access Share on The split-up system: integrating neural networks and rule-based reasoning in the legal domain Authors: John Zeleznikow Collaborative Law and Artificial Intelligence Research Project, Database Research Laboratory, Applied Computing, Research Institute, La Trobe University, Bundoora, Victoria, Australia 3083 Collaborative Law and Artificial Intelligence Research Project, Database Research Laboratory, Applied Computing, Research Institute, La Trobe University, Bundoora, Victoria, Australia 3083View Profile , Andrew Stranieri Collaborative Law and Artificial Intelligence Research Project, Database Research Laboratory, Applied Computing, Research Institute, La Trobe University, Bundoora, Victoria, Australia 3083 Collaborative Law and Artificial Intelligence Research Project, Database Research Laboratory, Applied Computing, Research Institute, La Trobe University, Bundoora, Victoria, Australia 3083View Profile Authors Info & Claims ICAIL '95: Proceedings of the 5th international conference on Artificial intelligence and lawMay 1995 Pages 185–194https://doi.org/10.1145/222092.222235Online:24 May 1995Publication History 45citation618DownloadsMetricsTotal Citations45Total Downloads618Last 12 Months30Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Rissland_2011,title={Catching Gray Cygnets: an initial exploration},year={2011},author={Edwina L. Rissland and Edwina L. Rissland and Xing Xu and Xiaoxi Xu},doi={10.1145/2018358.2018381},pmid={null},pmcid={null},mag_id={2046077826},journal={null},abstract={In this paper, we describe exploratory experiments for detecting potential "Gray Cygnet" cases that follow a known Black Swan. Gray Cygnets (GCs) are cases that are highly similar and subsequent to novel, surprising, provocative, exceptional cases, so-called Black Swans. They too are surprising, exceptional and provocative in the sense of continuing the change initiated by the Black Swan. Our experiments were carried out using a corpus of common law cases from the United States, particularly New York and Massachusetts, and the United Kingdom primarily in the era 1852-1916 during which there was dramatic change in the prevailing doctrine regarding recovery for damages by a remote buyer. It was provoked by the 1852 landmark case Thomas & Wife v. Winchester and elaborated in subsequent cases. Our methods use similarity measures from Case-Based Reasoning (CBR) based on classic claim lattice and nearest neighbor techniques. We also define and use "supreme on-point cases" (sopc's) that reside in the root node of a claim lattice. We define four simple tests to determine whether a problem case is a potential Gray Cygnet. The tests use the position of a known Black Swan in the analysis of a problem case, specifically whether it appears as a sopc, or mopc, and/or as a nearest neighbor. We also use the tests as an ensemble by using the decision of their majority vote. To evaluate the performance of our tests, we created an answer key based on a closed form definition of a GC in our domain.}}
@ARTICLE{Cholvy_1999,title={Checking regulation consistency by using SOL-resolution},year={1999},author={Laurence Cholvy and Laurence Cholvy},doi={10.1145/323706.323717},pmid={null},pmcid={null},mag_id={2046580268},journal={null},abstract={This paper addresses the problem of regulation consistency checking. Regulations are sets of rules which express what is obligatory, permitted, forbidden and under which conditions. We first define a first order language to model regulations. Then we introduce a definition of regulation consistency. We show that checking the consistency of a regulation comes to generate some particular consequences of some first order formulas. Then, we show that we can apply Inoue's inference rule, SOL-resolution, which is complete for generating, from some clauses, their consequences which satisfy a given condition.}}
@ARTICLE{Hafner_1987,title={Conceptual organization of case law knowledge bases},year={1987},author={Carole D. Hafner and Carole D. Hafner},doi={10.1145/41735.41740},pmid={null},pmcid={null},mag_id={2048451539},journal={null},abstract={Conceptual retrieval requires the computer to have knowledge of legal concepts and issues, and their relationship to the case law collection. This paper discusses the organization of a case law knowledge base in terms of three interacting components: a  domain knowledge model  defines the basic concepts of a case law domain;  individual case descriptors  describe the particular constellation of concepts that pertain to each case, organized into a frame-based superstructure according to the legal roles they fill; and  issue/case discrimination trees  represent the significance of each case relative to a model of the normative relationships of the legal domain. Each of these components is described and justified by showing its contribution to the goal of conceptual retrieval.}}
@ARTICLE{Branting_1994,title={A computational model of ratio decidendi},year={1994},author={L. Karl Branting and L. Karl Branting},doi={10.1007/bf00871744},pmid={null},pmcid={null},mag_id={2052916539},journal={Artificial Intelligence and Law},abstract={This paper proposes a model ofratio decidendi as a justification structure consisting of a series of reasoning steps, some of which relate abstract predicates to other abstract predicates and some of which relate abstract predicates to specific facts. This model satisfies an important set of characteristics ofratio decidendi identified from the jurisprudential literature. In particular, the model shows how the theory under which a case is decided controls its precedential effect. By contrast, a purely exemplar-based model ofratio decidendi fails to account for the dependency of precedential effect on the theory of decision.}}
@ARTICLE{Cohen_1960,title={A Coefficient of agreement for nominal Scales},year={1960},author={Jacob Cohen and Jacob Cohen},doi={10.1177/001316446002000104},pmid={null},pmcid={null},mag_id={2053154970},journal={Educational and Psychological Measurement},abstract={null}}
@ARTICLE{Bench‐Capon_1992,title={Isomorphism and legal knowledge based systems},year={1992},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Frans Coenen and Frans Coenen},doi={10.1007/bf00118479},pmid={null},pmcid={null},mag_id={2053716189},journal={Artificial Intelligence and Law},abstract={This paper discusses some engineering considerations that should be taken into account when building a knowledge based system, and recommends isomorphism, the well defined correspondence of the knowledge base to the source texts, as a basic principle of system construction in the legal domain. Isomorphism, as it has been used in the field of legal knowledge based systems, is characterised and the benefits which stem from its use are described. Some objections to and limitations of the approach are discussed. The paper concludes with a case study giving a detailed example of the use of the isomorphic approach in a particular application.}}
@ARTICLE{Rissland_1989,title={Interpreting statutory predicates},year={1989},author={Edwina L. Rissland and Edwina L. Rissland and David B. Skalak and David B. Skalak},doi={10.1145/74014.74021},pmid={null},pmcid={null},mag_id={2053839853},journal={null},abstract={In this paper we discuss a hybrid approach to the problem of statutory interpretation that involves combining our past approach to case-based reasoning (“CBA”), as exemplified in our previous HYPO and TAX-HYPO systems, with traditional rule-based reasoning (“RBR”), as exemplified by expert systems. We do not tackle the fullblown version of statutory interpretation, which would include reasoning with legislative intent or other normative aspects (the “ought”), but confine ourselves to reasoning with explicit cases and rules. We discuss strategies that can be used to guide interpretation, particularly the interleaving of CBR and RBR, and how they are used in an agenda-based architecture, called CABARET, which we are currently developing in a general way and experimenting with in the particular area of Section §280A(c)(1) of the U.S. Internal Revenue Code, which deals with the so called “home office deduction”.}}
@ARTICLE{Wyner_2008,title={An ontology in OWL for legal case-based reasoning},year={2008},author={Adam Wyner and Adam Wyner},doi={10.1007/s10506-008-9070-8},pmid={null},pmcid={null},mag_id={2054329666},journal={Artificial Intelligence and Law},abstract={The paper gives ontologies in the Web Ontology Language (OWL) for Legal Case-based Reasoning (LCBR) systems, giving explicit, formal, and general specifications of a conceptualisation LCBR. Ontologies for different systems allows comparison and contrast between them. OWL Ontologies are standardised, machine-readable formats that support automated processing with Semantic Web applications. Intermediate concepts, concepts between base-level concepts and higher level concepts, are central in LCBR. The main issues and their relevance to ontological reasoning and to LCBR are discussed. Two LCBR systems (AS-CATO, which is based on CATO, and IBP) are analysed in terms of basic and intermediate concepts. Central components of the OWL ontologies for these systems are presented, pointing out differences and similarities. The main novelty of the paper is the ontological analysis and representation in OWL of LCBR systems. The paper also emphasises the important issues concerning the representation and reasoning of intermediate concepts.}}
@ARTICLE{Young_2010,title={A Method for Identifying Software Requirements Based on Policy Commitments},year={2010},author={Jessica D. Young and Jessica D. Young and Annie I. Antón and Annie I. Antón},doi={10.1109/re.2010.17},pmid={null},pmcid={null},mag_id={2060691344},journal={null},abstract={Online policy documents—such as privacy policies, notices of privacy practices, and terms of use—describe organizations’ information practices for collecting, storing, and using consumers’ personal information. Organizations need to ensure that the commitments they express in their policy documents reflect their actual business practices. This compliance is significant in the United States where the Federal Trade Commission regulates fair business practices. Therefore, the requirements engineers developing systems for organizations need to understand the policy documents in order to know the information practices with which the software must comply. The requirements engineers also must ensure that the commitments expressed in these policy documents are incorporated into the software requirements. In this paper, we present a summative case study of a commitment analysis approach. The approach was developed during a formative case study of four healthcare organizations’ policy documents. Within this approach, we obtain requirements from policy documents based on our theory of commitments, privileges, and rights. During our summative case study we applied our commitment analysis approach to eight healthcare organizations’ policy documents in order to validate the methodology. We discuss the results of the summative study, in which we found that most of the statements express organizational practices or procedures. The top seen classification conveys pledges made by the organization based on organizational practices. The second most seen classification expresses actions that the user is entitled to perform based on organizational practices.}}
@ARTICLE{Araszkiewicz_2015,title={Representation of an actual divorce dispute in the parenting plan support system},year={2015},author={Michał Araszkiewicz and Michał Araszkiewicz and Agata Łopatkiewicz and Agata Łopatkiewicz and Adam Zienkiewicz and Adam Zienkiewicz and Tomasz Żurek and Tomasz Zurek},doi={10.1145/2746090.2746119},pmid={null},pmcid={null},mag_id={2062480765},journal={null},abstract={This paper evaluates the Parenting Plan Support System, a partially implemented decision support system designed to help parents to draft an agreement concerning relations with their children after the divorce, against the background of a real-life case. The focus here is on knowledge representation issues and the functioning of the inference engine.}}
@ARTICLE{Potts_1994,title={Inquiry-based requirements analysis},year={1994},author={Colin Potts and Colin Potts and Kenji Takahashi and Kenji Takahashi and Annie I. Antón and Annie I. Antón},doi={10.1109/52.268952},pmid={null},pmcid={null},mag_id={2064419470},journal={IEEE Software},abstract={This approach emphasizes pinpointing where and when information needs occur; at its core is the inquiry cycle model, a structure for describing and supporting discussions about system requirements. The authors use a case study to describe the model's conversation metaphor, which follows analysis activities from requirements elicitation and documentation through refinement. >}}
@ARTICLE{Dick_1991,title={Representation of legal text for conceptual retrieval},year={1991},author={Judith P. Dick and Judith P. Dick},doi={10.1145/112646.112676},pmid={null},pmcid={null},mag_id={2066936986},journal={null},abstract={null}}
@ARTICLE{Prakken_1997,title={Argument-based extended logic programming with defeasible priorities},year={1997},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1080/11663081.1997.10510900},pmid={null},pmcid={null},mag_id={2069176561},journal={Journal of Applied Non-Classical Logics},abstract={ABSTRACT Inspired by legal reasoning, this paper presents a semantics and proof theory of a system for defeasible argumentation. Arguments are expressed in a logic-programming language with both weak and strong negation, conflicts between arguments are decided with the help of priorities on the rules. An important feature of the system is that these priorities are not fixed, but are themselves defeasibly derived as conclusions within the system. Thus debates on the choice between conflicting arguments can also be modelled. The semantics of the system is given with a fixpoint definition, while its proof theory is stated in dialectical style, where a proof takes the form of a dialogue between a proponent and an opponent of an argument: an argument is shown to be justified if the proponent can make the opponent run out of moves in whatever way the opponent attacks.}}
@ARTICLE{Sergot_1991,title={Indian central civil service pension rules: a case study in logic programming applied to regulations},year={1991},author={Marek Sergot and Marek Sergot and Anand Kamble and A. S. Kamble and K. K. Bajaj and K. K. Bajaj},doi={10.1145/112646.112661},pmid={null},pmcid={null},mag_id={2069803795},journal={null},abstract={Logic programming has been applied to a variety of examples in law and rules and regulations of different kinds. The best known examples are the representations of the British Nationality Act 1981 [Sergot et al. 1986] and United Kingdom social security legislation [Bench-Capon et al. 1987] constructed at Imperial College, London. The paper [Sergot 1988] and the survey [Sergot 1990] contain references to other applications. Essentially, these applications construct a legal analysis program by representing some fragment of legislation in logic and then executing the representatio~ either as a stand-alone program or as a compment of some bigger system.}}
@ARTICLE{Massey_2013,title={Automated text mining for requirements analysis of policy documents},year={2013},author={Aaron K. Massey and Aaron K. Massey and Jacob Eisenstein and Jacob Eisenstein and Annie I. Antón and Annie I. Antón and Peter Swire and Peter Swire},doi={10.1109/re.2013.6636700},pmid={null},pmcid={null},mag_id={2075876574},journal={null},abstract={Businesses and organizations in jurisdictions around the world are required by law to provide their customers and users with information about their business practices in the form of policy documents. Requirements engineers analyze these documents as sources of requirements, but this analysis is a time-consuming and mostly manual process. Moreover, policy documents contain legalese and present readability challenges to requirements engineers seeking to analyze them. In this paper, we perform a large-scale analysis of 2,061 policy documents, including policy documents from the Google Top 1000 most visited websites and the Fortune 500 companies, for three purposes: (1) to assess the readability of these policy documents for requirements engineers; (2) to determine if automated text mining can indicate whether a policy document contains requirements expressed as either privacy protections or vulnerabilities; and (3) to establish the generalizability of prior work in the identification of privacy protections and vulnerabilities from privacy policies to other policy documents. Our results suggest that this requirements analysis technique, developed on a small set of policy documents in two domains, may generalize to other domains.}}
@ARTICLE{Rissland_1989,title={Dimension-based analysis of hypotheticals from supreme court oral argument},year={1989},author={Edwina L. Rissland and Edwina L. Rissland},doi={10.1145/74014.74030},pmid={null},pmcid={null},mag_id={2082657773},journal={null},abstract={In this paper we examine a sequence of hypotheticals taken from a Supreme Court oral argument. We use the idea of a “dimension,” developed previously in our case-based reasoning system HYPO, to analyze the hypotheticals and to speculate on how the Justices might have arrived at them. The case we consider is taken from the area of Fourth Amendment law concerning warrantless search and seizure.}}
@ARTICLE{Hage_1996,title={A theory of legal reasoning and a logic to match},year={1996},author={Jaap Hage and Jaap Hage},doi={10.1007/bf00118493},pmid={null},pmcid={null},mag_id={2084234373},journal={Artificial Intelligence and Law},abstract={This paper describes a model of legal reasoning and a logic for reasoning with rules, principles and goals that is especially suited to this model of legal reasoning. The paper consists of three parts. The first part describes a model of legal reasoning based on a two-layered view of the law. The first layer consists of principles and goals that express fundamental ideas of a legal system. The second layer contains legal rules which in a sense summarise the outcome of the interaction of the principles and goals for a number of case types. Both principles, goals and rules can be used in legal arguments, but their logical roles are different. One characteristic of the model of legal reasoning described in the first part of the paper is that it takes these logical differences into account. Another characteristic is that it pays serious attention to the phenomena of reasoning about the validity and acceptance of rules, respectively principles and goals, and about the application of legal rules, and the implications of these arguments for the use of rules, principles and goals in deriving legal conclusions for concrete cases.

The second part of the paper first describes a logic (Reason-Based Logic) that is especially suited to deal with legal arguments as described in terms of the previously discussed model. The facilities of the logic are illustrated by means of examples that correspond to the several aspects of the model.

The third part of the paper deals with a number of logico-philosophical reflections on Reason-Based Logic. The occasion is also used to compare these presuppositions with theories of defeasible reasoning based on the comparison of arguments.}}
@ARTICLE{Gao_2012,title={Mining Business Contracts for Service Exceptions},year={2012},author={Xibin Gao and Xibin Gao and Munindar P. Singh and Munindar P. Singh and Pankaj Mehra and P. Mehra and Pankaj Mehra},doi={10.1109/tsc.2011.1},pmid={null},pmcid={null},mag_id={2084516954},journal={IEEE Transactions on Services Computing},abstract={A contract is a legally binding agreement between real-world business entities whom we treat as providing services to one another. We focus on business rather than technical services. We think of a business contract as specifying the functional and nonfunctional behaviors of and interactions among the services. In current practice, contracts are produced as text documents. Thus the relevant service capabilities, requirements, qualities, and risks are hidden and difficult to access and reason about. We describe a simple but effective unsupervised information extraction approach and tool, Contract Miner, for discovering service exceptions at the phrase level from a large contract repository. Our approach involves preprocessing followed by an application of linguistic patterns and parsing to extract the service exception phrases. Identifying such (noun) phrases can help build service exception vocabularies that support the development of a taxonomy of business terms, and also facilitate modeling and analyzing service engagements. A lightweight online tool that comes with Contract Miner highlights the relevant text in service contracts and thereby assists users in reviewing contracts. Contract Miner produces promising results in terms of precision and recall when evaluated over a corpus of manually annotated contracts.}}
@ARTICLE{Hartley_2000,title={Legal ease and ‘Legalese’},year={2000},author={James Hartley and James Hartley},doi={10.1080/10683160008410828},pmid={null},pmcid={null},mag_id={2087811309},journal={Psychology Crime & Law},abstract={Abstract This paper reviews the evidence for Redish's (1979) seven propositions concerning legal text. These propositions are: (i) many legal documents cannot be read and understood by lay persons; (ii) people without legal training have to read and understand legal documents; (iii) much legal writing is unintelligible, even to lawyers; (iv) tradition – not necessity – and a lack of understanding of the audience – are the major reasons that legal language is so obscure; (v) legal language can be made clear without losing its precision; (vi) it is not the technical vocabulary but the complex sentence structure that makes legal writing difficult to understand; and (vii) clarity is not the same as simplicity, brevity or ‘Plain English’. The evidence supports all of these propositions, except perhaps the fifth and sixth. The research shows that writing legal text requires more attention to be given to readers than is typically the case.}}
@ARTICLE{Skalak_1991,title={Argument moves in a rule-guided domain},year={1991},author={David B. Skalak and David B. Skalak and Edwina L. Rissland and Edwin L. Rissland},doi={10.1145/112646.112647},pmid={null},pmcid={null},mag_id={2087964658},journal={null},abstract={null}}
@ARTICLE{Lee_2006,title={Building problem domain ontology from security requirements in regulatory documents},year={2006},author={Seok-Won Lee and Seok-Won Lee and Seok-Won Lee and Robin A. Gandhi and Robin A. Gandhi and Divya Muthurajan and Divya Muthurajan and Deepak S. Yavagal and Deepak S. Yavagal and Gail-Joon Ahn and Gail-Joon Ahn},doi={10.1145/1137627.1137635},pmid={null},pmcid={null},mag_id={2088955355},journal={null},abstract={Establishing secure systems assurance based on Certification and Accreditation (C&A) activities, requires effective ways to understand the enforced security requirements, gather relevant evidences, perceive related risks in the operational environment, and reveal their causal relationships with other domain concepts. However, C&A security requirements are expressed in multiple regulatory documents with complex interdependencies at different levels of abstractions that often result in subjective interpretations and non-standard implementations. Their non-functional nature imposes complex constraints on the emergent behavior of software-intensive systems, making them hard to understand, predict, and control. To address these issues, we present novel techniques from software requirements engineering and knowledge engineering for systematically extracting, modeling, and analyzing security requirements and related concepts from multiple C&A-enforced regulatory documents. We employ advanced ontological engineering processes as our primary modeling technique to represent complex and diverse characteristics of C&A security requirements and related domain knowledge. We apply our methodology to build problem domain ontology from regulatory documents enforced by the Department of Defense Information Technology Security Certification and Accreditation Process (DITSCAP).}}
@ARTICLE{Badreddin_2013,title={Regulation-Based Dimensional Modeling for Regulatory Intelligence},year={2013},author={Omar Badreddin and Omar Badreddin and Gunter Mussbacher and Gunter Mussbacher and Daniel Amyot and Daniel Amyot and Saeed Ahmadi Behnam and Saeed Ahmadi Behnam and Rouzbahan Rashidi-Tabrizi and Rouzbahan Rashidi-Tabrizi and Edna Braun and Edna Braun and Mohammad Alhaj and Mohammad Alhaj and Gregory Richards and Gregory Richards},doi={10.1109/relaw.2013.6671340},pmid={null},pmcid={null},mag_id={2089969903},journal={null},abstract={Regulations are a source of evolving requirements for products and organizations. As regulatory institutions shift towards outcome-based regulations, they increasingly adopt legislation performance modeling, at the basis of regulatory intelligence. In this context, performance modeling refers to the measuring of important business aspects in a coordinated manner and the use of these measurements for improved decision making. Considering that in many cases regulatory texts already exist, it is necessary to build a performance model based on existing regulations that may be still prescriptive rather than outcome-based. The process of turning the underlying textual legislation into a formal performance model that can be assessed by Business Intelligence (BI) tools is complicated due to organizational, cultural, and technological reasons. In this paper, we present a methodology from a technical perspective that enables regulatory institutions to reason about regulations and compliance with regulations as new dimensions. We demonstrate the methodology using traffic law as an example regulation, jUCMNav for performance modeling, and IBM Cognos for BI reporting.}}
@ARTICLE{Sartor_2010,title={Doing justice to rights and values: teleological reasoning and proportionality},year={2010},author={Giovanni Sartor and Giovanni Sartor},doi={10.1007/s10506-010-9095-7},pmid={null},pmcid={null},mag_id={2092703685},journal={Artificial Intelligence and Law},abstract={This paper studies how legal choices, and in particular legislative determinations, need to consider multiple rights and values, and can be assessed accordingly. First it is argued that legal norms (and in particular constitutional right-norms) often prescribe the pursuit of goals, which may be in conflict one with another. Then a model of teleological reasoning is brought to bear on choices affecting different goals, among which those prescribed by constitutional norms. An analytical framework is provided for evaluating such choices with regard to possible alternatives. The assessment of legislative choices according to proportionality is then considered, and is modelled using the provided analytical framework. Finally, the framework is expanded to include the ideas of reasonableness and institutional deference, and the corresponding margins of appreciation.}}
@ARTICLE{Bench‐Capon_2001,title={Back to the Future: Dimensions Revisited},year={2001},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Edwina L. Rissland and Edwina L. Rissland},doi={null},pmid={null},pmcid={null},mag_id={2096182694},journal={null},abstract={Most recent work on reasoning with cases in law has taken the style of rea- soning used in the CATO system as its model, and uses the notion of factors, as found in that system. Fundamental to CATO, a successor the HYPO system, were factors, which are closely related to HYPO-style dimensions. In this paper, we will argue that the simplification involved in using factors, while it has proved pragmatically useful both for clarifying understanding of certain aspects of reasoning with cases and for implementation, causes problems with domain analysis and precludes certain kinds of argument that we would wish to model. We therefore believe that the time is now ripe to go back to the original notion of dimensions, while retaining the insights that have been gained from working with the simpler notion of factors. The paper uses two case studies to argue that this is so.}}
@ARTICLE{Rifaut_2008,title={Using Goal-Oriented Requirements Engineering for Improving the Quality of ISO/IEC 15504 based Compliance Assessment Frameworks},year={2008},author={André Rifaut and André Rifaut and Éric Dubois and Eric Dubois},doi={10.1109/re.2008.44},pmid={null},pmcid={null},mag_id={2101881914},journal={null},abstract={Within the context of business processes design and deployment we introduce and illustrate the use of goal models for capturing compliance requirements applicable over business processes configurations. More specifically we explain how a goal-oriented approach can be used together with the ISO/IEC 15504 standard in order to provide a formal framework according to which the compliance of business processes against regulations and their associated requirements can be assessed and measured. The overall approach is discussed and illustrated through the handling of a real business case related to the Basel II Accords on operational risk management in the financial sector.}}
@ARTICLE{Verheij_2003,title={Dialectical argumentation with argumentation schemes: an approach to legal logic},year={2003},author={Bart Verheij and Bart Verheij},doi={10.1023/b:arti.0000046008.49443.36},pmid={null},pmcid={null},mag_id={2106536319},journal={Artificial Intelligence and Law},abstract={This paper describes an approach to legal logic based on the formal analysis of argumentation schemes. Argumentation schemes - a notion borrowed from the field of argumentation theory - are a kind of generalized rules of inference, in the sense that they express that given certain premises a particular conclusion can be drawn. However, argumentation schemes need not concern strict, abstract, necessarily valid patterns of reasoning, but can be defeasible, concrete and contingently valid, i.e., valid in certain contexts or under certain circumstances. A method is presented to analyze argumentation schemes and it is shown how argumentation schemes can be embedded in a formal model of dialectical argumentation. The approach also provides insight into the role of critical questions.}}
@ARTICLE{Ashley_2003,title={A Predictive Role for Intermediate Legal Concepts},year={2003},author={Kevin D. Ashley and Kevin D. Ashley and Stefanie Br and Stefanie Br},doi={null},pmid={null},pmcid={null},mag_id={2107025559},journal={null},abstract={Experiments described here demonstrate a role that intermediate legal con- cepts play in predicting the decisions of new cases. The experiments compare vari- ations of each of an Issue-Based Prediction algorithm (IBP) and a CATO prediction algorithm in which the use of legal concepts are ablated in various ways. The results confirm those legal philosophical theories that assert that intermediate concepts in le- gal principles perform a guiding or extending role in deciding new cases.}}
@ARTICLE{Governatori_2008,title={Detecting Regulatory Compliance for Business Process Models through Semantic Annotations},year={2008},author={Guido Governatori and Guido Governatori and Jörg Hoffmann and Jörg Hoffmann and Shazia Sadiq and Shazia Sadiq and Ingo Weber and Ingo Weber},doi={10.1007/978-3-642-00328-8_2},pmid={null},pmcid={null},mag_id={2109550417},journal={null},abstract={A given business process may face a large number of regulatory obligations the process may or comply with. Providing tools and techniques to evaluate the compliance degree of a given process is a key objective in emerging business process platforms. We propose a diagnostic framework to assess the compliance gaps present in a given process. Checking whether a process is compliant with the rules involves enumerating all reachable states and is hence, in general, a hard search problem. The approach taken here allows to provide useful diagnostic information in polynomial time based on two underlying techniques. A conceptually faithful representation for regulatory obligations is firstly provided by a formal rule language based on a non-monotonic deontic logic of violations. Secondly, processes are formalized through semantic annotations that allow a logical state space to be created. The intersection of the two allows us to devise an efficient method to detect compliance gaps.}}
@ARTICLE{Governatori_2005,title={Temporalised normative positions in defeasible logic},year={2005},author={Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Giovanni Sartor and Giovanni Sartor},doi={10.1145/1165485.1165490},pmid={null},pmcid={null},mag_id={2109634618},journal={null},abstract={We propose a computationally oriented non-monotonic multi-modal logic arising from the combination of temporalised agency and temporalised normative positions. We argue about the defeasible nature of these notions and then we show how to represent and reason with them in the setting of Defeasible Logic.}}
@ARTICLE{Siena_2009,title={Towards a framework for law-compliant software requirements},year={2009},author={Alberto Siena and Alberto Siena and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi and John Mylopoulos and John Mylopoulos},doi={10.1109/icse-companion.2009.5070994},pmid={null},pmcid={null},mag_id={2109987863},journal={null},abstract={During the requirements elicitation phase, analysts have often to take into consideration laws and regulations enacted by different levels of government. The purpose of this paper is twofold. First, a systematic process is outlined which, given a problem and a collection of legal prescriptions, generates a set of requirements that address the problem while complying with the prescriptions. Second, the conceptual framework is outlined, which characterises the process by providing both legal concepts proposed in theoretical studies in the legal domain and concepts from goal-oriented requirements engineering. The issues and challenges of the proposed framework are also evaluated, with regard to expected results.}}
@ARTICLE{Zave_1997,title={Four dark corners of requirements engineering},year={1997},author={Pamela Zave and Pamela Zave and Michael Jackson and Michael Jackson},doi={10.1145/237432.237434},pmid={null},pmcid={null},mag_id={2113435553},journal={ACM Transactions on Software Engineering and Methodology},abstract={Research in requirements engineering has produced an extensive body of knowledge, but there are four areas in which the foundation of the discipline seems weak or obscure. This article shines some light in the “four dark corners,” exposing problems and proposing solutions. We show that all descriptions involved in requirements engineering should be descriptions of the environment. We show that certain control information is necessary for sound requirements engineering, and we explain the close association between domain knowledge and refinement of requirements. Together these conclusions explain the precise nature of requirements, specifications, and domain knowledge, as well as the precise nature of the relationships among them. They establish minimum standards for what  information should be represented in a requirements language. They also make it possible to determine exactly what it means for requirements and engineering to be successfully completed.}}
@ARTICLE{Valente_1995,title={ON-LINE: an architecture for modelling legal information},year={1995},author={André Valente and Andre Valente and Joost Breuker and Joost Breuker},doi={10.1145/222092.222265},pmid={null},pmcid={null},mag_id={2118127527},journal={null},abstract={This paper describes ON-LINE (ONtology-based Legal Information Environment), an architecture for a legal workbench which combines two major functions: legal information serving and legal analysis. Some of the main features of ON-LINE are: the integrated storage and representation of legal text and knowledge by using interconnected knowledge aud text repositories; a representation of legal knowledge based on a functional ontology of law; the emphssis on legal modelling as a central task in legal practice. ON-LINE comprises three main modules. The Legal Information Server is able to retrieve legal information baaed on either textnal or conceptual search. The Legal Information ModelZing Toolkit is a collection of integrated tools to transform legaJ text into legal knowledge. The Legal Analysis Environment contains reasoning tools to perform two of the central legal tasks: assessment and planning. The architecture is intended to be a basis for experimentation, and it is therefore highly extensible. ONLINE is partially implemented in Common Lisp and it is supported by the LOOM system.}}
@ARTICLE{Lau_2005,title={Legal information retrieval and application to e-rulemaking},year={2005},author={Gloria T. Lau and Gloria T. Lau and Kincho H. Law and Kincho H. Law and Gio Wiederhold and Gio Wiederhold},doi={10.1145/1165485.1165508},pmid={null},pmcid={null},mag_id={2118769224},journal={null},abstract={The complexity and diversity of government regulations make understanding the regulations a non-trivial task. One of the issues is the existence of multiple sources of regulations and interpretive guides; the latter are often independent of governing bodies. This work aims to develop an information infrastructure for legal information retrieval with applications to electronic-rulemaking. The pilot study focuses on accessibility regulations from the US Federal government, private organizations and European agencies. A shallow parser is developed to consolidate different regulations into a unified XML format, which is well suited for handling semi-structured data such as legal documents. Handcrafted rules and a text mining tool are developed to extract the important features, such as concepts, measurements, effective dates and so on, and to incorporate them into the corpus.To compare and locate related provisions from different regulatory documents, we employ Information Retrieval techniques to combine generic features with domain knowledge. Structural information from regulations, such as the hierarchical organization of provisions and heavy referencing among provisions, are used to help improve the relatedness analysis. Results are obtained to illustrate the use of regulatory structure and domain knowledge in provision comparisons. Application to an e-rulemaking scenario for a rights-of-way drafted regulation is shown to demonstrate extended capabilities of the prototype system.}}
@ARTICLE{Weber-Jahnke_2009,title={Finding Defects in Natural Language Confidentiality Requirements},year={2009},author={Jens H. Weber-Jahnke and Jens H. Weber-Jahnke and A. Onabajo and Adeniyi Onabajo},doi={10.1109/re.2009.41},pmid={null},pmcid={null},mag_id={2119027445},journal={null},abstract={Large-scale software systems must adhere to complex, multi-lateral security and privacy requirements from regulations. It is industrial practice to define such requirements in form of natural language (NL) documents. Currently existing approaches to analyzing NL confidentiality requirements rely on a manual linguistic transformation and normalization of the original text, prior to the analysis. This paper presents an alternative approach to analyzing requirements by using semantic annotations placed directly into the original NL documents. The benefits of this alternative approach are twofold: (1) it can effectively be supported by an interactive annotation tool and (2) there is a direct traceability between annotation structures and the original NL documents. We have evaluated our method and tool support using the same real-world case study that was used to evaluate the earlier linguistic approach. Our results show that our method generates the same results, i.e., it uncovers the same problems.}}
@ARTICLE{Young_2011,title={Commitment analysis to operationalize software requirements from privacy policies},year={2011},author={Jessica D. Young and Jessica D. Young},doi={10.1007/s00766-010-0108-6},pmid={null},pmcid={null},mag_id={2125689148},journal={Requirements Engineering},abstract={Online privacy policies describe organizations’ privacy practices for collecting, storing, using, and protecting consumers’ personal information. Users need to understand these policies in order to know how their personal information is being collected, stored, used, and protected. Organizations need to ensure that the commitments they express in their privacy policies reflect their actual business practices, especially in the United States where the Federal Trade Commission regulates fair business practices. Requirements engineers need to understand the privacy policies to know the privacy practices with which the software must comply and to ensure that the commitments expressed in these privacy policies are incorporated into the software requirements. In this paper, we present a methodology for obtaining requirements from privacy policies based on our theory of commitments, privileges, and rights, which was developed through a grounded theory approach. This methodology was developed from a case study in which we derived software requirements from seventeen healthcare privacy policies. We found that legal-based approaches do not provide sufficient coverage of privacy requirements because privacy policies focus primarily on procedural practices rather than legal practices.}}
@ARTICLE{Allen_1995,title={Better language, better thought, better communication: the A-Hohfeld language for legal analysis},year={1995},author={Layman E. Allen and Layman E. Allen and Charles S. Saxon and Charles S. Saxon},doi={10.1145/222092.222245},pmid={null},pmcid={null},mag_id={2128312778},journal={null},abstract={A-HOHFELD is a representational language used in MINT (Multiple INTerpretation) Interpretation-Assistance (expert) systems for precisely expressing alternative structural interpretations of sets of legal rules. It draws heavily upon the timlamental legal conceptions formulated by Wesley N. HoMeld at the dawn of the Twentieth Century. In the current version of AHOHFELD the original conceptions have been modified and extended in seeking to define a language sufficiently robust to express all LEGAL RELATIONS and all changes in legal states of affairs. Hohfeld emphasized the use of fundamental legal conceptions in the analysis of judicial reasoning, elsewhere we have shown the use of A-HOHFELD for the analysis of sets of statutory rules, and here we illustrate its use in thinking about legal doctrine. This use of A-HOHFELD is offered as a possible example of where fluency in a more precise and complete language might have facilitated an earlier recognition of remedial alternatives that have apparently only recently been appearing in legal literature and judicial decisions. To the extent that AHOHFELD so strengthens legal analysis, it farther exemplifies how work on problems of artificial intelligence in computer science tiuittidly feeds back to law and illustrates how precision in language contributes to thought as well as communication.}}
@ARTICLE{Kowalski_1989,title={The treatment of negation in logic programs for representing legislation},year={1989},author={Robert Kowalski and Robert A. Kowalski},doi={10.1145/74014.74016},pmid={null},pmcid={null},mag_id={2132691907},journal={null},abstract={Logic programs represent knowledge in the form of  implications  A if B 1  and … B n , n ≥ 0 where the  conclusion  A is an atomic formula and each  condition  B i  is either an atomic formula or the negation of an atomic formula. Any variables are assumed to be universally quantified, with a scope which is the entire sentence. A negated condition “not A i ” is deemed to hold if the corresponding positive condition A i  can be shown to fail to hold. This interpretation of negative conditions is called negation by failure (NBF) [Cl 78]. It has the characteristic that only the positive “if-half” of a definition needs to be given explicity. The negative “only-if” half is given implicitly by NBF.  The obvious problem with NBF is that it supplies the only-if halves of implications, whether or not they are intended. I shall discuss a possible solution to this problem in the context of discussing the more general problem of representing negative conclusions. I shall focus on examples taken from our formalisation of the 1981 British Nationality Act (BNA) [SSKKHC 86]. I shall argue that many negative sentences can be regarded as integrity constraints and consequently can be eliminated by transformations such as those developed by Asirelli et al [ASM 85] and Kowalski and Sadri [KS 88]. Among such sentences are ones expressing prohibitions. The interpretation of prohibitions as integrity constraints suggests a possible approach to the treatment of deontic modalities.}}
@ARTICLE{Fleiss_1973,title={The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability.},year={1973},author={Joseph L. Fleiss and Joseph L. Fleiss and Jacob Cohen and Jacob Cohen},doi={10.1177/001316447303300309},pmid={null},pmcid={null},mag_id={2133012565},journal={Educational and Psychological Measurement},abstract={null}}
@ARTICLE{Breaux_2005,title={Mining rule semantics to understand legislative compliance},year={2005},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Annie I. Antón},doi={10.1145/1102199.1102210},pmid={null},pmcid={null},mag_id={2139647326},journal={null},abstract={Organizations in privacy-regulated industries (e.g. healthcare and financial institutions) face significant challenges when developing policies and systems that are properly aligned with relevant privacy legislation. We analyze privacy regulations derived from the Health Insurance Portability and Accountability Act (HIPAA) that affect information sharing practices and consumer privacy in healthcare systems. Our analysis shows specific natural language semantics that formally characterize rights, obligations, and the meaningful relationships between them required to build value into systems. Furthermore, we evaluate semantics for rules and constraints necessary to develop machine-enforceable policies that bridge between laws, policies, practices, and system requirements. We believe the results of our analysis will benefit legislators, regulators and policy and system developers by focusing their attention on natural language policy semantics that are implementable in software systems.}}
@ARTICLE{Atkinson_2007,title={Argumentation and standards of proof},year={2007},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/1276318.1276339},pmid={null},pmcid={null},mag_id={2141322619},journal={null},abstract={In this paper we examine some previous AI and Law attempts to characterise standards of proof, and relate these to the notions of acceptability found in argumentation frameworks, an approach which forms the basis of much recent work on argumentation. We distinguish between the justification of facts and the justication of choices relating to the law and its interpretation. Standards of proof most naturally arise in connection with facts, but points of law have analogous degrees of justification.}}
@ARTICLE{Hassan_2013,title={Towards a process for legally compliant software},year={2013},author={Waël Hassan and Waël Hassan and Luigi Logrippo and Luigi Logrippo},doi={10.1109/relaw.2013.6671345},pmid={null},pmcid={null},mag_id={2141836776},journal={null},abstract={We propose a method and a process for legal software requirements extraction and compliance checking. We describe a requirements extraction model, a set of rules for specifying the format of the extracted information, a set of UML-based principles for translating the extracted information into a language based on predicate logic, and finally, a tool that analyzes the resulting logic model and displays the results of the analysis. The translation principles are based on a Governance Analysis Model (GAM) which is described in UML; the language is our Governance Analysis Language (GAL) and the tool is our Governance Analysis Tool (GAT). MIT's logic analyzer Alloy is the engine on which GAT runs. GAL is translated into assertions in Alloy's language and the Alloy tool can find counterexamples indicating situations of non-compliance.}}
@ARTICLE{Prakken_2008,title={A formal model of adjudication dialogues},year={2008},author={Henry Prakken and Henry Prakken},doi={10.1007/s10506-008-9066-4},pmid={null},pmcid={null},mag_id={2143314283},journal={Artificial Intelligence and Law},abstract={This article presents a formal dialogue game for adjudication dialogues. Existing AI & law models of legal dialogues and argumentation-theoretic models of persuasion are extended with a neutral third party, to give a more realistic account of the adjudicator's role in legal procedures. The main feature of the model is a division into an argumentation phase, where the adversaries plea their case and the adjudicator has a largely mediating role, and a decision phase, where the adjudicator decides the dispute on the basis of the claims, arguments and evidence put forward in the argumentation phase. The model allows for explicit decisions on admissibility of evidence and burden of proof by the adjudicator in the argumentation phase. Adjudication is modelled as putting forward arguments, in particular undercutting and priority arguments, in the decision phase. The model reconciles logical aspects of burden of proof induced by the defeasible nature of arguments with dialogical aspects of burden of proof as something that can be allocated by explicit decisions on legal grounds.}}
@ARTICLE{Kharbili_2011,title={Enterprise Regulatory Compliance Modeling Using CoReL: An Illustrative Example},year={2011},author={Marwane El Kharbili and Marwane El Kharbili and Qin Ma and Qin Ma and Pierre Kelsen and Pierre Kelsen and Elke Pulvermueller and Elke Pulvermueller},doi={10.1109/cec.2011.39},pmid={null},pmcid={null},mag_id={2146251792},journal={null},abstract={Regulatory compliance management is a critical and challenging task, especially in the context of Business Process Management. It requires a comprehensive framework for dealing with compliance requirements: elicitation, modeling, static and dynamic checking and reporting. We previously defined CoReL, a domain specific language for the domain of compliance decision-making. This paper shows how CoReL can be used to model compliance requirements using an illustrative example. In particular, we show how CoReL's agnosticism of logical formalisms and coverage of enterprise business aspects leverages the task of compliance modeling to the business user level.}}
@ARTICLE{Shamsaei_2012,title={An approach to specify and analyze goal model families},year={2012},author={Azalia Shamsaei and Azalia Shamsaei and Daniel Amyot and Daniel Amyot and Alireza Pourshahid and Alireza Pourshahid and Edna Braun and Edna Braun and Eric Yu and Eric Yu and Gunter Mussbacher and Gunter Mussbacher and Rasha Tawhid and Rasha Tawhid and Nick Cartwright and Nick Cartwright},doi={10.1007/978-3-642-36757-1_3},pmid={null},pmcid={null},mag_id={2151317734},journal={null},abstract={Goal-oriented languages have been used for years to model and reason about functional, non-functional, and legal requirements. It is however difficult to develop and maintain these models, especially when many models overlap with each other. This becomes an even bigger challenge when a single, generic model is used to capture a family of related goal models but different evaluations are required for each individual family member. In this work, we use ITU-T's Goal-oriented Requirement Language (GRL) and the jUCMNav tool to illustrate the problem and to formulate a solution that exploits the flexibility of standard GRL. In addition, we report on our recent experience on the modeling of aerodrome regulations. We demonstrate the usefulness of specifying families of goal models to address challenges associated with the maintenance of models used in the regulatory domain. We finally define and illustrate a new tool-supported algorithm used to evaluate individual goal models that are members of the larger family model.}}
@ARTICLE{Goedertier_2006,title={Designing compliant business processes with obligations and permissions},year={2006},author={Stijn Goedertier and Stijn Goedertier and Jan Vanthienen and Jan Vanthienen},doi={10.1007/11837862_2},pmid={null},pmcid={null},mag_id={2151737601},journal={null},abstract={The sequence and timing constraints on the activities in business processes are an important aspect of business process compliance. To date, these constraints are most often implicitly transcribed into control-flow-based process models. This implicit representation of constraints, however, complicates the verification, validation and reuse in business process design. In this paper, we investigate the use of temporal deontic assignments on activities as a means to declaratively capture the control-flow semantics that reside in business regulations and business policies. In particular, we introduce PENELOPE, a language to express temporal rules about the obligations and permissions in a business interaction, and an algorithm to generate compliant sequence-flow-based process models that can be used in business process design.}}
@ARTICLE{Loui_1993,title={A design for reasoning with policies, precedents, and rationales},year={1993},author={Ronald P. Loui and Ronald P. Loui and Jeff Norman and Jeff Norman and Jarrod Olson and Jon Olson and Andrew S. Merrill and Andrew S. Merrill},doi={10.1145/158976.159002},pmid={null},pmcid={null},mag_id={2152325153},journal={null},abstract={The studies of Tom Gordon [Gordon87, Gordon89, Gordon93a] and Henry Prakken ~akken91, Prakken93a, Prakken93b], together with the work of Rissland and Ashley (e.g., [Ashley89a], [Ashley&Aleven91], lJ@.sland&Skalak9 1]) show an interest in models of reasoning that directly address the concepts of argument, defeat among arguments, and dialectical processes through which arguments provide warrant for conclusions. This interest is distinct from the longstanding interest, among legal scholars, in models of reasoning based on relevance, deorttic, and intuitionistic logics.}}
@ARTICLE{Islam_2010,title={Towards a Framework to Elicit and Manage Security and Privacy Requirements from Laws and Regulations},year={2010},author={Shareeful Islam and Shareeful Islam and Haralambos Mouratidis and Haralambos Mouratidis and Stefan Wagner and Stefan Wagner},doi={10.1007/978-3-642-14192-8_23},pmid={null},pmcid={null},mag_id={2153064832},journal={null},abstract={[Context and motivation] The increasing demand of software systems to process and manage sensitive information has led to the need that software systems should comply with relevant laws and regulations, which enforce the privacy and other aspects of the stored information. [Question/problem] However, the task is challenging because concepts and terminology used for requirements engineering are mostly different to those used in the legal domain and there is a lack of appropriate modelling languages and techniques to support such activities. [Principal ideas/results] The legislation need to be analysed and align with the system requirements. [Contribution] This paper motivates the need to introduce a framework to assist the elicitation and management of security and privacy requirements from relevant legislation and it briefly presents the foundations of such a framework along with an example.}}
@ARTICLE{Carletta_1996,title={Assessing agreement on classification tasks: the kappa statistic},year={1996},author={Jean Carletta and Jean Carletta},doi={null},pmid={null},pmcid={null},mag_id={2153804780},journal={Computational Linguistics},abstract={Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.}}
@ARTICLE{Антон_2004,title={A requirements taxonomy for reducing Web site privacy vulnerabilities},year={2004},author={Юлов Антон and I. Antón and Brandon E. Earp and B. Earp},doi={10.1007/s00766-003-0183-z},pmid={null},pmcid={null},mag_id={2154790136},journal={Requirements Engineering},abstract={The increasing use of personal information on Web-based applications can result in unexpected disclosures. Consumers often have only the stated Web site policies as a guide to how their information is used, and thus on which to base their browsing and transaction decisions. However, each policy is different, and it is difficult—if not impossible—for the average user to compare and comprehend these policies. This paper presents a taxonomy of privacy requirements for Web sites. Using goal-mining, the extraction of pre-requirements goals from post-requirements text artefacts, we analysed an initial set of Internet privacy policies to develop the taxonomy. This taxonomy was then validated during a second goal extraction exercise, involving privacy policies from a range of health care related Web sites. This validation effort enabled further refinement to the taxonomy, culminating in two classes of privacy requirements: protection goals and vulnerabilities. Protection goals express the desired protection of consumer privacy rights, whereas vulnerabilities describe requirements that potentially threaten consumer privacy. The identified taxonomy categories are useful for analysing implicit internal conflicts within privacy policies, the corresponding Web sites, and their manner of operation. These categories can be used by Web site designers to reduce Web site privacy vulnerabilities and ensure that their stated and actual policies are consistent with each other. The same categories can be used by customers to evaluate and understand policies and their limitations. Additionally, the policies have potential use by third-party evaluators of site policies and conflicts.}}
@ARTICLE{Verheij_2003,title={DefLog: on the Logical Interpretation of Prima Facie Justified Assumptions},year={2003},author={Bart Verheij and Bart Verheij},doi={10.1093/logcom/13.3.319},pmid={null},pmcid={null},mag_id={2156156961},journal={Journal of Logic and Computation},abstract={Assumptions are often not considered to be definitely true, but only as prima facie justified. When an assumption is prima facie justified, there can for instance be a reason against it, by which the assumption is not actually justified. The assumption is then said to be defeated. This requires a revision of the standard conception of logical interpretation of sets of assumptions in terms of their models. Whereas in the models of a set of assumptions, all assumptions are taken to be true, an interpretation of prima facie justified assumptions must distinguish between the assumptions that are actually justified in the interpretation and those that are defeated. In the present paper, the logical interpretation of prima facie justified assumptions is investigated. The central notion is that of a dialectical interpretation of a set of assumptions. The basic idea is that a prima facie justified assumption is not actually justified, but defeated when its so-called dialectical negation is justified. The properties of dialectical interpretation are analysed by considering partial dialectical interpretations, or stages, and by establishing the notion of dialectical justification. The latter leads to a characterization of the existence and multiplicity of the dialectical interpretations of a set of assumptions. Since dialectical interpretations are a variant of stable semantics, the results are relevant for existing work on nonmonotonic logic and defeasible reasoning, on which the present work builds. Instead of focusing on defeasible rules or arguments, the present approach is sentence-based. A particular innovation is the use of a conditional that is prima facie justified (just like other assumptions) instead of an inconclusive conditional.}}
@ARTICLE{Emmerich_1999,title={Managing standards compliance},year={1999},author={Wolfgang Emmerich and Wolfgang Emmerich and Anthony Finkelstein and Anthony Finkelstein and Carlo Montangero and C. Montangero and Sebastian Antonelli and S. Antonelli and Stephen Armitage and S. Armitage and Richard Stevens and Richard Stevens},doi={10.1109/32.824413},pmid={null},pmcid={null},mag_id={2157855401},journal={IEEE Transactions on Software Engineering},abstract={Software engineering standards determine practices that "compliant" software processes shall follow. Standards generally define practices in terms of constraints that must hold for documents. The document types identified by standards include typical development products, such as user requirements, and also process-oriented documents, such as progress reviews and management reports. The degree of standards compliance can be established by checking these documents against the constraints. It is neither practical nor desirable to enforce compliance at all points in the development process. Thus, compliance must be managed rather than imposed. We outline a model of standards and compliance and illustrate it with some examples. We give a brief account of the notations and method we have developed to support the use of the model and describe a support environment we have constructed. The principal contributions of our work are: the identification of the issue of standards compliance; the development of a model of standards and support for compliance management; the development of a formal model of product state with associated notation; a powerful policy scheme that triggers checks; and a flexible and scalable compliance management view.}}
@ARTICLE{Ghanavati_2014,title={Compliance with Multiple Regulations},year={2014},author={Sepideh Ghanavati and Sepideh Ghanavati and Llio Humphreys and Llio Humphreys and Guido Boella and Guido Boella and Guido Boella and Guido Boella and Luigi Di and Luigi Di Caro and Livio Robaldo and Livio Robaldo and Leendert van der Torre and Leendert van der Torre},doi={10.1007/978-3-319-12206-9_35},pmid={null},pmcid={null},mag_id={2158585249},journal={null},abstract={With an increase in regulations, it is challenging for organizations to identify relevant regulations and ensure that their business processes comply with legal provisions. Multiple regulations cover the same domain and can interact with, complement or contradict each other. To overcome these challenges, a systematic approach is required. This paper proposes a thorough approach integrating the Eunomos knowledge and document management system with Legal-URN framework, a Requirements Engineering based framework for business process compliance).}}
@ARTICLE{Robinson_1994,title={Supporting multi-perspective requirements engineering},year={1994},author={William Robinson and William N. Robinson and Stephen Fickas and Stephen Fickas},doi={10.1109/icre.1994.292383},pmid={null},pmcid={null},mag_id={2160287154},journal={null},abstract={Supporting collaborating requirements engineers as they independently construct a specification is highly desirable. We show how collaborative requirements engineering can be supported using a planner, domain abstractions, and automated decision science techniques. In particular we show how requirements conflict resolution can be assisted through a combination of multi-agent multi-criteria optimization and heuristic resolution generation. We then summarize the use of our tool to rationally reconstruct a library specification. This line of research is significant in that it brings conflict detection and resolution into a requirements engineering framework. This particular work expands the automation found in previous results (W. Robinson, 1993). >}}
@ARTICLE{Horty_2004,title={THE RESULT MODEL OF PRECEDENT},year={2004},author={John F. Horty and John F. Horty},doi={10.1017/s1352325204000151},pmid={null},pmcid={null},mag_id={2160361145},journal={Legal Theory},abstract={The result model of precedent holds that a legal precedent controls a fortiori cases— those cases, that is, that are at least as strong for the winning side of the precedent as the precedent case itself. This paper defends the result model against some objections by Larry Alexander, drawing on ideas from the field of Artificial Intelligence and Law in order to define an appropriate strength ordering for cases.}}
@ARTICLE{Thurimella_2007,title={Evolution in Product Line Requirements Engineering: A Rationale Management Approach},year={2007},author={Anil Kumar Thurimella and A.K. Thurimella and Bernd Bruegge and B. Bruegge},doi={10.1109/re.2007.11},pmid={null},pmcid={null},mag_id={2160519824},journal={null},abstract={Product line evolution is one of the burning issues in product line requirements engineering. It is more complicated than single system requirements engineering because of the conflicting requirements across the product lines, the effect of evolution on the reuse and customization, and the global distribution of product line organizations. As product lines are long-term investments, handling their evolution is critical. This paper proposes a new technique called rationale-based product line evolution. It is based on the Questions, Options and Criteria model and a modified version of EasyWinWin. The technique allows the evaluation of change requests in product line requirements based on the informal collaboration of stakeholders, capturing the forces that cause evolution and using them for the justification of change requests. The technique has been implemented in the Sysiphus tool and was validated using empirical evaluation.}}
@ARTICLE{Kitchenham_2002,title={Preliminary guidelines for empirical research in software engineering},year={2002},author={Barbara Kitchenham and Barbara Kitchenham and Shari Lawrence Pfleeger and Shari Lawrence Pfleeger and Lesley M. Pickard and Lesley M. Pickard and Peter W. Jones and Peter W. Jones and Peter Jones and Peter W. Jones and Peter G. Jones and David C. Hoaglin and D.C. Hoaglin and Khaled El Emam and K. El Emam and Jarrett Rosenberg and J. Rosenberg},doi={10.1109/tse.2002.1027796},pmid={null},pmcid={null},mag_id={2163851162},journal={IEEE Transactions on Software Engineering},abstract={Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.}}
@ARTICLE{Perry_2000,title={Empirical studies of software engineering: a roadmap},year={2000},author={Dewayne E. Perry and Dewayne E. Perry and Adam Porter and Adam Porter and Lawrence G. Votta and Lawrence G. Votta},doi={10.1145/336512.336586},pmid={null},pmcid={null},mag_id={2165043113},journal={null},abstract={In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.}}
@ARTICLE{Rissland_1995,title={A hybrid CBR-IR approach to legal information retrieval},year={1995},author={Edwina L. Rissland and Edwina L. Rissland and Jody J. Daniels and Jody J. Daniels},doi={10.1145/222092.222125},pmid={null},pmcid={null},mag_id={2165120645},journal={null},abstract={In this paper we discuss a hybrid approach combining CaseBased Reasoning (CBR) and Information Retrieval (IR) for the retrieval of legal documents. Our hybrid CBR-IR approach takes as input a standard symbolic representation of a problem ease and retrieves texts of relevant cases from a document corpus dramatically larger than the case base available to the CBR system. Our system works by first performing a standard HYPO-style CBR anatysis and then using texts associated with certain important classes of cases found in this analysis to “seed” a modified version of INQUERY’s relevance feedback mechanism in order to generate a query. Our approach provides two benefits: it extends the reach of CBR (for retrievat purposes) to much larger corpora, and it enables the injection of knowledgebased techniques into traditional IR. We deseribe our CBRIR approach and report on on-going experiments performed in two different legal domains.}}
@ARTICLE{Ghanavati_2010,title={Integrating business strategies with requirement models of legal compliance},year={2010},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton and Alberto Siena and Alberto Siena and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi and Angelo Susi and Angelo Susi},doi={10.1504/ijeb.2010.034171},pmid={null},pmcid={null},mag_id={2166442019},journal={International Journal of Electronic Business},abstract={Goal-oriented requirements engineering uses modelling to improve domain understanding and requirements quality. Regulations and laws impose additional context and constraints on goals and can limit the satisfaction of stakeholder needs. Organisations and software developers need modelling tools to assess the degree to which business strategies are effective in ensuring that business goals are met while complying with applicable laws and regulations. In this paper, we analyse the capabilities of the Goal-oriented Requirement Language (GRL) in supporting such assessments. Our analysis is based on four scenarios involving healthcare business processes and compliance with the Health Insurance Portability and Accountability Act.}}
@ARTICLE{Hamdaqa_2009,title={Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents},year={2009},author={Mohammad Hamdaqa and Mohammad Hamdaqa and Abdelwahab Hamou-Lhadj and Abdelwahab Hamou-Lhadj},doi={10.1109/itng.2009.161},pmid={null},pmcid={null},mag_id={2169037632},journal={null},abstract={Regulated companies are required to comply with the many laws, regulations, standards, and guidelines that apply to them. The sheer volume of regulatory compliance requirements for even a small company can be considerably high, which renders the understanding of such authoritative rules a challenging task without tool support. After inspecting several regulatory documents, we noticed that they contain a significant number of citations that, if explored effectively, can reveal important information about the containing documents. In this paper, we propose a technique called citation analysis that aims at helping users to understand and analyze regulatory documents in an efficient manner. Our approach is based on the exploration of citation graphs extracted from regulatory documents. We discuss the challenges when dealing with citations. We also present an overview of a tool that can support citation analysis.}}
@ARTICLE{Horty_2011,title={RULES AND REASONS IN THE THEORY OF PRECEDENT},year={2011},author={John F. Horty and John F. Horty},doi={10.1017/s1352325211000036},pmid={null},pmcid={null},mag_id={2170244904},journal={Legal Theory},abstract={The doctrine of precedent, as it has evolved within the common law, has at its heart a form of reasoning—broadly speaking, a logic—according to which the decisions of earlier courts in particular cases somehow generalize to constrain the decisions of later courts facing different cases, while still allowing these later courts a degree of freedom in responding to fresh circumstances. Although the techniques for arguing on the basis of precedent are taught early on in law schools, mastered with relative ease, and applied on a daily basis by legal practitioners, it has proved to be considerably more difficult to arrive at a theoretical understanding of the doctrine itself—a clear articulation of the underlying logic. My purpose in this paper is to describe a new framework within which we can begin to address this problem. I concentrate on two of the most fundamental questions in the theory of precedent. First, how is it, exactly, that precedent cases constrain future decisions—what is the mechanism of constraint? And second, how is a balance then achieved between the constraints of precedent and the freedoms allowed to later courts for developing the law? The view I present will be contrasted with three other views, or models, of precedential constraint appearing in the literature. The first is the rule model. A precedent case normally contains, not only a description of the facts of the case along with a decision on the basis of those facts, but also some particular rule through which that decision was reached. According to the rule model, it is this rule that carries the precedential constraint. Constraint by precedent just is constraint by rules; a precedent case constrains the decision of a later court when the rule contained in that precedent applies to the fact situation confronting the later court.}}
@ARTICLE{Boehm_1981,title={Software Engineering Economics},year={1981},author={Barry Boehm and Barry Boehm},doi={null},pmid={null},pmcid={null},mag_id={2171816001},journal={null},abstract={This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.}}
@ARTICLE{Hage_1997,title={Reasoning with Rules},year={1997},author={Jaap Hage and Jaap Hage},doi={10.1007/978-94-015-8873-7_3},pmid={null},pmcid={null},mag_id={2238529471},journal={null},abstract={According to the semantic notion of logical validity, an argument is valid if the truth of its premises guarantees the truth of its conclusion. In chapter I, I tried to make it plausible that this semantic notion of validity is not very suitable if we are dealing with rules. The elaborate discussion in chapter II of reasons and their relation to principles, goals and rules, should have substantiated this claim.}}
@ARTICLE{Walton_2016,title={An argumentation framework for contested cases of statutory interpretation},year={2016},author={Douglas Walton and Douglas Walton and Giovanni Sartor and Giovanni Sartor and Fabrizio Macagno and Fabrizio Macagno},doi={10.1007/s10506-016-9179-0},pmid={null},pmcid={null},mag_id={2307562718},journal={Artificial Intelligence and Law},abstract={This paper proposes an argumentation-based procedure for legal interpretation, by reinterpreting the traditional canons of textual interpretation in terms of argumentation schemes, which are then classified, formalized, and represented through argument visualization and evaluation tools. The problem of statutory interpretation is framed as one of weighing contested interpretations as pro and con arguments. The paper builds an interpretation procedure by formulating a set of argumentation schemes that can be used to comparatively evaluate the types of arguments used in cases of contested statutory interpretation in law. A simplified version of the Carneades Argumentation System is applied in a case analysis showing how the procedure works. A logical model for statutory interpretation is finally presented, covering pro-tanto and all-things-considered interpretive conclusions.}}
@ARTICLE{Al-Abdulkarim_2016,title={A methodology for designing systems to reason with legal cases using Abstract Dialectical Frameworks},year={2016},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-016-9178-1},pmid={null},pmcid={null},mag_id={2326194808},journal={Artificial Intelligence and Law},abstract={This paper presents a methodology to design and implement programs intended to decide cases, described as sets of factors, according to a theory of a particular domain based on a set of precedent cases relating to that domain. We use Abstract Dialectical Frameworks (ADFs), a recent development in AI knowledge representation, as the central feature of our design method. ADFs will play a role akin to that played by Entity---Relationship models in the design of database systems. First, we explain how the factor hierarchy of the well-known legal reasoning system CATO can be used to instantiate an ADF for the domain of US Trade Secrets. This is intended to demonstrate the suitability of ADFs for expressing the design of legal cased based systems. The method is then applied to two other legal domains often used in the literature of AI and Law. In each domain, the design is provided by the domain analyst expressing the cases in terms of factors organised into an ADF from which an executable program can be implemented in a straightforward way by taking advantage of the closeness of the acceptance conditions of the ADF to components of an executable program. We evaluate the ease of implementation, the performance and efficacy of the resulting program, ease of refinement of the program and the transparency of the reasoning. This evaluation suggests ways in which factor based systems, which are limited by taking as their starting point the representation of cases as sets of factors and so abstracting away the particular facts, can be extended to address open issues in AI and Law by incorporating the case facts to improve the decision, and by considering justification and reasoning using portion of precedents.}}
@ARTICLE{Bench‐Capon_2015,title={Cases and Stories, Dimensions and Scripts},year={2015},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Floris Bex and Floris Bex},doi={null},pmid={null},pmcid={null},mag_id={2401243629},journal={null},abstract={Stories and legal cases have much in common, but there are also differences. Both can be seen as a sequence of events, but in a legal case the facts and events are legally qualified. Moreover, the point of a story is usually implicit, whereas the outcome of a legal case is explicitly explained. Stories have been mainly used in AI and Law to explore the evidence presented in legal cases, but here we will explore the relationship on the assumption the facts of the case have already been established, and so include legal qualification and the decision. We illustrate our approach the well known wild animals and Popov v Hayashi cases.}}
@ARTICLE{Soltana_2016,title={Model-Based Simulation of Legal Requirements: Experience from Tax Policy Simulation},year={2016},author={Ghanem Soltana and Ghanem Soltana and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand},doi={10.1109/re.2016.11},pmid={null},pmcid={null},mag_id={2461209617},journal={null},abstract={Using models for expressing legal requirements is now commonplace in Requirements Engineering. Models of legal requirements, on the one hand, facilitate communication between software engineers and legal experts, and on the other hand, provide a basis for systematic and automated analysis. The most prevalent application of legal requirements models is for checking the compliance of software systems with laws and regulations. In this experience paper, we explore a complementary application of legal requirements models, namely simulation. We observe that, in domains such as taxation, the same models that underlie legal compliance analysis bring important added value by enabling simulation. Concretely, this paper reports on the model-based simulation of selected legal requirements (policies) derived from Luxembourg's Income Tax Law. The simulation scenario considered in the paper is aimed at analyzing the impact of a current tax law reform proposal in Luxembourg. We describe our approach for simulation along with empirical results demonstrating the feasibility and accuracy of the approach. We further present lessons learned from the experience.}}
@ARTICLE{Storrs_1991,title={GROUP DECISION MAKING},year={1991},author={Graham Storrs and Graham Storrs},doi={10.1016/b978-0-12-086441-6.50022-2},pmid={null},pmcid={null},mag_id={2476518516},journal={null},abstract={null}}
@ARTICLE{Lu_2008,title={Compliance aware business process design},year={2008},author={Ruopeng Lu and Ruopeng Lu and Shazia Sadiq and Shazia Sadiq and Guido Governatori and Guido Governatori},doi={10.1007/978-3-540-78238-4},pmid={null},pmcid={null},mag_id={2481890071},journal={Lecture Notes in Computer Science},abstract={Historically, business process design has been driven by business objectives, specifically process improvement. However this cannot come at the price of control objectives which stem from various legislative, standard and business partnership sources. Ensuring the compliance to regulations and industrial standards is an increasingly important issue in the design of business processes. In this paper, we advocate that control objectives should be addressed at an early stage, i.e., design time, so as to minimize the problems of runtime compliance checking and consequent violations and penalties. To this aim, we propose supporting mechanisms for business process designers. This paper specifically presents a support method which allows the process designer to quantitatively measure the compliance degree of a given process model against a set of control objectives. This will allow process designers to comparatively assess the compliance degree of their design as well as be better informed on the cost of non-compliance.}}
@ARTICLE{Bex_2011,title={Arguments, Stories and Criminal Evidence},year={2011},author={Floris Bex and Floris Bex},doi={10.1007/978-94-007-0140-3},pmid={null},pmcid={null},mag_id={2498346826},journal={null},abstract={In this book a theory of reasoning with evidence in the context of criminal cases is developed. The main subject of this study is not the law of evidence but rather the rational process of proof, whic}}
@ARTICLE{Maxwell_2013,title={Reasoning About Legal Text Evolution for Regulatory Compliance in Software Systems},year={2013},author={Jeremy C. Maxwell and Jeremy C. Maxwell},doi={null},pmid={null},pmcid={null},mag_id={2518792881},journal={null},abstract={null}}
@ARTICLE{Breaux_2006,title={An algorithm to generate compliance monitors from regulations},year={2006},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Ana I. Anton},doi={null},pmid={null},pmcid={null},mag_id={2519403618},journal={null},abstract={Developing software systems in heavily regulated industries requires methods to ensure systems comply with regulations and law. An algorithm to generate finite state machines (FSM) from stakeholder rights and obligations for compliance monitoring is proposed. Rights and obligations define what people are permitted or required to do; these rights and obligations affect software requirements and design. The FSM allows stakeholders, software developers and compliance officers to trace events through the invocation of rights and obligations as preand postconditions. Compliance is monitored by instrumenting runtime systems to report these events and detect violations. Requirements and software engineers specify the rights and obligations, and our algorithm performs three supporting tasks: 1) identify ambiguities, 2) balance rights with obligations, and 3) generate finite state machines. Preliminary validation of the algorithm includes FSMs generated from U.S. healthcare regulations and tool support to parse these specifications and generate the FSMs.}}
@ARTICLE{Maxwell_2010,title={A refined production rule model for aiding in regulatory compliance},year={2010},author={Jeremy C. Maxwell and Jeremy C. Maxwell and Annie I. Antón and Ana I. Anton},doi={null},pmid={null},pmcid={null},mag_id={2522999910},journal={null},abstract={Software engineers are being asked to develop software for increasingly regulated environments. When systems are not dependably compliant, companies must pay the high cost of non-compliance, including the cost of lost reputation and brand damage. Regulations represent the minimum level of security and dependability with which systems must comply. We develop a methodology for creating production rule models to aid developers in specifying legally compliant software requirements. By querying production rule models, software engineers can gain valuable knowledge of the legal text. They can perform an initial compliance analysis and obtain preliminary compliance requirements that can be further refined in consultation with a lawyer. We model the law using the legal concepts of rights, obligations, privileges, no-rights, powers, liabilities, immunities, and disabilities. Herein, we develop heuristics for specifying production rules that model legal texts. We refined our methodology within the context of a case study in which we model the Privacy Rule, Part E, of the Health Insurance Portability and Accountability Act (HIPAA).}}
@ARTICLE{Boehm_1993,title={Software engineering economics},year={1993},author={Barry Boehm and Barry Boehm},doi={null},pmid={null},pmcid={null},mag_id={2551072204},journal={null},abstract={This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.}}
@ARTICLE{Al-Abdulkarim_2016,title={ANGELIC Secrets: Bridging from factors to facts in US trade secrets},year={2016},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2576499168},journal={null},abstract={null}}
@ARTICLE{Bex_2017,title={Introduction to the special issue on Artificial Intelligence for Justice (AI4J)},year={2017},author={Floris Bex and Floris Bex and Henry Prakken and Henry Prakken and Tom van Engers and Tom M. van Engers and Bart Verheij and Bart Verheij},doi={10.1007/s10506-017-9198-5},pmid={null},pmcid={null},mag_id={2594099527},journal={Artificial Intelligence and Law},abstract={null}}
@ARTICLE{Bench‐Capon_2017,title={HYPO’S legacy: introduction to the virtual special issue},year={2017},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-017-9201-1},pmid={null},pmcid={null},mag_id={2620701673},journal={Artificial Intelligence and Law},abstract={This paper is an introduction to a virtual special issue of AI and Law exploring the legacy of the influential HYPO system of Rissland and Ashley. The papers included are: Arguments and cases: An inevitable intertwining, BankXX: Supporting legal arguments through heuristic retrieval, Modelling reasoning with precedents in a formal dialogue Game, A note on dimensions and factors, An empirical investigation of reasoning with legal cases through theory construction and application, Automatically classifying case texts and predicting outcomes, A factor-based definition of precedential constraint and An improved factor based approach to precedential constraint. After describing HYPO, in this introduction to the special issue I look at various aspects of its influence on AI and Law: the developments led by Rissland at Amherst; the developments led by Ashley in Pittsburgh; the expression of these ideas in terms of rule based systems, and their subsequent formalisation; value based theories, which were inspired by a critique of HYPO; and contemporary approaches which revive the idea of dimensions.}}
@ARTICLE{Horkoff_2019,title={Goal-oriented requirements engineering: an extended systematic mapping study},year={2019},author={Jennifer Horkoff and Jennifer Horkoff and Fatma Başak Aydemir and Fatma Basak Aydemir and Evellin Cardoso and Evellin Cardoso and Tong Li and Tong Li and Alejandro Maté and Alejandro Maté and Elda Paja and Elda Paja and Mattia Salnitri and Mattia Salnitri and Luca Piras and Luca Piras and John Mylopoulos and John Mylopoulos and Paolo Giorgini and Paolo Giorgini},doi={10.1007/s00766-017-0280-z},pmid={null},pmcid={null},mag_id={2755269200},journal={Requirements Engineering},abstract={Over the last two decades, much attention has been paid to the area of goal-oriented requirements engineering (GORE), where goals are used as a useful conceptualization to elicit, model, and analyze requirements, capturing alternatives and conflicts. Goal modeling has been adapted and applied to many sub-topics within requirements engineering (RE) and beyond, such as agent orientation, aspect orientation, business intelligence, model-driven development, and security. Despite extensive efforts in this field, the RE community lacks a recent, general systematic literature review of the area. In this work, we present a systematic mapping study, covering the 246 top-cited GORE-related conference and journal papers, according to Scopus. Our literature map addresses several research questions: we classify the types of papers (e.g., proposals, formalizations, meta-studies), look at the presence of evaluation, the topics covered (e.g., security, agents, scenarios), frameworks used, venues, citations, author networks, and overall publication numbers. For most questions, we evaluate trends over time. Our findings show a proliferation of papers with new ideas and few citations, with a small number of authors and papers dominating citations; however, there is a slight rise in papers which build upon past work (implementations, integrations, and extensions). We see a rise in papers concerning adaptation/variability/evolution and a slight rise in case studies. Overall, interest in GORE has increased. We use our analysis results to make recommendations concerning future GORE research and make our data publicly available.}}
@ARTICLE{Holmes_1899,title={The Theory of Legal Interpretation},year={1899},author={Oliver Wendell Holmes and Oliver Wendell Holmes},doi={10.2307/1321531},pmid={null},pmcid={null},mag_id={2795379120},journal={Harvard Law Review},abstract={null}}
@ARTICLE{Modgil_2011,title={Metalevel argumentation},year={2011},author={Sanjay Modgil and Sanjay Modgil and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1093/logcom/exq054},pmid={null},pmcid={null},mag_id={2913787061},journal={Journal of Logic and Computation},abstract={The abstract nature of Dung’s theory of argumentation accounts for its wide-spread application as a general framework for various species of non-monotonic reasoning, and, more generally, reasoning in the presence of conflict. In this article, we formalize reasoning about argumentation within the Dung argumentation paradigm itself. A metalevel Dung argumentation framework is itself instantiated by arguments that make statements about arguments, their interactions, and their evaluation in an object-level argumentation framework.We show how Dung’s theory, and object-level extensions of Dung’s theory, such as those intended to accommodate preferences, can then be uniformly characterised by metalevel argumentation in a Dung framework. We then discuss how this provides for application of the full range of theoretical and practical developments of Dung’s theory, to extensions of Dung’s theory, and provides for integration and further augmentation of these extensions.}}
@ARTICLE{Lauritsen_2015,title={On balance},year={2015},author={Marc Lauritsen and Marc Lauritsen},doi={10.1007/s10506-015-9163-0},pmid={null},pmcid={null},mag_id={2914827576},journal={Artificial Intelligence and Law},abstract={In the course of legal reasoning--whether for purposes of deciding an issue, justifying a decision, predicting how an issue will be decided, or arguing for how it should be decided--one often is required to reach (and assert) conclusions based on a balance of reasons that is not straightforwardly reducible to the application of rules. Recent AI and Law work has modeled reason-balancing, both within and across cases, with set-theoretic and rule- or value-ordering approaches. This article explores a way to model balancing in quantitative terms that may yield new questions, insights, and tools.}}
@ARTICLE{Pollock_null,title={Cognitive Carpentry},year={null},author={John L. Pollock and John L. Pollock},doi={10.7551/mitpress/1887.001.0001},pmid={null},pmcid={null},mag_id={4243806410},journal={null},abstract={null}}
@ARTICLE{Amyot_null,title={Evaluating goal models within the goal-oriented requirement language},year={null},author={Daniel Amyot and Daniel Amyot and Sepideh Ghanavati and Sepideh Ghanavati and Jennifer Horkoff and Jennifer Horkoff and Gunter Mussbacher and Gunter Mussbacher and Liam Peyton and Liam Peyton and Eric Yu and Eric Yu},doi={10.1002/int.20433},pmid={null},pmcid={null},mag_id={4253041212},journal={International Journal of Intelligent Systems},abstract={In this article, we introduce the application of rigorous analysis procedures to goal models to provide several benefits beyond the initial act of modeling. Such analysis can allow modelers to assess the satisfaction of goals, facilitate evaluation of high-level design alternatives, help analysts decide on the high-level requirements and design of the system, test the sanity of a model, and support communication and learning. The analysis of goal models can be done in very different ways depending on the nature of the model and the purpose of the analysis. In our work, we use the Goal-oriented Requirement Language (GRL), which is part of the User Requirements Notation (URN). URN, a new Recommendation of the International Telecommunications Union, provides the first standard goal-oriented language. Using GRL, we develop an approach to analysis that can be done by evaluating qualitative or quantitative satisfaction levels of the actors and intentional elements (e.g., goals and tasks) composing the model. Initial satisfaction levels for some of the intentional elements are provided in a strategy and then propagated to the other intentional elements of the model through the various links that connect them. The results allow for an assessment of the relative effectiveness of design alternatives at the requirements level. Although no specific propagation algorithm is imposed in the URN standard, different criteria for defining evaluation mechanisms are described. We provide three algorithms (quantitative, qualitative, and hybrid) as examples, which satisfy the constraints imposed by the standard. These algorithms have been implemented in the open-source jUCMNav tool, an Eclipse-based editor for URN models. The algorithms are presented and compared with the help of a telecommunication system example. © 2010 Wiley Periodicals, Inc.}}
@ARTICLE{Sommerville_1997,title={Requirements Engineering: A Good Practice Guide},year={1997},author={Ian Sommerville and Ian Sommerville and Pete Sawyer and Pete Sawyer},doi={null},pmid={null},pmcid={null},mag_id={1537202},journal={null},abstract={From the Publisher:
Requirements engineering is the process of discovering, documenting and managing the requirements for a computer-based system. The goal of requirements engineering is to produce a set of system requirements which, as far as possible, is complete, consistent, relevant and reflects what the customer actually wants. Although this ideal is probably unattainable, the use of a systematic approach based on engineering principles leads to better requirements than the informal approach which is still commonly used. This book presents a set of guidelines which reflect the best practice in requirements engineering. Based on the authors' experience in research and in software and systems development, these guidelines explain in an easy-to-understand way how you can improve your requirements engineering processes. The guidelines are applicable for any type of application and, in general, apply to both systems and software engineering.}}
@ARTICLE{Prakken_2011,title={On Modelling Burdens and Standards of Proof in Structured Argumentation},year={2011},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.3233/978-1-60750-981-3-83},pmid={null},pmcid={null},mag_id={4145160},journal={null},abstract={A formal model is proposed of argumentation with burdens and standards of proof, overcoming shortcomings of earlier work. The model is based on a distinction between default and inverted burdens of proof. This distinction is formalised by adapting the definition of defeat of the ASPIC+ framework for structured argumentation. Since ASPIC+ generates abstract argumentation frameworks, the model is thus given a Dungean semantics. It is shown to adequately capture shifting proof burdens as well as Carneades’ definitions of proof standards.}}
@ARTICLE{Singhal_2001,title={Modern Information Retrieval : A Brief Overview},year={2001},author={Amit Singhal and Bimal Metha and Amit Singhal and Marc Levy and Greg Meredith and Tony Andrews and Brian C. Beckman and Johannes Klein and Amit Mital},doi={null},pmid={null},pmcid={null},mag_id={8870360},journal={IEEE Data(base) Engineering Bulletin},abstract={For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.}}
@ARTICLE{Atkinson_2004,title={Computational Representation of Persuasive Argument},year={2004},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={18236625},journal={null},abstract={In this report we discuss the typology of dialogues given by Walton and Krabbe and offer a precise interpretation of them. We go on to discuss one of these dialogue types persuasion in the context of practical reasoning and the problems associated with such reasoning. We propose a perspective on practical reasoning as presumptive justification and critical questions, giving an extension to the account proposed by Walton [24]. This provides us with a foundation for a protocol, named PARMA, for a dialogue game based on this theory. We go on to give an axiomatic and denotational semantics for PARMA and discuss two implementations based on PARMA.}}
@ARTICLE{Hasan_2014,title={Classification and Qualitative Analysis of Non-Functional Requirements Approaches},year={2014},author={Mohammad Mahmudul Hasan and Md. Mahmudul Hasan and Md. Mahmudul Hasan and Pericles Loucopoulos and Pericles Loucopoulos and Pericles Loucopoulos and Pericles Loucopoulos and Μάρα Νικολαϊδου and Mara Nikolaidou},doi={10.1007/978-3-662-43745-2_24},pmid={null},pmcid={null},mag_id={18746366},journal={null},abstract={A considerable number of methods and tools have been proposed for the treatment of non-functional requirements (NFRs). There is ample evidence that NFRs play a significant role in the Information Systems Engineering process. However, there is surprisingly an absence of an agreed position regarding the definition of NFRs, their classification and presentation. This paper reports on a systematic literature review of the documented NFR approaches, classifies these approaches according to different criteria and provides a qualitative analysis of their scopes and characteristics. The results of this analysis can serve system developers as the means of deriving appropriate methods and tools of NFRs engineering process in the system development.}}
@ARTICLE{Bain_1986,title={Judge: a case-based reasoning system},year={1986},author={William M. Bain and William M. Bain},doi={10.1007/978-1-4613-2279-5_1},pmid={null},pmcid={null},mag_id={18790563},journal={null},abstract={People tend to improve their abilities to reason about situations by amassing experiences in reasoning. The more situations which a person knows about, the more able he is to account for feature differences between a new input and old knowledge. A computer program which can improve its ability to reason must also have access to situations which it has previously analyzed or reasoned about. Previous experiences thus require some mechanism for orderly storage and retrieval. The inability to save accounts of previous experiences for future application and modification represents a serious shortcoming of most, if not all, rule-based expert systems.}}
@ARTICLE{Schweighofer_1999,title={Legal Knowledge Representation: Automatic Text Analysis in Public International and European Law},year={1999},author={Erich Schweighofer and Erich Schweighofer},doi={null},pmid={null},pmcid={null},mag_id={20771186},journal={null},abstract={From the Publisher:
This volume is a presentation of all methods of legal knowledge representation from the point of view of jurisprudence as well as computer science. A new method of automatic analysis of legal texts is presented in four case studies.}}
@ARTICLE{Siena_2013,title={Automated Reasoning for Regulatory Compliance},year={2013},author={Alberto Siena and Alberto Siena and Silvia Ingolfo and Silvia Ingolfo and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi and John Mylopoulos and John Mylopoulos},doi={10.1007/978-3-642-41924-9_5},pmid={null},pmcid={null},mag_id={26567153},journal={null},abstract={Regulatory compliance is gaining attention from information systems engineers who must design systems that at the same time satisfy stakeholder requirements and comply with applicable laws. In our previous work, we have introduced a conceptual modelling language called Nomos 2 that aids requirements engineers analyze law to identify alternative ways for compliance. This paper presents an implemented reasoning tool that supports analysis of law models. The technical contributions of the paper include the formalization of reasoning mechanisms, their implementation in the NRTool, as well as an elaborated evaluation framework intended to determine whether the tool is scalable with respect to problem size, complexity as well as search space. The results of our experiments with the tool suggest that this conceptual modelling approach scales to real life regulatory compliance problems.}}
@ARTICLE{Wyner_2010,title={Lexical Semantics and Expert Legal Knowledge towards the Identification of Legal Case Factors},year={2010},author={Adam Wyner and Adam Wyner and Wim Peters and Wim Peters},doi={null},pmid={null},pmcid={null},mag_id={28214186},journal={null},abstract={Legal case factors are textually represented facts which are represented in reported legal case decisions. Precedent decisions contribute to the decision of a case under consideration. As textually represented facts, factors linguistically encode semantic properties and relationships among the entities which can be leveraged to identify and extract the legal case factors from decisions. We integrate legal and linguistic resources in a text analysis tool with which we annotate textual passages. Using annotations tailored to legal case factors, the legal researcher can rapidly zero in on textual spans which represent specific combinations of factors, participants, and semantic properties which bear on who played what role with respect to a factor. The research reports progress on the development of a tool.}}
@ARTICLE{Lloyd_1987,title={Foundations of logic programming; (2nd extended ed.)},year={1987},author={John W. Lloyd and J. W. Lloyd},doi={null},pmid={null},pmcid={null},mag_id={31561947},journal={null},abstract={null}}
@ARTICLE{McCarty_1983,title={Permissions and obligations},year={1983},author={L. Thorne McCarty and L. Thorne McCarty},doi={null},pmid={null},pmcid={null},mag_id={34550249},journal={null},abstract={This article describes a formal semantics for the deontic concepts -- the concepts of permission and obligation -- which arises naturally from the representations used in artificial intelligence systems Instead of treating deontic logic as a branch of modal logic, with the standard possible worlds semantics, we first develop a language for describing actions, and we define the concepts of permission and obligation in terms of these action descriptions. Using our semantic definitions, we then derive a number of intuitively plausible inferences, and we show generally that the paradoxes which are so frequently associated with deontic logic do not arise in our system.}}
@ARTICLE{Shamsaei_2011,title={A Systematic Review of Compliance Measurement Based on Goals and Indicators},year={2011},author={Azalia Shamsaei and Azalia Shamsaei and Daniel Amyot and Daniel Amyot and Alireza Pourshahid and Alireza Pourshahid},doi={10.1007/978-3-642-22056-2_25},pmid={null},pmcid={null},mag_id={37577332},journal={null},abstract={Business process compliance management is an important part of corporate governance as it helps meet objectives while avoiding consequences and penalties. Although there is much research in this area, we believe goal-oriented compliance management using Key Performance Indicators (KPIs) to measure the compliance level of organizations is an area that can be further developed. To investigate this hypothesis, we undertook a systematic literature review, querying four major search engines and performing manual searches in related workshops and citations. From a research body consisting of 198 articles and their references, we have systematically selected 32 papers. We grouped these papers into five categories and highlighted their main contributions. The results show that all selected papers were written in the last five years, and that few effectively represent compliance results using dashboards or similar tools. Although all individual pieces are available, no existing solution yet combines goals with KPIs for measuring the overall compliance level of an organization.}}
@ARTICLE{Rissland_1980,title={Overview of an example generation system},year={1980},author={Edwina L. Rissland and Edwina L. Rissland and Elliot Soloway and Elliot Soloway and Elliot Soloway},doi={null},pmid={null},pmcid={null},mag_id={37626337},journal={null},abstract={This paper addresses the process of generating examples which meet specified criteria; we call this activity CONSTRAINED EXAMPLE GENERATION (CEG). We present the motivation for and architecture of an existing example generator which solves CEG problems in several domains of mathematics and computer science, e.g., the generation of LISP test data, simple recursive programs, and piecewise linear functions.}}
@ARTICLE{Halas_2008,title={Organizational aspect of trusted legally valid long-term electronic archive solution},year={2008},author={Helena Halas and Helena Halas and Jan Porekar and Jan Porekar and Tomaž Klobučar and Tomaž Klobučar and Tomaz Klobučar and Aleksej Jerman Blazič and Aleksej Jerman Blazič},doi={null},pmid={null},pmcid={null},mag_id={55205078},journal={WSEAS Transactions on Information Science and Applications archive},abstract={Due to increase of electronic business and business process dematerialization organizations are facing today a problem of preserving vast amounts of electronic documents in coherent and trustworthy manner. A large amount of digital documents are produced every day even in small and medium-sized companies. The documents range from simple receipts to complex legal contracts and service level agreements. Many such documents need to be stored and preserved for longer period of time. Some services and technical solutions providing long term proofs of authenticity, integrity and non-repudiation of electronic documents are available on the market today. In order for these technical electronic archiving solutions and services to be successfully adopted by organizations they need to be deployed in a proper operational and organizational manner. Beside this organization needs to establish required operational procedures and to operate in accordance with them to assure that trusted electronic archive is legally valid. In this paper we present the first set of organizational approaches that organizations need to utilize in order to successfully integrate the operational and legal aspects of electronic archiving and to change the business processes accordingly. Following the approach of pattern oriented organizational design we capture the organizational trusted archiving solutions and best practices in the form of patterns, providing the context of the problem, the generic solution captured in the form of organizational diagrams, and preconditions that need to be met by the organization, and dependencies on other patterns are described. Finally the paper presents implementation of the generic solution to different organizations' contexts and indicates influence of different applications of the pattern to further solution development.}}
@ARTICLE{Shortliffe_1976,title={Mycin: computer-based medical consultations},year={1976},author={Edward H. Shortliffe},doi={null},pmid={null},pmcid={null},mag_id={57206305},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Goodrich_1986,title={Historical Aspects of Legal Interpretation},year={1986},author={Peter Goodrich and Peter Goodrich},doi={null},pmid={null},pmcid={null},mag_id={63325788},journal={Indiana Law Journal},abstract={One of the most interesting developments within contemporary legal theory has been the increasing importance accorded to the concept of interpretation. It is fortunately no longer possible to speak uncritically of, or simply to assume, the communicational and linguistic dimensions of legal regulation or of legal institutional discourse. While the concepts of communicationof discourse, language, text and sign-have long been key terms of debate within philosophy, literary theory and cultural studies, it is really only very recently that lawyers and particularly the legal academy have begun to take a serious if somewhat defensive interest in these disputes. The issues raised and the interests threatened are ponderous and vast; many of the dogmatic articles of legal faith are at stake and it should not be viewed as surprising if the debates as to the substantive implications of different forms of interpretation appear at times extreme and the positions adopted seem labored or untenable. The issues raised are intrinsically political: a direct challenge is presented to the traditionally established motive and characteristics of legal method, the humanistic tenets of legal philology are denied, and the liberal ideology of the rule of law itself is again placed in question. In such a context the current jurisprudential debates have an uncharacteristic urgency, for it is not simply the legal educational apparatus that is asked to change its course but, more dramatically, it is substantive legal practice and the corresponding professional status or standing of the law that are placed in balance. Clearly the issues adverted to are too broad to form anything more than a general though dynamic context to the present inquiry into the historical status of the legal text as object of interpretation. As a further complication,}}
@ARTICLE{Bex_2004,title={Reinterpreting arguments in dialogue : an application to evidential reasoning},year={2004},author={Floris Bex and Henry Prakken and Thomas F. Gordon},doi={null},pmid={null},pmcid={null},mag_id={63493041},journal={null},abstract={This paper presents a formalisation of two typical legal dialogue moves in a formal dialogue game for argumentation. The moves concern two ways of reinterpreting a general rule used in an argument, viz. by 'unpacking' and 'refining' the rule. The moves can be made not only by the user but also by the attacker of the rule, in order to reveal new ways to attack it. The new dialogue game is illustrated with examples from legal evidential reasoning, in which these types of moves are particularly common.}}
@ARTICLE{Pennington_1993,title={The story model for juror decision making},year={1993},author={Nancy Pennington and Nancy Pennington and Nancy Pennington and Reid Hastie and Reid Hastie},doi={10.1017/cbo9780511752896.010},pmid={null},pmcid={null},mag_id={65563293},journal={null},abstract={Introduction   The goal of our research over the past ten years has been to understand the cognitive strategies that individual jurors use to process trial information in order to make a decision prior to deliberation. We have approached this goal with the perspective of psychologists who are interested in how people think and behave. First, we have developed a theory that we believe describes the cognitive strategies that jurors use. We call this theory the story model, and it is described in the first section of the paper. Second, we have conducted extensive empirical work to test the theory. This work is summarized in the second section of the paper. The story model has been developed in the context of criminal trials, so it will be presented and discussed in those terms. In the final section of the paper, we discuss some of our current research directions.   The story model   We call our theory the story model because we propose that a central cognitive process in juror decision making is  story construction  (Bennett & Feldman, 1981; Pennington, 1981, 1991; Pennington & Hastie, 1980, 1981a, 1981b, 1986, 1988, 1992). Although story construction is central in our theory and has been the focus of most of our empirical research, it is but one of three processes that we propose.}}
@ARTICLE{Karagiannis_2008,title={A Business Process-Based Modelling Extension for Regulatory Compliance.},year={2008},author={Dimitris Karagiannis and Dimitris Karagiannis},doi={null},pmid={null},pmcid={null},mag_id={65598986},journal={null},abstract={null}}
@ARTICLE{Durán_1999,title={A Requirements Elicitation Approach Based in Templates and Patterns.},year={1999},author={Amador Durán and Amador Durán Toro and Beatriz Bernárdez Jiménez and Beatriz Bernárdez Jiménez and Antonio Ruiz–Cortés and Antonio Ruiz Cortés and Miguel Toro Bonilla and Miguel Toro Bonilla},doi={null},pmid={null},pmcid={null},mag_id={69445307},journal={null},abstract={One of the main problems of requirements elicitation is expressing customer requirements in a form that can be understood not only by requirements engineers but also by noncomputer professional customers and users. The usual choice for expressing elicited requirements is natural language, since it is frequently the only common language to all participants. Problems of natural language are well–known, but using more formal notations too early is a risky choice that can make requirements impossible to understand for customers and users. Moreover, requirements engineers do not usually have good writing skills, and sometimes semantically correct requirements, expressed in natural language, are not understood because of the way they are written. In this paper, we present requirements templates that can improve requirements elicitation and expression, and two kinds of patterns: linguistic patterns, which are very used sentences in natural language requirements descriptions that can be parameterized and integrated into templates, and requirements patterns, which are generic requirements templates that are found very often during the requirements elicitation process and that can be reused with some adaptation.}}
@ARTICLE{Palmer_2005,title={From Lerotholi to Lando: Some Examples of Comparative Law Methodology},year={2005},author={Vernon Valentine Palmer and Vernon Valentine Palmer},doi={10.1093/ajcl/53.1.261},pmid={null},pmcid={null},mag_id={71650669},journal={American Journal of Comparative Law},abstract={Methodological discussions, it has been said, are a good cure for insomnia.4 Of course any number of legal topics have been known to cure that disorder, so clearly excitement is not the best measure of a subject's true importance. Today the importance of methodology to comparative law is indisputable and crucial, and recent years have witnessed an intense and lively debate over new directions in comparative law. These discussions have been keeping many thoughtful lawyers awake even though some may have been merely dozing.}}
@ARTICLE{Nissenbaum_2004,title={Privacy as contextual integrity},year={2004},author={Helen Nissenbaum and Helen Nissenbaum},doi={null},pmid={null},pmcid={null},mag_id={72496981},journal={Washington Law Review},abstract={The practices of public surveillance, which include the monitoring of individuals in public through a variety of media (e.g., video, data, online), are among the least understood and controversial challenges to privacy in an age of information technologies. The fragmentary nature of privacy policy in the United States reflects not only the oppositional pulls of diverse vested interests, but also the ambivalence of unsettled intuitions on mundane phenomena such as shopper cards, closed-circuit television, and biometrics. This Article, which extends earlier work on the problem of privacy in public, explains why some of the prominent theoretical approaches to privacy, which were developed over time to meet traditional privacy challenges, yield unsatisfactory conclusions in the case of public surveillance. It posits a new construct, “contextual integrity,” as an alternative benchmark for privacy, to capture the nature of challenges posed by information technologies. Contextual integrity ties adequate protection for privacy to norms of specific contexts, demanding that information gathering and dissemination be appropriate to that context and obey the governing norms of distribution within it. Building on the idea of “spheres of justice,” developed by political philosopher Michael Walzer, this Article argues that public surveillance violates a right to privacy because it violates contextual integrity; as such, it constitutes injustice and even tyranny.}}
@ARTICLE{Ashley_2009,title={What a Legal CBR Ontology Should Provide.},year={2009},author={Kevin D. Ashley and Kevin D. Ashley},doi={null},pmid={null},pmcid={null},mag_id={84093361},journal={null},abstract={null}}
@ARTICLE{Gómez-Pérez_2007,title={Ontological Engineering: with examples from the areas of Knowledge Management, e-Commerce and the Semantic Web. (Advanced Information and Knowledge Processing)},year={2007},author={Asuncíon Gómez-Pérez and Asunción Gómez-Pérez and Mariano Fernández-López and Mariano Fernández-López and Óscar Corcho and Oscar Corcho},doi={null},pmid={null},pmcid={null},mag_id={84142403},journal={null},abstract={null}}
@ARTICLE{Waterman_1980,title={Rule-based models of legal expertise},year={1980},author={D. A. Waterman and D. A. Waterman and Mark A. Peterson and Mark A. Peterson},doi={null},pmid={null},pmcid={null},mag_id={84461121},journal={null},abstract={This paper describes a rule-based legal decisonmaking system (LDS) that embodies the skills and knowledge of an expert in product liability law. The system is being used to study the effect of changes in legal doctrine on settlement strategies and practices. LDS is implemented in ROSIE, a rule-oriented language designed to facilitate the development of large expert systems. The ROSIE language is briefly described and our approach to modeling legal expertise using a prototype version of LDS is presented.}}
@ARTICLE{Walton_2010,title={Types of Dialogue and Burdens of Proof},year={2010},author={Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={90508172},journal={null},abstract={Burden of proof has recently come to be a topic of interest in argumentation systems for artificial intelligence (Prakken and Sartor, 2006, 2007, 2009; Gordon and Walton, 2007, 2009), but so far the main work on the subject seems to be in that type of dialogue which has most intensively been investigated generally, namely persuasion dialogue. The most significant exception is probably deliberation dialogue, where some recent work has begun to tentatively investigate burden of proof in that setting. In this paper, I survey work on burden of proof in the artificial intelligence literature on argumentation, and offer some thoughts on how this work might be extended to the other types of dialogue recognized by Walton and Krabbe (1995) that so far do not appear to have been much investigated in this regard.}}
@ARTICLE{Rissland_1984,title={The ubiquitous dialectic},year={1984},author={Edwina L. Rissland and Edwina L. Rissland},doi={null},pmid={null},pmcid={null},mag_id={94776975},journal={null},abstract={null}}
@ARTICLE{Kharbili_2012,title={Business process regulatory compliance management solution frameworks: a comparative evaluation},year={2012},author={Marwane El Kharbili and Marwane El Kharbili},doi={null},pmid={null},pmcid={null},mag_id={97210008},journal={null},abstract={Regulatory compliance management (RCM) is a problem gaining wide interest in the business process management (BPM) community. However, research has not yet provided a non-ambiguous and agreed-upon definition of RCM, and it is hard for newcomers to this field of research to get a clear overview of available results. This paper surveys and analyzes solutions proposed in research on RCM from the perspective of BPM, and gives an insight into the current strengths and limitations of solutions to RCM applied to BPM. We extract a set of evaluation criteria on RCM elicited from the surveyed works and proceed to a comparative analysis of the latter against the identified requirements.}}
@ARTICLE{Governatori_2012,title={Possible world semantics for defeasible deontic logic},year={2012},author={Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Erica Calardo and Erica Calardo},doi={10.1007/978-3-642-31570-1_4},pmid={null},pmcid={null},mag_id={102938656},journal={null},abstract={Defeasible Deontic Logic is a simple and computationally efficient approach for the representation of normative reasoning. Traditionally defeasible logics are defined proof theoretically based on the proof conditions for the logic. While several logic programming, operational and argumentation semantics have been provided for defeasible logics, possible world semantics for (modal) defeasible logics remained elusive. In this paper we address this issue.}}
@ARTICLE{Abraham_2009,title={Editorial: Hybrid learning machines},year={2009},author={Ajith Abraham and Ajith Abraham and Emilio Corchado and Emilio Corchado and Juan M. Corchado and Juan M. Corchado},doi={10.1016/j.neucom.2009.02.017},pmid={null},pmcid={null},mag_id={104452911},journal={Neurocomputing},abstract={null}}
@ARTICLE{Salvaneschi_2005,title={The Quality Matrix: A Management Tool for Software Quality Evaluation.},year={2005},author={Paolo Salvaneschi and Paolo Salvaneschi},doi={null},pmid={null},pmcid={null},mag_id={107016059},journal={null},abstract={null}}
@ARTICLE{Randolph_2005,title={Free-Marginal Multirater Kappa (multirater K[free]): An Alternative to Fleiss' Fixed-Marginal Multirater Kappa.},year={2005},author={Justus Randolph and Justus J. Randolph},doi={null},pmid={null},pmcid={null},mag_id={112197792},journal={null},abstract={null}}
@ARTICLE{MeCarty_1981,title={The representation of an evolving system of legal concepts: II. phototypes and deformations},year={1981},author={L. Thorne MeCarty and L. Thorne MeCarty and N. S. Sridharan and N. S. Sridharan},doi={null},pmid={null},pmcid={null},mag_id={118477235},journal={null},abstract={One of the principal goals of the TAXMAN project is to develop a theory about the structure and dynamics of legal concepts, using corporate tax law as an experimental problem domain In this paper we describe the "prototype* plus-deformation" model of legal conceptual structure a concept is represented here by a prototypical description plus a sequence of deformations of the prototype, where the deformations are selected from among the possible "mappmga" of one concrete description into another The paper focuses on the set of mappings, which is the most important component of the model because it makes manifest the basic coherence of the conceptual space The syntax and semantics of the mappings are described, and their role in the process of legal argument is suggested The formal modal is then illustrated by examples drawn from Eisner v Macomber, an early stock dividend case.}}
@ARTICLE{Araszkiewicz_2010,title={Balancing of Legal Principles and Constraint Satisfaction},year={2010},author={Michał Araszkiewicz and Michał Araszkiewicz},doi={null},pmid={null},pmcid={null},mag_id={133221902},journal={null},abstract={Robert Alexy is one of the main advocates of the so-called Rules and Principles Theory (hereafter: RPT). According to the RPT, legal norms can be divided into legal rules and legal principles. One of the main criteria for this distinction is-Alexy argues-that legal rules are applied by means of the Subsumption Formula, while legal principles-by means of the so-called Weight Formula (hereafter: WF). The WF offers an important insight into the structure of the process of balancing in legal reasoning. Alexy's proposal leads to many doubts and questions, however. The aim of the paper is to examine the appropriateness of the WF and the problem of balancing in legal reasoning from the perspective inspired by the constraint satisfaction theory of coherence, developed by Paul Thagard. My claim is that this theory enables us to elucidate many problematic features of the WF and to recast the structure of legal balancing in more transparent and efficient manner. The existing workable algorithms designed for computing other kinds of coherence-based reasoning (for instance, explanatory reasoning or analogical reasoning), make possible to adopt the programs employing these algorithms for computation of coherence in balancing of principles. However, the analysis presented here is mainly conceptual and it has only preparatory character in relation with the possible computational implementations in the future.}}
@ARTICLE{Greenberg_2011,title={why agency interpretations of ambiguous statutes should be subject to stare decisis},year={2011},author={Harold Greenberg},doi={null},pmid={null},pmcid={null},mag_id={140388281},journal={null},abstract={Agencies’ interpretations of ambiguous statutes under Chevron are not subject to a rule of stare decisis. Agencies may interpret and later reinterpret ambiguous statutes without settling the statute’s meaning. This article shows that this regime permits agencies to “interpret” law in legally unprincipled and inconsistent ways and prevents administrative statutes from meaningfully constraining agency policymaking. This article concludes that a rule of stare decisis should govern agencies’ interpretations of ambiguous statutes just as it governs judicial holdings. Taking seriously Chevron’s recognition of agencies’ power to interpret law, the conventional justifications for stare decisis – separation of powers, legislative supremacy, and the consistency of regulatory schemes – apply with equal force to agencies as to courts.}}
@ARTICLE{Guarino_2009,title={What Is an Ontology},year={2009},author={Nicola Guarino and Nicola Guarino and Daniel Oberle and Daniel Oberle and Steffen Staab and Steffen Staab},doi={10.1007/978-3-540-92673-3_0},pmid={null},pmcid={null},mag_id={153886719},journal={null},abstract={The word “ontology” is used with different senses in different communities. The most radical difference is perhaps between the philosophical sense, which has of course a well-established tradition, and the computational sense, which emerged in the recent years in the knowledge engineering community, starting from an early informal definition of (computational) ontologies as “explicit specifications of conceptualizations”. In this paper we shall revisit the previous attempts to clarify and formalize such original definition, providing a detailed account of the notions of conceptualization and explicit specification, while discussing at the same time the importance of shared explicit specifications.}}
@ARTICLE{Lodder_2001,title={A simple model to structure the information of parties in online alternative dispute resolution.},year={2001},author={A.R. Lodder and Arno R. Lodder},doi={null},pmid={null},pmcid={null},mag_id={163014208},journal={null},abstract={null}}
@ARTICLE{Freeman_1991,title={Dialectics and the Macrostructure of Arguments},year={1991},author={James B. Freeman and James B. Freeman},doi={null},pmid={null},pmcid={null},mag_id={164525691},journal={null},abstract={null}}
@ARTICLE{Woods_1974,title={Argumentum ad Verecundiam},year={1974},author={John Woods and John Woods and Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={165245974},journal={Philosophy and Rhetoric},abstract={It is commonplace that the informai fallacies constitute a branch of logic (though we should prefer to say "the analysis of argument") virtually bereft of theoretical articulation. In recent years, the practical importance of the study of informai fallacies has been recognized in many an introductory logic text, but very little systematic or sustained attention has been given to exploiting the potentialities of this important area. Fallacies hâve been of relatively constant interest since Aristotle, but on the whole the tendency has been to approach them intuitively and taxonomically rather than to pursue their theoretical aspects at appropriate levels of generality and systematicity. One need not hâve done very much teaching of introductory logic to appreciate that the lack of clear and adequate characterizations of the fallacies makes it virtually impossible to commend to students fallacy-theoretic notions as a really effective strategy in the analysis of argumentation. Yet there can be little doubt that the usefulness of such strategies for adjudicating actual argumentation in naturai language could be greatly advanced by a more systematic attempi to raise ourselves beyond mere intuitions about f allaciousness.}}
@ARTICLE{McLaren_1999,title={Case Representation, Acquisition, and Retrieval in SIROCCO},year={1999},author={Bruce M. McLaren and Bruce M. McLaren and Kevin D. Ashley and Kevin D. Ashley},doi={10.1007/3-540-48508-2_18},pmid={null},pmcid={null},mag_id={165919144},journal={null},abstract={As part of our investigation of how abstract principles are operationalized to facilitate their application to specific fact situations, we have begun to develop and experiment with SIROCCO (System for Intelligent Retrieval of Operationalized Cases and COdes), a CBR retrieval and analysis system applied to the domain of engineering ethics. SIROCCO is intended to retrieve decided engineering ethics cases and previously applied ethics codes to assist engineers and students in analyzing new cases. Here we describe a limited but expressive language designed to represent a wide range of ethics cases in SIROCCO, a world-wide web tool developed to perform case acquisition and support a measure of consistency in representation, and an experiment to validate the initial phase of SIROCCO's retrieval algorithm and test its sensitivity to small variations in case description.}}
@ARTICLE{Rissland_1985,title={AI and legal reasoning},year={1985},author={Edwina L. Rissland and Edwina L. Rissland},doi={null},pmid={null},pmcid={null},mag_id={166506977},journal={null},abstract={This paper presents a summary of the responses of a panel to issues on AI and legal reasoning. The panel consisted of Edwina L. Rissland, Chair (University of Massachusetts), Kevin D. Ashley (University of Massachusetts), Michael O. Dyer (UCLA), Anne v.d.l. Gardner (Stanford), L. Thome McCarty (Rutgers), and Donald A. Waterman (RAND). Among the issues addressed by the panel were: 1. What are the characteristics of the legal domain that make it interesting or amenable to AI approaches - what is special about it; 2. The open-textured nature of legal concepts and the implications this has for using AI-techniques, especially knowledge representation; 3. The complementarity of rule-based and case-based reasoning - how cases are used, especially when the rules "run out"; 4. The pervasive role of analogy in legal reasoning; 5. The special role played by hypothetical in the legal domain and how hypos help with argumentation and strategic case planning; 6. The interleaving of justification, explanation, and argumentation; 7. How common law systems can be seen to be systems which learn from cases; 8. The appropriateness and feasibility of intelligent aids for practicing litigators and other legal experts; 9. Implications for other domains-like medicine - that use case-based reasoning; 10. Methodological and other issues.

For each issue considered, the comments of the panelists are summarized.}}
@ARTICLE{Leng_2014,title={INTERACTING WITH KNOWLEDGE BASED SYSTEMS THROUGH DIALOGUE GAMES},year={2014},author={Paul Leng and Paul H. Leng},doi={null},pmid={null},pmcid={null},mag_id={167638657},journal={null},abstract={Approaches to text generation have emphasised the need to plan dialogues between user and computer, but have paid insufficient attention to the highly conventional nature of much human-human interaction. It is possible to conceive of certain classes of dialogues as highly rule-governed activities, and to do so facilitates the planning of such dialogues, by decreasing the reliance on a model of the user. One such class of dialogue is interactive argument, in which a topic is discussed with the aim of reaching a conclusion as to the truth of some proposition. We contend that interactive argument provides a useful model of interaction with a KBS, could provide a novel means of knowledge elicitation, and would serve as a fruitful model for educational systems also. Details of a dialogue manager based on a well known dialogue game (Mackenzie’s DC) are given. Since KBS will exist independently of this way of interacting with them we give details of a Dialogue Abstract Machine (DAM), which could serve as a target interface between our dialogue manager and a KBS of any formalism.}}
@ARTICLE{Kharbili_2009,title={A Semantic Framework for Compliance Management in Business Process Management.},year={2009},author={Marwane El Kharbili and Marwane El Kharbili and Elke Pulvermüller and Elke Pulvermüller},doi={null},pmid={null},pmcid={null},mag_id={169608898},journal={null},abstract={null}}
@ARTICLE{Bench‐Capon_2006,title={Justifying Actions by Accruing Arguments},year={2006},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Henry Prakken and Henry Prakken},doi={null},pmid={null},pmcid={null},mag_id={172988810},journal={null},abstract={null}}
@ARTICLE{Antoniou_2002,title={Defeasible logic with dynamic priorities},year={2002},author={Grigoris Antoniou and Grigoris Antoniou},doi={null},pmid={null},pmcid={null},mag_id={177421036},journal={null},abstract={Defeasible logic is a nonmonotonic reasoning approach based on rules and priorities. Its design supports efficient implementation, and it shows promise to be successfully deployed in applications.

So far only static priorities have been used, provided by an external superiority relation. In this paper we show how dynamic priorities can be integrated, where priority information is obtained from the deductive process itself. Dynamic priorities have been studied for other related reasoning systems, such as default logic and argumentation. We define a proof theory, study its formal properties, and provide an argumentation semantics.}}
@ARTICLE{Siena_2012,title={Requirements, intentions, goals and applicable norms},year={2012},author={Alberto Siena and Alberto Siena and Silvia Ingolfo and Silvia Ingolfo and Angelo Susi and Angelo Susi and Ivan Jureta and Ivan Jureta and Anna Perini and Anna Perini and John Mylopoulos and John Mylopoulos},doi={10.1007/978-3-642-33999-8_24},pmid={null},pmcid={null},mag_id={179502385},journal={null},abstract={Norms such as laws and regulations are an additional source of requirements as they cause domain actors to modify their goals to reach compliance. However, norms can not be modeled directly as goals because of both an ontological difference, and an abstraction gap that causes the need to explore a potentially large space of alternatives. This paper presents the problem of deriving goals from norms and illustrates the open research challenges.}}
@ARTICLE{Schlag_1985,title={RULES AND STANDARDS},year={1985},author={Pierre Schlag and Pierre Schlag},doi={null},pmid={null},pmcid={null},mag_id={183799186},journal={UCLA Law Review},abstract={Every student of law has at some point encountered the “bright line rule” and the “flexible standard.” In one torts casebook, for instance, Oliver Wendell Holmes and Benjamin Cardozo find themselves on opposite sides of a railroad crossing dispute. They disagree about what standard of conduct should define the obligations of a driver who comes to an unguarded railroad crossing. Holmes offers a rule: The driver must stop and look. Cardozo rejects the rule and instead offers a standard: The driver must act with reasonable caution. Which is the preferable approach? Holmes suggests that the requirements of due care at railroad crossings are clear and, therefore, it is appropriate to crystallize these obligations into a simple rule of law. Cardozo counters with scenarios in which it would be neither wise nor prudent for a driver to stop and look. Holmes might well have answered that Cardozo’s scenarios are exceptions and that exceptions prove the rule. Indeed, Holmes might have parried by suggesting that the definition of a standard of conduct by means of a legal rule is predictable and certain, whereas standards and juries are not. This dispute could go on for quite some time.}}
@ARTICLE{Feldt_2010,title={Validity threats in empirical software engineering research - An initial survey},year={2010},author={Robert Feldt and Robert Feldt and Ana Magazinius and Ana Magazinius},doi={null},pmid={null},pmcid={null},mag_id={190190587},journal={null},abstract={In judging the quality of a research study it is very important to consider threats to the validity of the study and the results. This is particularly important for empirical research where there is often a multitude of possible threats. With a growing focus on empirical research methods in software engineering it is important that there is a consensus in the community on this importance, that validity analysis is done by every researcher and that there is common terminology and support on how to do and report it. Even though there are previous relevant results they have primarily focused on quantitative research methods and in particular experiments. Here we look at the existing advice and guidelines and then perform a review of 43 papers published in the ESEM conference in 2009 and analyse the validity analysis they include and which threats and strategies for overcoming them that were given by the authors. Based on this analysis we then discuss what is working well and less well in validity analysis of empirical software engineering research and present recommendations on how to better support validity analysis in the future.}}
@ARTICLE{Wray_2002,title={Formulaic Language and the Lexicon: The Whole and the Parts},year={2002},author={Alison Wray and Alison Wray and Alison Wray},doi={10.1017/cbo9780511519772.002},pmid={null},pmcid={null},mag_id={192273426},journal={null},abstract={‘Twelve-inches-one-foot. Three-feet-make-a-yard. Fourteen-pounds-make-a-stone. Eight-stone-a-hundred-weight’.… Unhearing, unquestioning, we rocked to our chanting, hammering the gold nails home. ‘Twice-two-are-four. One-God-is-Love. One-Lord-is-King. One-King-is-George. One-George-is-Fifth …’ So it was always; had been, would be for ever; we asked no questions; we didn't hear what we said; yet neither did we ever forget it.  Laurie Lee:  Cider with Rosie . Penguin:53–4    She would go and smile and be nice and say ‘So kind of you. I'm so pleased. One is so glad to know people like one's books’. All the stale old things. Rather as you put a hand into a box and took out some useful words already strung together like a necklace of beads.  Agatha Christie:  Elephants Can Remember . Pan:12    Introduction   In a series of advertisements run on British TV early in 1993 by the breakfast cereal manufacturer Kellogg, people were asked what they thought Rice Krispies were made of, and expressed surprise at discovering that the answer was rice. Somehow they had internalized this household brand name without ever analyzing it into its component parts. It was as if the name of the product had taken on a life of its own, and required no more reference back to its ‘meaning’ than do words of foreign origin such as  chop suey  (‘mixed bits’) and  spaghetti  (‘little cords’). But how could this come about in the case of a name which, although oddly spelled, so transparently refers to crisp rice? In actual fact, overlooking the internal composition of names is a far more common phenomenon than we might at first think.}}
@ARTICLE{Dunne_2005,title={Discovering inconsistency through examination dialogues},year={2005},author={Paul E. Dunne and Paul E. Dunne and Sylvie Doutre and Sylvie Doutre and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={200205250},journal={null},abstract={In this paper we introduce examination dialogues, an addition to the dialogue typology of Walton and Krabbe. In educational settings the purpose of dialogue is often to elicit the position of a student, e.g. to test understanding. In other settings, a frequently adopted tactic is to attack an opponent's stance by exposing internal inconsistencies in their argument. In real debate such inconsistencies will often be rather more subtle than elementary logical fallacies since they arise from contradictions apparent in the opponent's value system. Protagonists will be better positioned to judge the applicability of this tactic as more information is determined concerning the exact nature of their opponent's case, e.g. the arguments favoured and values endorsed. One obstacle, however, is that following a request to state a view, the challenged party may refuse to comment. In this paper we present an approach to modelling the evolution of examination dialogues based on the concept of value-based argument frameworks and outline some algorithmic issues regarding argument selection.}}
@ARTICLE{Damian_2004,title={Requirements Engineering challenges in multi-site software development organizations},year={2004},author={Daniela Damian and Daniela Damian and Didar Zowghi and Didar Zowghi},doi={null},pmid={null},pmcid={null},mag_id={204749382},journal={null},abstract={1 The paper is a revised version of the paper entitled "The impact of stakeholders’ geographical distribution on managing requirements in a multi-site organization” published in the Proceedings of the IEEE Int’l Conference on Requirements Engineering, 2002 Abstract The increasing globalization of software industry demands an investigation of Requirements Engineering (RE) in multi-site software development organizations. Requirements engineering is a task difficult enough when done locally -but it is even more difficult when cross-functional stakeholder groups specify requirements across cultural, language and time zone boundaries. This paper reports on a field study that investigated RE challenges introduced by stakeholders’ geographical distribution in a multi-site organization. The goal was to examine RE practice in global software development, to formulate recommendations for improvement as well as to provide directions for future research on methods and tools. Based on the empirical evidence, we have constructed a model of how remote communication and knowledge management, cultural diversity and time differences negatively impact on requirements gathering, negotiation and specification. Findings reveal that aspects such as a lack of a common understanding of requirements, together with reduced awareness of working local context, trust level and ability to share work artifacts significantly challenge the effective collaboration of remote stakeholders in negotiating a set of requirements that satisfies geographically distributed customers. The paper concludes with recommendations for improving RE practice in this setting.}}
@ARTICLE{Teufel_1999,title={Argumentative zoning information extraction from scientific text},year={1999},author={Simone Teufel and Simone Teufel},doi={null},pmid={null},pmcid={null},mag_id={205532704},journal={null},abstract={null}}
@ARTICLE{Horkoff_2015,title={Using Goal Models Downstream: A Systematic Roadmap and Literature Review},year={2015},author={Jennifer Horkoff and Jennifer Horkoff and Tong Li and Tong Li and Fenglin Li and Feng-Lin Li and Mattia Salnitri and Mattia Salnitri and Evellin Cardoso and Evellin Cardoso and Paolo Giorgini and Paolo Giorgini and John Mylopoulos and John Mylopoulos},doi={10.4018/ijismd.2015040101},pmid={null},pmcid={null},mag_id={232406460},journal={International Journal of Information System Modeling and Design},abstract={Goal models have proven useful for capturing, understanding, and communicating requirements during early stages of software development. However, the utility of goal models is greatly enhanced when they can be exploited during downstream stages of the requirements analysis process (e.g. requirements elaboration, validation, planning), and can be used as part of the entire system life cycle (e.g., architectural and behavioral process design, coding, testing, monitoring, adaptation, and evolution). In order to better understand the progress that has been made in integrating goal models with downstream system development, the authors ask: what approaches exist that map/integrate/transform goal models to later stage software artifacts? To answer this question, they conduct a systematic survey, producing a roadmap of work summarizing 243 publications. Results include a categorization of the â€œwhy?â€ and â€œhow?â€ for each approach. Furthermore, they select the 50 most prominent publications, based on citation numbers, in order to perform an in-depth literature review. Findings show that there is a wide variety of proposals with a variety of proposed goal models and targets, covering multiple paradigms, motivated by a variety of purposes. The authors conclude that although much work has been done in this area, the work is fragmented, following multiple separate strands of goal-orientation, and is often still in early stages of maturity.}}
@ARTICLE{Akhigbe_2014,title={Creating Quantitative Goal Models: Governmental Experience},year={2014},author={Okhaide Akhigbe and Okhaide Akhigbe and Mohammad Alhaj and Mohammad Alhaj and Daniel Amyot and Daniel Amyot and Omar Badreddin and Omar Badreddin and Edna Braun and Edna Braun and Nick Cartwright and Nick Cartwright and Nick Cartwright and Gregory Richards and Gregory Richards and Gunter Mussbacher and Gunter Mussbacher},doi={10.1007/978-3-319-12206-9_40},pmid={null},pmcid={null},mag_id={283361348},journal={null},abstract={Precision in goal models can be enhanced using quantitative rather than qualitative scales. Selecting appropriate values is however often difficult, especially when groups of stakeholders are involved. This paper identifies and compares generic and domain-specific group decision approaches for selecting quantitative values in goal models. It then reports on the use of two approaches targeting quantitative contributions, actor importance, and indicator definitions in the Goal-oriented Requirement Language. The approaches have been deployed in two independent branches of the Canadian government.}}
@ARTICLE{Wyner_2013,title={On the Instantiation of Knowledge Bases in Abstract Argumentation Frameworks},year={2013},author={Adam Wyner and Adam Wyner and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Paul E. Dunne and Paul E. Dunne},doi={10.1007/978-3-642-40624-9_3},pmid={null},pmcid={null},mag_id={319522146},journal={null},abstract={Abstract Argumentation Frameworks afs provide a fruitful basis for exploring issues of defeasible reasoning. Their power largely derives from the abstract nature of the arguments within the framework, where arguments are atomic nodes in an undifferentiated relation of attack. This abstraction conceals different conceptions of argument, and concrete instantiations encounter difficulties as a result of conflating these conceptions. We distinguish three distinct senses of the term. We provide an approach to instantiating AF in which the nodes are restricted to literals and rules, encoding the underlying theory directly. Arguments, in each of the three senses, then emerge from this framework as distinctive structures of nodes and paths. Our framework retains the theoretical and computational benefits of an abstract af, while keeping notions distinct which are conflated in other approaches to instantiation.}}
@ARTICLE{Verheij_2005,title={Virtual Arguments: On the Design of Argument Assistants for Lawyers and Other Arguers},year={2005},author={Bart Verheij and Bart Verheij},doi={null},pmid={null},pmcid={null},mag_id={319736696},journal={null},abstract={The First Prototype: ARGUE!.- Improved Naturalness: ARGUMED 2.0.- A Logical Extension: ARGUMED 3.0 based on DEFLOG.- A Comparison of Argument Assistants and Mediators.- Theories of Defeasible Argumentation.- Argument Assistants: Conclusions and Prospects.}}
@ARTICLE{Governatori_2012,title={Rule based business process compliance},year={2012},author={Guido Governatori and Guido Governatori and Sidney Shek and Sidney Shek},doi={null},pmid={null},pmcid={null},mag_id={323140412},journal={null},abstract={In this paper we report on the development and evaluation of a business process compliance checker, based on the compliance-by-design methodology proposed by Governatori and Sadiq [9]. For a screencast see http://www.youtube.com/watch?v=gFmDQJNai_4}}
@ARTICLE{Freeman_1991,title={Dialectics and the Macrostructure of Arguments: A Theory of Argument Structure},year={1991},author={James B. Freeman and James B. Freeman},doi={null},pmid={null},pmcid={null},mag_id={331942934},journal={null},abstract={null}}
@ARTICLE{Ingolfo_2014,title={Nòmos 3: Legal Compliance of Roles and Requirements},year={2014},author={Silvia Ingolfo and Silvia Ingolfo and Ivan Jureta and Ivan Jureta and Alberto Siena and Alberto Siena and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi},doi={10.1007/978-3-319-12206-9_22},pmid={null},pmcid={null},mag_id={337784159},journal={null},abstract={The problem of regulatory compliance for a software system consists of ensuring through a systematic, tool-supported process that the system complies with all elements of a relevant law. To deal with the problem, we build a model of the law and contrast it with a model of the requirements of the system. In earlier work, we proposed a modelling language for law (Nomos 2) along with a reasoning mechanism that answers questions about compliance. In this paper we extend Nomos 2 to include the concepts of role and requirement so that we can reason about compliance in specific domains. Also, Nomos 3 represents the distribution of responsibilities to roles, distinguishing social from legal roles. Nomos 3 models allow us to reason about compliance of requirements and roles with the norms that constitute a law. A small case study is used to illustrate the elements of Nomos 3 and the kinds of reasoning it supports.}}
@ARTICLE{Campbell_1984,title={Data processing and the law},year={1984},author={Colm Campbell and C. M. Campbell},doi={null},pmid={null},pmcid={null},mag_id={396555231},journal={null},abstract={null}}
@ARTICLE{Hage_1996,title={Reasoning with rules : an essay on legal reasoning and its underlying logic},year={1996},author={Jaap Hage and Jaap Hage},doi={null},pmid={null},pmcid={null},mag_id={560017174},journal={null},abstract={Preface. I: Introduction. II: Of Reasons. A. Philosophical Roots. B. A Causal Account of Reasons. C. Reasons and Principles. D. Kinds of Reasons. III: Reasoning with Rules. IV: Reason-Based Logic. V: Applications of Reason-Based Logic. VI: Concluding Observations. A. Possible Amendments to Reason-Based Logic. B. Semantics for Rules. C. Reason-Based Logic in Comparison. D. Two Metaphors of Reasoning. References. Index of Names. Index of Subjects.}}
@ARTICLE{Tarello_1980,title={L'interpretazione della legge},year={1980},author={Giovanni Tarello and Giovanni Tarello},doi={null},pmid={null},pmcid={null},mag_id={561017678},journal={null},abstract={null}}
@ARTICLE{Bing_1977,title={Legal decisions and information systems},year={1977},author={Jon Bing and Jon Bing and Jon Bing and Trygve Harvold and Trygve Harvold},doi={null},pmid={null},pmcid={null},mag_id={561674077},journal={null},abstract={null}}
@ARTICLE{Dimock_2006,title={Classic Readings and Cases in the Philosophy of Law},year={2006},author={Susan Dimock and Susan Dimock and Susan Dimock},doi={null},pmid={null},pmcid={null},mag_id={571028914},journal={null},abstract={PART 1: WHAT IS LAW? Chapter 1: Traditional Natural Law Theory: Law for the Common Good St. Thomas Aquinas, Law for the Common Good Chapter 2: Legal Positivism I: Law as Command John Austin, The Command Theory of Law Chapter 3: American Legal Realism: Law as Judicial Pronouncement Oliver Wendell Holmes, Law as Systematized Prediction of What the Courts Will Do Jerome Frank, Law as the Product of Court Decisions Chapter 4: Legal Positivism II: Law as the Union of Primary and Secondary Rules H.L.A. Hart, Law as the Union of Primary and Secondary Rules Chapter 5: Law and Economics: Law as Efficiency Susan Dimock, Law and Economics Chapter 6: Feminist Jurisprudence: Law as a Patriarchal Institution Patricia Smith, Law as a Patriarchal Institution Catharine A. MacKinnon, Law as Male Power Additional Readings Cases for Discussion Palsgraf v. Long Island Rail Road Co. Lynch v. Fisher Hammontree v. Jenner Stewart v. Dutra Construction Co. Stockberger V. United States McFall v. Shimp Farwell v. Keaton Berman v. Allan Sindell v. Abbott Laboratories Moore v. Regents of the University of California Kowalski v. Tesmer Penn Central Transportation Co. v. New York City Kelo v. City of New London PART 2: THE SEPARATION THESIS, LEGAL REASONING AND LEGAL INDETERMINACY: H.L.A. Hart and His Critics Chapter 7: The Separation of Law and Morality H.L.A. Hart, Positivism and the Separation of Law and Morals Chapter 8: The Morality of Law Lon L. Fuller, Positivism and Fidelity to Law-A Reply to Professor Hart The Morality that Makes Law Possible Chapter 9: Law as a System of Rights Ronald Dworkin, Rules, Principles, and Rights Hard Cases Integrity in Law Chapter 10: Hart's Response to Dworkin H.L.A. Hart, Defending Legal Positivism Chapter 11: Law as an Indeterminate Patchwork of Irreconcilable Ideologies Andrew Altman, Legal Realism, Critical Legal Studies, and Dworkin Critical Legal Studies and the Rule of Law Additional Readings Cases for Discussion Riggs v. Palmer State of Maryland v. Rusk Raich v. Ashcroft Small v. United States Korematsu v. United States Plessey v. Ferguson Brown v. Board of Education United States v. Virginia Hopwood v. Texas Grutter v. Bollinger Michael M. v. Sonoma County Personnel Administrator of Massachusetts v. Feeney Afroyim v. Rusk PART 3: CIVIL DISOBEDIENCE AND THE OBLIGATION TO OBEY LAW Chapter 12: The Duty to Oppose Injustice Martin Luther King, Jr., Letter from Birmingham Jail Chapter 13: Civil Disobedience and Conscientious Refusal John Rawls, Civil Disobedience and Conscientious Refusal Chapter 14: The Benefit of Challenging Uncertain Laws Ronald Dworkin, Civil Disobedience Additional Readings Cases for Discussion Schenck v. United States Whitney v. California Walker v. City of Birmingham Minersville School District, Board of Education v. Gobitis Wisconsin v. Yoder Employment Division, Dept. of Human Resources of Oregon v. Smith United States v. Schoon PART 4: LAW AND LIBERTY Chapter 15: Civil Disobedience and the Presumption of an Obligation to Obey the Law Chapter 16: In Defense of Liberty John Stuart Mill, On Liberty Chapter 17: Paternalism Gerald Dworkin, Paternalism Chapter 18: Legal Moralism Lord Patrick Devlin, The Enforcement of Morals Chapter 19: A Refutation of Legal Moralism H.L.A. Hart, Law, Liberty, and Morality Additional Readings Cases for Discussion John Doe v. University of Michigan Texas v. Johnson Chen v. California New York Times v. Sullivan New York Times Co. v. United States Village of Skokie v. National Socialist Party of America Hernandez v. Commonwealth of Virginia Boy Scouts of America v. Dale Miller v. California Paris Adult Theater I v. Slaton Reno v. American Civil Liberties Union Engel v. Vitale Edwards v. Aguillard Van Orden v. Perry Griswold v. Connecticut Roe v. Wade Planned Parenthood v. Casey Bowers v. Hardwick Lawrence v. Texas Loving v. Virginia Goodridge v. Department of Public Health PART 5: PUNISHMENT Chapter 20: Utilitarianism Jeremy Bentham, An Introduction to the Principles of Morals and Legislation Chapter 21: Retributivism I: A Kantian Theory of Punishment Immanuel Kant, The Right of Punishing and Pardoning Chapter 22: Retributivism II: Fair Play Herbert Morris, Persons and Punishment Chapter 23: Retributivism III: The Value of Victims Jean Hampton, A New Theory of Retribution Chapter 24: Restitution Randy Barnett, Restitution: A New Paradigm of Criminal Justice Chapter 25: Restorative Justice Gorden Bazemore, Three Paradigms for Juvenile Justice Additional Readings Cases for Discussion Miranda v. Arizona Gregg v. Georgia McCleskey v. Kemp Atkins v. Virginia Roper v. Simons Rummel v. Estelle Hamdi v. Rumfeld The Insanity Defense: M'Naghten and Durham State v. Kelly United States v. Oviedo PART 6: CONSTIUTIONAL INTERPRETATION Chapter 26: The Moral Reading of the American Constitution Ronal Dworkin, The Moral Reading of the American Constitution Appendix The Bill of Rights and other Amendments to the Constitution of the United States Selections from the Canadian Charter of Rights and Freedoms Glossary}}
@ARTICLE{Tolman_1954,title={The Federal Courts and the Federal System},year={1954},author={Leland L. Tolman and Leland L. Tolman and Warner W. Gardner and Warner W. Gardner and Henry M. Hart join( and Henry M. Hart join( and Herbert Wechsler and Herbert Wechsler},doi={10.2307/1119559},pmid={null},pmcid={null},mag_id={572621280},journal={California Law Review},abstract={null}}
@ARTICLE{Alexander_2001,title={The Rule of Rules: Morality, Rules, and the Dilemmas of Law},year={2001},author={Larry Alexander and Emily Sherwin and Emily Sherwin},doi={null},pmid={null},pmcid={null},mag_id={590922284},journal={null},abstract={Rules perform a moral function by restating moral principles in concrete terms, so as to reduce the uncertainty, error, and controversy that result when individuals follow their own unconstrained moral judgment. Although reason dictates that we must follow rules to avoid destructive error and controversy, rules—and hence laws—are imperfect, and reason also dictates that we ought not follow them when we believe they produce the wrong result in a particular case. In The Rule of Rules Larry Alexander and Emily Sherwin examine this dilemma.

Once the importance of this moral and practical conflict is acknowledged, the authors argue, authoritative rules become the central problems of jurisprudence. The inevitable gap between rules and background morality cannot be bridged, they claim, although many contemporary jurisprudential schools of thought are misguided attempts to do so. Alexander and Sherwin work through this dilemma, which lies at the heart of such ongoing jurisprudential controversies as how judges should reason in deciding cases, what effect should be given to legal precedent, and what status, if any, should be accorded to “legal principles.” In the end, their rigorous discussion sheds light on such topics as the nature of interpretation, the ancient dispute among legal theorists over natural law versus positivism, the obligation to obey law, constitutionalism, and the relation between law and coercion.

Those interested in jurisprudence, legal theory, and political philosophy will benefit from the edifying discussion in The Rule of Rules.}}
@ARTICLE{Palmieri_2015,title={Handling Regulatory Goal Model Families as Software Product Lines},year={2015},author={Anthony Palmieri and Anthony Palmieri and Anthony Palmieri and Philippe Collet and Philippe Collet and Daniel Amyot and Daniel Amyot},doi={10.1007/978-3-319-19069-3_12},pmid={null},pmcid={null},mag_id={600309345},journal={null},abstract={Goal models can capture the essence of legal and regulation statements and many of their relationships, enabling compliance analysis. However, current goal modeling approaches do not scale well when handling large regulations with many variable parts that depend on different aspects of regulated organizations. In this paper, we propose a tool-supported approach that integrates the Goal-oriented Requirement Language and feature modeling to handle regulatory goal model families. We show how they can be organized as a Software Product Line (SPL), ensuring the consistency of the SPL as a whole, and providing an adapted derivation process associated to a feature model configuration. The proposed approach is also evaluated on large generated SPLs with results suggesting its capability to address scalability concerns.}}
@ARTICLE{Michie_1979,title={Expert systems in the micro-electronic age},year={1979},author={Donald Michie and Donald Michie},doi={null},pmid={null},pmcid={null},mag_id={611848211},journal={null},abstract={null}}
@ARTICLE{Chafee_1931,title={The principles of judicial proof or the process of proof as given by logic, psychology, and general experience and illustrated in judicial trials},year={1931},author={Zechariah Chafee and Z. Chafee join( and John Henry Wigmore and John Henry Wigmore},doi={10.2307/3308605},pmid={null},pmcid={null},mag_id={631826017},journal={null},abstract={null}}
@ARTICLE{Bundy_1983,title={Proceedings of the Eighth International Joint Conference on Artificial Intelligence : IJCAI-83, 8-12 August 1983, Karlsruhe, West Germany},year={1983},author={Alan Bundy and Alan Bundy},doi={null},pmid={null},pmcid={null},mag_id={638760486},journal={null},abstract={null}}
@ARTICLE{Perroni_2013,title={Play: Psychoanalytic Perspectives, Survival and Human Development},year={2013},author={Emilia Perroni and Emilia Perroni},doi={10.4324/9780203547182},pmid={null},pmcid={null},mag_id={641435010},journal={null},abstract={Section I. The Origins of Play and the Play Space. Chapter One: Listening. Perroni, Introduction: On Listening. Ofarim, Listening in Parenting and Therapy as a Life-Giving Container and as Preparation for the Capacity to Play. Zakai, Hearing, Listening, Being Attentive and Everything in Between. Chapter Two: Psychoanalysis and Play. Perroni, Introduction: Play from Freud to Winnicott. Lurie, Play as a World of Magic and Drama: Winnicott's Ideas about Play and their Application to Children's Psychotherapy. Kulka, Psyche or Soul in Psychoanalysis: Towards the Conceptualization of Play as a Psychoanalytic Transcendental Selfobject. Chapter Three: Space and Play. Perroni, Introduction: Play as a Movement of the Soul: Some Thoughts on Order and Disorder. Bauman, Rites and Games for Creating Sacred Holy Space. Levy, The Dream's Navel and the Hunt in the Forest: Comments on the Structure of Space. Section II. Play, War and Survival. Chapter Four: Survival, Motherhood and Play. Perroni, Introduction: Some Observations on the Exhibition "There are no Childish Games," at the Holocaust Museum of Yad Vashem in Jerusalem. Rosner, Playing in the Shadow of the Holocaust: Memories from a Hiding Place - A Personal Testimony. Hopp, Creativity and Play in the Shadow of War: A Discussion of "The Big Notebook" by Agota Kristof. Chapter Five: War and Play. Perroni, Introduction: The Concept of Enemy. Mann, Why War? Between Transformational and Terminal Links in the Field of Therapeutic Play and Beyond. Porat, Meltzer, Images of War and Images of Peace in Sand-Play Therapy. Section III. Play and Fatherhood. Chapter Six: Fathers and Sons. Perroni, Introduction: Fatherhood or Motherhood? Bernstein, Daedalus and Icarus: Thoughts on Relations between Fathers and Adolescent Sons. Munk, Father's Truth-and-Lies Game: On Peter Weir's 'The Truman Show'. Section IV. Play and the Theatre Arts. Chapter Seven: The Theater and Play. Perroni, Introduction: To Act and to Tell. Bar Giora, Children and Theater. Meiri, The Actor as an Eternal Child: The Role of Acting in the Training of the Actors. Chapter Eight: Play and Masquerading. Perroni, Introduction: The Play of the Soul behind the Mask. Raz, The Mask. Ankory, Carnival: The Return to Chaos. Perroni, Afterword - On a Personal Note.}}
@ARTICLE{Akhigbe_2015,title={Information Technology Artifacts in the Regulatory Compliance of Business Processes: A Meta-Analysis},year={2015},author={Okhaide Akhigbe and Okhaide Akhigbe and Daniel Amyot and Daniel Amyot and Gregory Richards and Gregory Richards},doi={10.1007/978-3-319-17957-5_6},pmid={null},pmcid={null},mag_id={641753920},journal={null},abstract={To address recent calls for better regulatory compliance of business processes (electronic or not), this study synthesizes research findings of nine peer-reviewed systematic literature reviews and state-of-the-art studies on business process compliance to examine Information Technology artifacts used in the compliance of business processes to laws and regulations. Results from the 342 unique studies covered by these reviews demonstrate that eight types of Information Technology artifacts are used in the design, execution and after-execution lifecycle phases of regulatory compliance to perform compliance modeling, checking, analysis and enactment tasks. The results also demonstrate with statistical evidence that compliance enactment (which involves putting mechanisms in place to address changes in the compliance representations, business processes or regulatory requirements) has received the least attention. This is a worrisome situation given the dynamic environment in which businesses operate. We argue that the findings in this work are relevant toward understanding the current state of the domain, providing insights on what phases and tasks need improvement.}}
@ARTICLE{Feteris_1999,title={Fundamentals of Legal Argumentation: A Survey of Theories on the Justification of Judicial Decisions},year={1999},author={Eveline T. Feteris and Eveline T. Feteris},doi={null},pmid={null},pmcid={null},mag_id={650096817},journal={null},abstract={Preface. Introduction. 1. Research Into Legal Argumentation. 2. A Survey of Approaches and Topics. 3. The Logical Approach. 4. Toulmin's Argumentation Model. 5. Perelman's New Rhetoric. 6. Habermas' Theory of Communicative Rationality. 7. MacCormick's Theory of the Justification of Legal Decisions. 8. Alexy's Procedural Theory of Legal Argumentation. 9. Aarnio's Theory of the Justification of Legal Interpretations. 10. Peczenik's Theory of Transformations in the Law. 11. The Pragma-Dialectical Theory of Legal Argumentation in the Context of a Critical Discussion. 12. Towards a Theory of Legal Argumentaton in the Context of a Critical Discussion. Bibliography. Author Index. Subject Index.}}
@ARTICLE{Walton_2005,title={Fundamentals of critical argumentation},year={2005},author={Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={651638839},journal={null},abstract={1. Arguments and dialogues 2. Concepts useful for understanding arguments 3. Argumentation schemes 4. Argument reconstruction 5. Dialogues 6. Detecting bias 7. Relevance 8. Practical reasoning in a dialogical framework.}}
@ARTICLE{심헌섭_1983,title={Robert Alexy, Theorie der juristischen Argumentation (Die Theorie der rationalen Diskurses als Theorie der juristischen Begrundung) Suhrkamp Verlag, Frankfurt/M 1978},year={1983},author={심헌섭 and 심헌섭},doi={null},pmid={null},pmcid={null},mag_id={658969731},journal={null},abstract={null}}
@ARTICLE{Vanbelle_2009,title={Agreement between raters and groups of raters},year={2009},author={Sophie Vanbelle and Sophie Vanbelle},doi={null},pmid={null},pmcid={null},mag_id={781776930},journal={null},abstract={null}}
@ARTICLE{Mishra_2008,title={Issues with Incorporating Regulatory Compliance into Agile Development: A Critical Analysis},year={2008},author={Sushma Mishra and Sushma Mishra and Heinz Roland Weistroffer and H. Roland Weistroffer},doi={null},pmid={null},pmcid={null},mag_id={792226912},journal={null},abstract={Agile development methodology is widely used for software development in organizations. Incorporating regulatory compliance aspects in development process is important. This paper discusses various issues in considering compliance aspects into development process. An analysis of different aspects of compliance related issues is presented.}}
@ARTICLE{Fitzgerald_2017,title={Continuous software engineering: A roadmap and agenda},year={2017},author={Brian Fitzgerald and Brian Fitzgerald and Klaas-Jan Stol and Klaas-Jan Stol},doi={10.1016/j.jss.2015.06.063},pmid={null},pmcid={null},mag_id={806202468},journal={Journal of Systems and Software},abstract={null}}
@ARTICLE{Boella_2010,title={A formal study on legal compliance and interpretation},year={2010},author={Guido Boella and Guido Boella and Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Leendert van der Torre and Leendert van der Torre},doi={null},pmid={null},pmcid={null},mag_id={921941780},journal={null},abstract={This paper proposes a logical framework to capture the norm change power and the limitations of the judicial system in revising the set of constitutive rules defining the concepts on which the applicability of norms is based. In particular, we reconstruct the legal arguments leading to an extensive or restrictive interpretation of norms.}}
@ARTICLE{A_1992,title={An expertext system for building standards},year={1992},author={Casson A and Stone D},doi={null},pmid={null},pmcid={null},mag_id={933819360},journal={null},abstract={The paper describes work on a current collaborative research project involving UK Universities and the Scottish Office's Building Directorate. Previous work by the Directorate had identified limitations in a conventional expert systems approach to the development and management of building Standards information. Work in the current project seeks to overcome these limitations by combining representational paradigms. In particular, a system is being designed and implemented which is based on and seeks to extend the concept of Headed Record Expertext (Diaper 90) in which formalised information can be attached, where appropriate, to nodes in a hypertext version of the Standards. The systemalso handles navigation rules and provides intelligent guidance for the reader through the hyperdocument. The underlying node/arc representational structure is also intended to support the capture of argumentation material generated during the development, maintenance and use ofStandardsinformation.}}
@ARTICLE{Bench‐Capon_2003,title={Computational Models, Argumentation Theories and Legal Practice},year={2003},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and James B. Freeman and James B. Freeman and Hanns Hohmann and Hanns Hohmann and Henry Prakken and Henry Prakken},doi={10.1007/978-94-017-0431-1_4},pmid={null},pmcid={null},mag_id={988889973},journal={null},abstract={In this chapter we will draw upon insights from computational approaches and argumentation theories to create a framework for the rational reconstruction of legal argument. Taking the perspective of a lawyer we develop a conceptual model intended to accommodate all stages involved in legal argument. We then relate Argumentation Theory and work in AI and Law to this conceptual model. We conclude by considering the scope for the two disciplines learning from one another, and by drawing attention to areas that we believe offer fruitful opportunities for inter-disciplinary research.}}
@ARTICLE{Baldacchino_2016,title={Variational Bayesian mixture of experts models and sensitivity analysis for nonlinear dynamical systems},year={2016},author={Tara Baldacchino and Tara Baldacchino and Elizabeth J. Cross and Elizabeth J. Cross and Keith Worden and Keith Worden and Jennifer Rowson and Jennifer Rowson},doi={10.1016/j.ymssp.2015.05.009},pmid={null},pmcid={null},mag_id={1190147142},journal={Mechanical Systems and Signal Processing},abstract={null}}
@ARTICLE{Saeki_2009,title={Checking Regulatory Compliance of Business Processes and Information Systems},year={2009},author={Motoshi Saeki and Motoshi Saeki and Motoshi Saeki and Motoshi Saeki and Haruhiko Kaiya and Haruhiko Kaiya and Satoshi Hattori and Satoshi Hattori},doi={10.1007/978-3-642-20116-5_6},pmid={null},pmcid={null},mag_id={1415187483},journal={null},abstract={In these years, many laws and regulations are being enacted to prevent business processes (BPs) and information systems (ISs) from their malicious users. As a result, it is significant for organizations to ensure that their BPs and ISs comply with these regulations. This paper proposes a technique to apply a formal technique to ensure the regulatory compliance of BP or IS descriptions written in use case models. We translate the use case models of the behavior of BPs and ISs into finite state transition machines. Regulations are represented with computational tree logic (CTL) and their satisfiability are automatically verified using a model checker SMV. The modality of regulations can be specified with temporal operators based on branching time semantics of the CTL in our technique.}}
@ARTICLE{Rochat_2015,title={Layers of awareness in development},year={2015},author={Philippe Rochat and Philippe Rochat},doi={10.1016/j.dr.2015.07.009},pmid={null},pmcid={null},mag_id={1467646731},journal={Developmental Review},abstract={null}}
@ARTICLE{Gog_2001,title={Modeling legislation using natural language processing},year={2001},author={Ron van Gog and R. Van Gog and Tom van Engers and T.M. Van Engers},doi={10.1109/icsmc.2001.969873},pmid={null},pmcid={null},mag_id={1483691238},journal={null},abstract={This paper describes the possibilities of the translation of legislation, which is written in natural language, into a formal language, i.e. UML/OCL. The tool OPAL (Object-oriented Parsing and Analysis of Legislation) is developed to support the automatic modelling of legislation with the use of appropriate NLP techniques. The aim is not to perform this modelling in a batch fashion from legislation to final model, but interactively in dialogue with the knowledge engineer. The main components of OPAL are a parser (based on a chart-parser algorithm) and a model generator. A special component called modelling interface is added to OPAL to give the knowledge engineer the possibility to keep track of the modelling process and to make adjustments to the final model.}}
@ARTICLE{Brewka_2010,title={Abstract dialectical frameworks},year={2010},author={Gerhard Brewka and Gerhard Brewka and Stefan Woltran and Stefan Woltran},doi={null},pmid={null},pmcid={null},mag_id={1486709607},journal={null},abstract={In this paper we introduce dialectical frameworks, a powerful generalization of Dung-style argumentation frameworks where each node comes with an associated acceptance condition. This allows us to model different types of dependencies, e.g. support and attack, as well as different types of nodes within a single framework. We show that Dung's standard semantics can be generalized to dialectical frameworks, in case of stable and preferred semantics to a slightly restricted class which we call bipolar frameworks. We show how acceptance conditions can be conveniently represented using weights respectively priorities on the links and demonstrate how some of the legal proof standards can be modeled based on this idea.}}
@ARTICLE{Moulin_2002,title={Explanation and Argumentation Capabilities: Towards the Creation of More Persuasive Agents},year={2002},author={Bernard Moulin and Bernard Moulin and Hengameh Irandoust and H. Irandoust and H. Irandoust and Hengameh Irandoust and Micheline Bélanger and M. Bélanger and G. Desbordes and Gaëlle Desbordes},doi={10.1023/a:1015023512975},pmid={null},pmcid={null},mag_id={1487234737},journal={Artificial Intelligence Review},abstract={During the past two decades many research teams have worked on the enhancement of the explanation capabilities of knowledge-based systems and decision support systems. During the same period, other researchers have worked on the development of argumentative techniques for software systems. We think that it would be interesting for the researchers belonging to these different communities to share their experiences and to develop systems that take advantage of the advances gained in each domain.

We start by reviewing the evolution of explanation systems from the simple reasoning traces associated with early expert systems to recent research on interactive and collaborative explanations. We then discuss the characteristics of critiquing systems that test the credibility of the user's solution. The rest of the paper deals with the different application domains that use argumentative techniques. First, we discuss how argumentative reasoning can be captured by a general structure in which a given claim or conclusion is inferred from a set of data and how this argument structure relates to pragmatic knowledge, explanation production and practical reasoning. We discuss the role of argument in defeasible reasoning and present some works in the new field of computer-mediated defeasible argumentation. We review different application domains such as computer-mediated communication, design rationale, crisis management and knowledge management, in which argumentation support tools are used. We describe models in which arguments are associated to mental attitudes such as goals, plans and beliefs. We present recent advances in the application of argumentative techniques to multi-agent systems. Finally, we propose research perspectives for the integration of explanation and argumentation capabilities in knowledge-based systems and make suggestions for enhancing the argumentation and persuasion capabilities of software agents.}}
@ARTICLE{Gangemi_2005,title={A constructive framework for legal ontologies},year={2005},author={Aldo Gangemi and Aldo Gangemi and Maria-Teresa Sagri and Maria-Teresa Sagri and Daniela Tiscornia and Daniela Tiscornia},doi={10.1007/978-3-540-32253-5_7},pmid={null},pmcid={null},mag_id={1487379640},journal={null},abstract={The increasing development of legal ontologies seems to offer interesting solutions to legal knowledge formalization, which in past experiences lead to a limited exploitation of legal expert systems for practical use. The paper describes how a constructive approach to ontology can provide useful components to create newly designed legal decision support systems either as local or Web-based semantic services. We describe the relation of our research to AI&Law and legal philosophy, the components of our Core Legal Ontology, the JurWordNet semantic lexicon, and some examples of use of legal ontologies for both norm conformity and compatibility. Our legal ontologies are based on DOLCE+, an extension of the DOLCE foundational ontology developed in the WonderWeb and Metokis EU projects.}}
@ARTICLE{Alexy_1985,title={Theorie der Grundrechte},year={1985},author={Robert Alexy and Robert Alexy},doi={null},pmid={null},pmcid={null},mag_id={1487979485},journal={null},abstract={null}}
@ARTICLE{Sterling_1994,title={The Art of Prolog: Advanced Programming Techniques},year={1994},author={Leon Sterling and Leon Sterling and Ehud Shapiro and Ehud Shapiro},doi={null},pmid={null},pmcid={null},mag_id={1488560674},journal={null},abstract={Part 1 Logic programs: basic constructs database programming recursive programming the computation model of logic programs theory of logic programs. Part 2 The Prolog language: pure Prolog programming in pure Prolog arithmetic structure inspection meta-logical predicates cuts and negation extra-logical predicates program development. Part 3 Advanced Prolog programming techniques: nondeterministic programming incomplete data structures second-order programming interpreters program transformation logic grammars search techniques. Part 4 Applications: game-playing programs a credit evaluation expert system an equation solver a compiler. Appendix: operators.}}
@ARTICLE{Corchado_2004,title={Development of CBR-BDI Agents: A Tourist Guide Application},year={2004},author={Juan M. Corchado and Juan M. Corchado and Juán Pavón and Juan Pavón and Emilio Corchado and Emilio Corchado and Laura Castillo and Luis Fernando Castillo},doi={10.1007/978-3-540-28631-8_40},pmid={null},pmcid={null},mag_id={1489361343},journal={Lecture Notes in Computer Science},abstract={In this paper we present an agent-based application of a wireless tourist guide that combines the Beliefs-Desires-Intentions approach with learning capabilities of Case Base Reasoning techniques. This application shows how to develop adaptive agents with a goal driven design and a decision process built on a CBR architecture. The resulting agent architecture has been validated by real users who have used the tourist guide application, on a mobile device, and can be generalized for the development of other personalized services.}}
@ARTICLE{Shavell_2004,title={Foundations of Economic Analysis of Law},year={2004},author={Steven Shavell and Steven Shavell},doi={null},pmid={null},pmcid={null},mag_id={1490508259},journal={null},abstract={This book provides a comprehensive, economically-oriented treatment of the basic areas of law, namely, property law, tort law, contract law, and criminal law. The book also deals with the litigation process and contains a part on welfare economics, morality, and law. Aimed at a broad audience, neither a legal background nor technical economic knowledge is needed to read it.}}
@ARTICLE{Maxwell_2008,title={Concept and Context in Legal Information Retrieval},year={2008},author={K. Tamsin Maxwell and K. Tamsin Maxwell and Burkhard Schäfer and Burkhard Schafer},doi={null},pmid={null},pmcid={null},mag_id={1491150152},journal={null},abstract={There exist two broad approaches to information retrieval (IR) in the legal domain: those based on manual knowledge engineering (KE) and those based on natural language processing (NLP). The KE approach is grounded in artificial intelligence (AI) and case-based reasoning (CBR), whilst the NLP approach is associated with open domain statistical retrieval. We provide some original arguments regarding the focus on KE-based retrieval in the past and why this is not sustainable in the long term. Legal approaches to questioning (NLP), rather than arguing (CBR), are proposed as the appropriate jurisprudential and cognitive underpinning for legal IR. Recall within the context of precision is proposed as a better fit to law than the 'total recall' model of the past, wherein conceptual and contextual search are combined to improve retrieval performance for both parties in a dispute.}}
@ARTICLE{Rissland_2009,title={Black Swans, Gray Cygnets and Other Rare Birds},year={2009},author={Edwina L. Rissland and Edwina L. Rissland},doi={10.1007/978-3-642-02998-1_2},pmid={null},pmcid={null},mag_id={1492353967},journal={null},abstract={Surprising, exceptional cases -- so-called black swans -- can provoke extraordinary change in the way we do things or conceptualize the world. While it is not unreasonable to be surprised by a black swan, to be surprised by subsequent cases that are similar enough that they might cause the same sort of upheaval is unforgivable. The problem is how to reason about these almost novel, not totally unforeseen, subsequent cases that I call gray cygnets.}}
@ARTICLE{Καρούσος_2010,title={Usability Evaluation of Web-Based Collaboration Support Systems: The Case of CoPe_it!},year={2010},author={Νίκος Καρούσος and Nikos Karousos and Spyridon Papaloukas and Spyridon Papaloukas and Nektarios Kostaras and Nektarios Kostaras and Michalis Xenos and Michalis Xenos and Manolis Tzagarakis and Manolis Tzagarakis and Nikos Karacapilidis and Nikos Karacapilidis},doi={10.1007/978-3-642-16318-0_28},pmid={null},pmcid={null},mag_id={1493956528},journal={null},abstract={Usability is considered as a very significant factor towards the wide acceptance of software applications. Although the usability evaluation can take place in different forms, the entire evaluation procedure usually follows predefined ways according to a classification of the common characteristics of software applications. However, contemporary Web 2.0 applications, which aim at both social network development and collaboration support, reveal the need for modifying the settings of the evaluation procedure. This is due to some unique characteristics of these applications, such as the support of both synchronous and asynchronous collaboration, the use of common spaces for working and information exchanging, and the advanced notification and awareness services. This paper explores these applications’ particularities with respect to the way the whole usability evaluation procedure is affected and proposes a composite evaluation technique based on the development of appropriate heuristics that is suitable for such cases. The aforementioned issues are elaborated through the case of CoPe_it!, a Web 2.0 tool that facilitates and enhances argumentative collaboration.}}
@ARTICLE{Elhauge_2008,title={Statutory Default Rules: How to Interpret Unclear Legislation},year={2008},author={Einer Elhauge},doi={null},pmid={null},pmcid={null},mag_id={1495389515},journal={null},abstract={1. Introduction and Overview 2. Why Courts Should Maximize Enactable Preferences when Statutes Are Unclear Part I. Current Preferences Default Rules 3. The General Theory for Current Preferences Default Rules 4. Inferring Current Preferences from Recent Legislative Action 5. Inferring Current Preferences from Agency Action Part II. Enactor Preferences Default Rules 6. From Legislative Intent to Probabilistic Estimates of Enactable Preferences 7. Moderation, Changed or Uncontemplated Circumstances, and a Theory of Meaning Part III. Preference-Eliciting Default Rules 8. Eliciting Legislative Preferences 9. Canons Favoring the Politically Powerless 10. Linguistic Canons of Statutory Construction 11. Interpretations that May Create International Conflict 12. Explaining Seeming Inconsistencies in Statutory Stare Decisis Part IV. Supplemental Default Rules 13. Tracking the Preferences of Political Subunits 14. Tracking High Court Preferences Part V. Objections 15. The Fit with Prior Political Science Models and Empirical Data 16. The Critiques of Politics by Interest Group Theory and Collective Choice Theory 17. Alternative Default Rules that Protect Reliance or Avoid Change or Effect 18. Rebutting Operational and Jurisprudential Objections Notes Index}}
@ARTICLE{Prakken_2006,title={Presumptions and Burdens of Proof},year={2006},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.2139/ssrn.963761},pmid={null},pmcid={null},mag_id={1496982372},journal={null},abstract={This paper studies the logical modelling of presumptions and their effects on the burden of proof. Presumptions are modelled as default rules and their effect on the burden of proof is defined in terms of a distinction between the burden of production, the burden of persuasion and the tactical burden of proof. These notions are logically characterised in such a way that presumptions enable a party to fulfil a burden of production or persuasion while shifting a tactical burden to the other party. Finally, it is shown how debates about what can be presumed can be modelled as debates about the backings of default rules.}}
@ARTICLE{Governatori_1996,title={Labelling Ideality and Subideality},year={1996},author={Guido Governatori and Guido Governatori},doi={10.1007/3-540-61313-7_80},pmid={null},pmcid={null},mag_id={1498006498},journal={null},abstract={In this paper we suggest ways in which logic and law may usefully relate; and we present an analytic proof system dealing with the Jones Porn's deontic logic of Ideality and Subideality, which offers some suggestions about how to embed legal systems in label formalism.}}
@ARTICLE{Eskridge_1994,title={Dynamic statutory interpretation},year={1994},author={William N. Eskridge and William N. Eskridge},doi={null},pmid={null},pmcid={null},mag_id={1498667339},journal={null},abstract={Acknowledgments Introduction: Why Statutory Interpretation Is Worth a Book I: The Practice of Dynamic Statutory Interpretation 1. The Insufficiency of Statutory Archaeology 2. The Dynamics of Statutory Interpretation 3. A Case Study: Labor Injunction Decisions, 1877-1938 II: Jurisprudential Theories for Reading Statutes Dynamically 4. Liberal Theories 5. Legal Process Theories 6. Normativist Theories III: Doctrinal Implications of Dynamic Statutory Jurisprudence 7. Legislative History Values 8. Vertical versus Horizontal Coherence 9. Canons of Statutory Construction as Interpretive Regimes Appendix 1 The Primary Legislative Inaction Precedents, 1962-1992 Appendix 2 Supreme Court Decisions Overruling Statutory Precedents, 1962-1992 Appendix 3 The Rehnquist Court's Canons of Statutory Construction Notes Index}}
@ARTICLE{Lu_2007,title={Compliance aware business process design},year={2007},author={Ruopeng Lu and Ruopeng Lu and Shazia Sadiq and Shazia Sadiq and Guido Governatori and Guido Governatori},doi={10.1007/978-3-540-78238-4_14},pmid={null},pmcid={null},mag_id={1498838992},journal={null},abstract={Historically, business process design has been driven by business objectives, specifically process improvement. However this cannot come at the price of control objectives which stem from various legislative, standard and business partnership sources. Ensuring the compliance to regulations and industrial standards is an increasingly important issue in the design of business processes. In this paper, we advocate that control objectives should be addressed at an early stage, i.e., design time, so as to minimize the problems of runtime compliance checking and consequent violations and penalties. To this aim, we propose supporting mechanisms for business process designers. This paper specifically presents a support method which allows the process designer to quantitatively measure the compliance degree of a given process model against a set of control objectives. This will allow process designers to comparatively assess the compliance degree of their design as well as be better informed on the cost of non-compliance.}}
@ARTICLE{Kolodner_1993,title={Case-based reasoning},year={1993},author={Janet L. Kolodner and Janet L. Kolodner},doi={null},pmid={null},pmcid={null},mag_id={1500151553},journal={null},abstract={Case-based reasoning is one of the fastest growing areas in the field of knowledge-based systems and this book, authored by a leader in the field, is the first comprehensive text on the subject. Case-based reasoning systems are systems that store information about situations in their memory. As new problems arise, similar situations are searched out to help solve these problems. Problems are understood and inferences are made by finding the closest cases in memory, comparing and contrasting the problem with those cases, making inferences based on those comparisons, and asking questions when inferences can't be made. This book presents the state of the art in case-based reasoning. The author synthesizes and analyzes a broad range of approaches, with special emphasis on applying case-based reasoning to complex real-world problem-solving tasks such as medical diagnosis, design, conflict resolution, and planning. The author's approach combines cognitive science and engineering, and is based on analysis of both expert and common-sense tasks. Guidelines for building case-based expert systems are provided, such as how to represent knowledge in cases, how to index cases for accessibility, how to implement retrieval processes for efficiency, and how to adapt old solutions to fit new situations. This book is an excellent text for courses and tutorials on case-based reasoning. It is also a useful resource for computer professionals and cognitive scientists interested in learning more about this fast-growing field.}}
@ARTICLE{Bing_1984,title={Handbook of Legal Information Retrieval},year={1984},author={Jon Bing and Jon Bing},doi={null},pmid={null},pmcid={null},mag_id={1500304810},journal={null},abstract={null}}
@ARTICLE{Scott_2000,title={Is Too Much Privacy Bad for Your Health? An Introduction to the Law, Ethics and HIPAA Rule on Medical Privacy},year={2000},author={Charity Scott and Charity Scott},doi={10.1385/1-59259-089-6:3},pmid={null},pmcid={null},mag_id={1500890771},journal={Georgia State University law review},abstract={After years of Congressional debates and agency rule-making, the HIPAA health care privacy rule has finally become effective. Why did it take so long? Many of the issues which deadlocked Congressional attempts to pass federal privacy legislation reflect deep disagreements over how much individual privacy should be protected. On one hand, patients often believe that no one except their closest health caregivers should be able to see their medical records without their permission. On the other hand, many other people and entities, mostly strangers to the patient, believe they should have access to those records without having to ask permission. They argue that such relatively free access to individual health care information is good for the patient, good for other patients, and good for society - in other words, that too much privacy protection is bad for our individual and collective health. Indeed, society has often tolerated sacrifices in individual medical privacy in order to promote the public good. This article examines the ethical trade-offs that we have made between the benefits of privacy protection and the benefits of sacrificing it in the name of social welfare, such as advances in scientific research, protection of the public health, higher efficiency and better quality in the delivery of health care, and even improved law enforcement. It analyzes how these ethical trade-offs have been reflected in our laws, Congressional debates, and recent HIPAA privacy rule. It also examines several key issues which created the impasse in enacting comprehensive federal legislation, and which continue to be controversial as providers face the task of compliance. The article serves as a guide for legal and health care professionals who would like to be introduced to what the controversy over health care privacy has been all about and how the HIPAA rule has, at least for the time being, resolved some aspects of it.}}
@ARTICLE{Susskind_1987,title={Expert Systems in Law},year={1987},author={Richard Susskind and Richard Susskind and R. Susskind},doi={null},pmid={null},pmcid={null},mag_id={1501756943},journal={null},abstract={null}}
@ARTICLE{Cozby_1977,title={Methods in behavioral research},year={1977},author={Paul C. Cozby and Paul C. Cozby},doi={null},pmid={null},pmcid={null},mag_id={1504336393},journal={null},abstract={1. Scientific Understanding of Behavior Uses of Research Methods The Scientific Approach Goals of Science Basic and Applied Research Study Terms Review Questions Activity Questions Answers 2. Where to Start Hypotheses and Predictions? Who We Study: A Note on Terminology Source of Ideas Library Research anatomy of a Research Article Study Terms Review Questions Activity Questions 3. Ethical Research milgram's Obedience Experiment The Belmont Report>/h4> Assessment of Risks and Benefits Informed Consent The Importance of Debriefing Alternatives to Deception Justice and the Selection of Participants Researcher Commitments Federal Regulations and the Institutional Review Board APA Ethics Code Research with Human Participants Ethics and Animal Research Risks and Benefits Revisited Misrepresentation: Fraud and Plagiarism Ethical Issues: Illustrative Article Study Terms Review Questions Activity Questions Answers 4. Studying Behavior Validity: An Introduction Variables Operational Definitions of Variables Relationships Between Variables Nonexperimental Versus Experimental Methods Independent and Dependent Variables Internal Validity: Inferring Causality External Validity Choosing a Method Evaluating Research: Summary of the Three Validities Studying Behavior: Illustrative Article Study Terms Review Questions Activity Questions Answers</h4< 5. Measurement Concepts Reliability of Measures Construct Validity of Measures Research on Personality and Individual Differences Reactivity of Measures Variables and Measurement Scales Measurement Concepts: Illustrative Article Study Terms Review Questions Activity Questions 6. Observational Methods Quantitative and Qualitative Approaches Naturalistic Observation Systematic Observation Case Studies Archival Research Observational Methods: Illustrative Article Study Terms Review Questions Activity Questions Answers 7. Asking People About Themselves: Survey Research Why Conduct Surveys? Constructing Questions to Ask Responses to Questions Finalizing the Questionnaire Administering Surveys Survey Designs to Study Changes Over Time Sampling From a Population Sampling Techniques Evaluating Samples Reasons for Using Convenience Samples Survey Research: Illustrative Article Study Terms Review Questions Activity Questions Answers 8. Experimental Design Confounding and Internal Validity Basic Experiments Assigning Participants to Experimental Conditions Independent Groups Design Repeated Measures Design Matched Pairs Design Experimental Design: Illustrative Article Study Terms Review Questions Activity Questions 9. Conducting Experiments Selecting Research Participants Manipulating the Independent Variable Measuring the Dependent Variable Additional Controls Additional Considerations Analyzing and Interpreting Results Communicating Research to Others Conducting Experiments: Illustrative Article Study Terms Review Questions Activity Questions Answers 10. Complex Experimental Designs Increasing the Number of Levels of an Independent Variable Increasing the Number of Independent Variables: Factorial Designs Illustrative Article: Complex Experimental Designs Study Terms Review Questions Activity Questions Answers 11. Quasi-Experimental and Single-Participant Designs Single Case Experimental Designs Program Evaluation Quasi-Experimental Designs Developmental Research Designs Illustrative Article Study Terms Review Questions Activity Questions 12. Understanding Research Results: Description and Correlation Scales of Measurement: A Review Analyzing the Results of Research Investigations Frequency Distributions Descriptive Statistics Graphing Relationships Correlation Coefficients: Describing the Strength of Relationships Effect Size Regression Equations Multiple Correlation / Regression Partial Correlation and the Third-Variable Problem Structural Equation Modeling Study Terms Review Questions Activity Questions Answers 13. Understanding Research Results: Statistical Inference Samples and Populations Inferential Statistics Null and Research Hypotheses Probability and Sampling Distributions Example: The T and F Tests Type I and Type Ii Errors Choosing a Significance Level Interpreting Nonsignificant Results Choosing a Sample Size: Power Analysis The Importance of Replications Significance of a Pearson R Correlation Coefficient Computer Analysis of Data Selecting the Appropriate Statistical Test Study Terms Review Questions Activity Questions 14. Generalizing Results Generalizing to Other Populations of Research Participants Cultural Considerations Generalizing to Other Experimenters Pretests and Generalization Generalizing from Laboratory Settings The Importance of Replications Evaluating Generalizations Via Literature Reviews and Meta-Analyses Using Research to Improve Lives Generalizing Results: Illustrative Article Study Terms Review Questions Activity Questions Appendix A: Writing Research Reports Introduction Writing Style Organization of the Report The Use of Headings Citing and Referencing Sources Abbreviations Some Grammatical Considerations Reporting Numbers and Statistics Conclusion Paper and Poster Presentations Sample Paper Appendix B: Statistical Tests Descriptive Statistics Statistical Significance and Effect Size Appendix C: Statistical Tables Table C.1 Critical Values of Chi-Square Table C.2 Critical Values of T Table C.3 Critical Values of F Table C.4 Critical Values of R (Pearson Product-Moment Correlation Coefficient) Glossary References Credits Index}}
@ARTICLE{Kerrigan_2003,title={A software infrastructure for regulatory information management and compliance assistance},year={2003},author={Shawn Kerrigan and Shawn Kerrigan and Charles Heenan and Charles Heenan and Kincho H. Law and Kincho H. Law},doi={null},pmid={null},pmcid={null},mag_id={1504728518},journal={null},abstract={The Regnet Project aims to develop a formal information infrastructure for regulatory information management and compliance assistance. This paper discusses three basic milestones of current research and development efforts. The first is the creation of a document repository containing federal and state regulations and supplemental documents. This repository includes a suite of concept hierarchies that enable users to browse documents according to the terms they contain. The second is an XML framework for representing regulations and associated metadata. The XML framework enables the augmentation of regulation text with tools and information that will help users understand and comply with the regulation. The third milestone is the creation of a compliance assistance system built upon the XML framework. The prototype effort for the document repository has been focused on environmental regulations and related documents. The compliance assistance system is illustrated in the domain of used oil management.}}
@ARTICLE{Lehman_1996,title={Laws of Software Evolution Revisited},year={1996},author={M. M. Lehman and Meir M. Lehman},doi={10.1007/bfb0017737},pmid={null},pmcid={null},mag_id={1506513518},journal={null},abstract={Data obtained during a 1968 study of the software process [8] led to an investigation of the evolution of OS/360 [13] and and, over a period of twenty years, to formulation of eight Laws of Software Evolution. The FEAST project recently initiated (see sections 4–6 below) is expected to throw additional light on the phenomenology underlying these laws, to increase understanding of them, to explore their finer detail, to expose their wider relevance and implications and to develop means for their beneficial exploitation. This paper is intended to trigger wider interest in the laws and in the FEAST study of feedback and feedback control in the context of the software process and its improvement to ensure beneficial exploitation of their potential.}}
@ARTICLE{Anderson_1991,title={Analysis of Evidence},year={1991},author={Terence Anderson and Terence Anderson and David A. Schum and David A. Schum and William Twining and William Twining},doi={null},pmid={null},pmcid={null},mag_id={1506680948},journal={null},abstract={1. Evidence and inference: some food for thought 2. Fact investigation and the nature of evidence 3. Principles of proof 4. Methods of analysis 5. The chart method 6. Outlines, chronologies and narrative 7. Analysing the decided case: anatomy of a cause celebre 8. Evaluating evidence 9. Probabilities, weight and probative force 10. Necessary but dangerous: generalizations and stories in argumentation about facts 11. The principles of proof and the law of evidence 12. The trial lawyer's standpoint.}}
@ARTICLE{Rescher_1977,title={Dialectics: A Controversy-Oriented Approach to the Theory of Knowledge},year={1977},author={Nicholas Rescher and Nicholas Rescher},doi={null},pmid={null},pmcid={null},mag_id={1507818620},journal={null},abstract={null}}
@ARTICLE{Governatori_2010,title={A conceptually rich model of business process compliance},year={2010},author={Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo},doi={null},pmid={null},pmcid={null},mag_id={1509149235},journal={null},abstract={In this paper we extend the preliminary work developed elsewhere and investigate how to characterise many aspects of the compliance problem in business process modeling. We first define a formal and conceptually rich language able to represent, and reason about, chains of reparational obligations of various types. Second, we devise a mechanism for normalising a system of legal norms. Third, we specify a suitable language for business process modeling able to automate and optimise business procedures and to embed normative constraints. Fourth, we develop an algorithm for compliance checking and discuss some computational issues regarding the possibility of checking compliance runtime or of enforcing it at design time.}}
@ARTICLE{Holt_1996,title={GASE: visualizing software evolution-in-the-large},year={1996},author={Richard C. Holt and Richard C. Holt and J.Y. Pak and J.Y. Pak},doi={10.1109/wcre.1996.558900},pmid={null},pmcid={null},mag_id={1509186716},journal={null},abstract={Large and long lived software systems, sometimes called legacy systems, must evolve if they are to remain useful. Too often, it is difficult to control or to understand this evolution. The paper presents an approach to visualizing software structural change. A visualization tool, called GASE (Graphical Analyzer for Software Evolution), has been used to elucidate the architectural changes in a sequence of eleven revisions of an eighty thousand line industrial software system.}}
@ARTICLE{Saeki_2008,title={Supporting the Elicitation of Requirements Compliant with Regulations},year={2008},author={Motoshi Saeki and Motoshi Saeki and Motoshi Saeki and Motoshi Saeki and Haruhiko Kaiya and Haruhiko Kaiya},doi={10.1007/978-3-540-69534-9_18},pmid={null},pmcid={null},mag_id={1509536809},journal={null},abstract={This paper presents a technique to check the compliance of requirements with regulations while eliciting requirements. In our technique, we semantically represent a regulation with combinations of case frames resulting from Case Grammar technique. We match a newly elicited requirement sentence with the case frames of regulation sentences and then check if the requirements include the obligation acts specified by the matched regulation sentences and if they do not have prohibited acts. If we find that a requirement sentence does not follow the regulation, the addition or removal of the illegal acts included in the requirements are suggested.}}
@ARTICLE{Casner_1984,title={Cases and text on property},year={1984},author={A. James Casner and A. James Casner},doi={null},pmid={null},pmcid={null},mag_id={1509554433},journal={null},abstract={Long regarded as a high-quality casebook, CASES AND TEXT ON PROPERTY is now also one of the most current. Be sure to examine a copy of the Fifth Edition of this classic before you choose materials for your next course.}}
@ARTICLE{Shadbolt_2015,title={Knowledge Elicitation: Methods, Tools and Techniques},year={2015},author={Nigel Shadbolt and Nigel Shadbolt and Paul R. Smart and Paul R Smart},doi={null},pmid={null},pmcid={null},mag_id={1512412022},journal={null},abstract={null}}
@ARTICLE{Wardeh_2007,title={PADUA Protocol: Strategies and Tactics},year={2007},author={Maya Wardeh and Maya Wardeh and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Frans Coenen and Frans Coenen},doi={10.1007/978-3-540-75256-1_42},pmid={null},pmcid={null},mag_id={1515155516},journal={null},abstract={In this paper we describe an approach to classifying objects in a domain where classifications are uncertain using a novel combination of argumentation and data mining. Classification is the topic of a dialogue game between two agents, based on an argument scheme and critical questions designed for use by agents whose knowledge of the domain comes from data mining. Each agent has its own set of examples which it can mine to find arguments based on association rules for and against a classification of a new instance. These arguments are exchanged in order to classify the instance. We describe the dialogue game, and in particular discuss the strategic considerations which agents can use to select their moves. Different strategies give rise to games with different characteristics, some having the flavour of persuasion dialogues and other deliberation dialogues.}}
@ARTICLE{Frege_1950,title={The foundations of arithmetic : a logico-mathematical enquiry into the concept of number},year={1950},author={Gottlob Frege and Gottlob Frege and Jasmine Austin and J. L. Austin and J. L. Austin},doi={null},pmid={null},pmcid={null},mag_id={1515494069},journal={null},abstract={This volume represents the first philosophically sound discussion of the concept of number in Western civilization. (Mathematics)}}
@ARTICLE{Tyree_1989,title={Expert systems in law},year={1989},author={Alan L. Tyree and Alan Tyree},doi={null},pmid={null},pmcid={null},mag_id={1516807177},journal={null},abstract={null}}
@ARTICLE{Shamsaei_2012,title={Indicator-based Policy Compliance of Business Processes},year={2012},author={Azalia Shamsaei and Azalia Shamsaei},doi={10.20381/ruor-6171},pmid={null},pmcid={null},mag_id={1517229853},journal={null},abstract={Business process compliance management has recently grabbed a lot of attention in both business and academia as it helps organizations not only to control and monitor their business processes from a legal point of view but also to avoid financial penalties and undesirable consequences to their reputation. Balancing compliance obligations with business objectives remains however a difficult challenge. We believe goal-oriented compliance management using Key Performance Indicators (KPIs) to measure the compliance level of organizations is an area that can be further developed to tackle this challenge. Goaloriented compliance management concepts have been explored before. However, there is little research on how to measure and improve the compliance level of organizations using KPIs while considering the impact of candidate adjustments on business goals. We discuss a proposal toward a framework to address the aforementioned problems.}}
@ARTICLE{Baroni_2009,title={Encompassing Attacks to Attacks in Abstract Argumentation Frameworks},year={2009},author={Pietro Baroni and Pietro Baroni and Federico Cerutti and Federico Cerutti and Massimiliano Giacomin and Massimiliano Giacomin and Giovanni Guida and Giovanni Guida},doi={10.1007/978-3-642-02906-6_9},pmid={null},pmcid={null},mag_id={1517651437},journal={null},abstract={In the traditional definition of Dung's abstract argumentation framework ($\ensuremath{AF}$), the notion of attack is understood as a relation between arguments, thus bounding attacks to start from and be directed to arguments. This paper introduces a generalized definition of abstract argumentation framework called $\ensuremath{AFRA}$ (Argumentation Framework with Recursive Attacks), where an attack is allowed to be directed towards another attack. From a conceptual point of view, we claim that this generalization supports a straightforward representation of reasoning situations which are not easily accommodated within the traditional framework. From the technical side, we first investigate the extension to the generalized framework of the basic notions of conflict-free set, acceptable argument, admissible set and of Dung's fundamental lemma. Then we propose a correspondence from the $\ensuremath{AFRA}$ to the $\ensuremath{AF}$ formalism, showing that it satisfies some basic desirable properties. Finally we analyze the relationships between $\ensuremath{AFRA}$ and a similar extension of Dung's abstract argumentation framework, called $\ensuremath{EAF+}$ and derived from the recently proposed formalism $\ensuremath{EAF}$.}}
@ARTICLE{Gordon_1995,title={The Pleadings Game: An Artificial Intelligence Model of Procedural Justice},year={1995},author={Thomas F. Gordon and Thomas F. Gordon and Thomas F. Gordon},doi={null},pmid={null},pmcid={null},mag_id={1518249528},journal={null},abstract={The Pleadings Game is a major contribution to artificial intelligence and legal theory. The book draws on jurisprudence and moral philosophy to develop a formal model of argumentation called the pleadings game. From a technical perspective, the work can be viewed as an extension of recent argumentation-based approaches to non-monotonic logic: (1) the game is dialogical rather than mono-logical; (2) the validity and priority of defeasible rules is subject to debate; and (3) resource limitations are acknowledged by rules for fairly dividing the burdens of representation and proof among the players. Gordon's work evaluates important jurisprudential theories of argumentation and reasoning in the context of the U.S. commercial law on secured transactions. Audience: It is not necessary to have a formal background in law to appreciate The Pleadings Game. It will be of equal interest to both the artificial intelligence community and legal theorists.}}
@ARTICLE{Joerling_2010,title={Data Breach Notification Laws: An Argument for a Comprehensive Federal Law to Protect Consumer Data},year={2010},author={Jill Joerling and Jill Joerling},doi={null},pmid={null},pmcid={null},mag_id={1518559304},journal={Washington University Journal of Law and Policy},abstract={null}}
@ARTICLE{Agnoloni_2010,title={Semantic web standards and ontologies for legislative drafting support},year={2010},author={Tommaso Agnoloni and Tommaso Agnoloni and Daniela Tiscornia and Daniela Tiscornia},doi={10.1007/978-3-642-15158-3_16},pmid={null},pmcid={null},mag_id={1520247511},journal={null},abstract={Machine readable open public data and the issue of multilingual web are open challenges promising to transform the relationship between citizens and European institutions. In this context the DA- LOS1 project aims at ensuring coherence and alignment in the legislative language, providing law-makers with knowledge management tools to improve the control over the multilingual complexity of European legislation and over the linguistic and conceptual issues involved in its transposition into national laws. This paper describes the design and implementation activities performed on the basis of a set of parallel texts in different languages on a specific legal topic. Natural language processing techniques have been applied to automatically build lexicons for each language. Lexical and conceptual multilingual alignment has been accomplished exploiting terms position in parallel documents. An ontology describing entities involved in the chosen domain has been developed in order to provide a semantic description of terms in lexicons. A modular integration of such resources, represented in RDF/OWL standard format, allowed their effective and flexible access from a legislative drafting application prototype, able to enrich legal documents with terms mark-up and semantic annotations.}}
@ARTICLE{Dung_1993,title={On the acceptability of arguments and its fundamental role in nonmonotonic reasoning and logic programming},year={1993},author={Phan Minh Dung and Phan Minh Dung},doi={null},pmid={null},pmcid={null},mag_id={1521560294},journal={null},abstract={The purpose of this paper is to study the fundamental mechanism humans use in argumentation and its role in different major approaches to commonsense reasoning in AI and logic programming. We present three novel results: We develop a theory for argumentation in which the acceptability of arguments is precisely defined. We show that logic programming and nonmonotonic reasoning in AI are different forms of argumentation. We show that argumentation can be viewed as a special form of logic programming with negation as failure.

This result introduces a general method for generating metainterpreters for argumentation systems.}}
@ARTICLE{McHugh_2012,title={Barriers to Adopting Agile Practices When Developing Medical Device Software},year={2012},author={Martin McHugh and Martin McHugh and Fergal McCaffery and Fergal McCaffery and Valentine Casey and Valentine Casey},doi={10.1007/978-3-642-30439-2_13},pmid={null},pmcid={null},mag_id={1521616433},journal={null},abstract={Agile methodologies such as XP and Scrum are founded upon the four values and twelve principles of agile software development. A software development project is only considered to be truly agile if these values and principles are followed. However, software developed for use in medical devices must be regulatory compliant and this can make the process of following a single agile methodology such as XP difficult to achieve. This paper outlines how we identified the barriers to agile adoption in the medical device software domain through performing a survey. These barriers include: lack of documentation; maintaining traceability; regulatory compliance; lack of up front planning and the process of managing multiple releases. Based on this research recommendations are also made as to how these barriers can be overcome.}}
@ARTICLE{Heijst_1995,title={The Role of Ontologies in Knowledge Engineering},year={1995},author={G.A.C.M. van Heijst},doi={null},pmid={null},pmcid={null},mag_id={1522164272},journal={null},abstract={null}}
@ARTICLE{Boella_2010,title={A logical understanding of legal interpretation},year={2010},author={Guido Boella and Guido Boella and Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Leendert van der Torre and Leendert van der Torre},doi={null},pmid={null},pmcid={null},mag_id={1522579748},journal={null},abstract={If compliance with a norm does not achieve its purpose, then its applicability must dynamically be restricted or expanded. Legal interpretation is a mechanism from law allowing norms to be adapted to unforeseen situations. We model this mechanism for norms regulating computer systems by representing the purpose of norms by social goals and by revising the constitutive rules defining the applicability of norms. We illustrate the interpretation mechanism by examples.}}
@ARTICLE{Antoniou_1999,title={On the Modeling and Analysis of Regulations},year={1999},author={Grigoris Antoniou and Grigoris Antoniou and David P. Billington and David Billington and Guido Governatori and Guido Governatori and Michael J. Maher and Michael J. Maher},doi={null},pmid={null},pmcid={null},mag_id={1523902149},journal={null},abstract={Regulations are a wide-spread and important part of government and business. They codify how products must be made and processes should be performed. Such regulations can be difficult to understand and apply. In an environment of growing complexity of, and change in, regulation, automated support for reasoning with regulations is becoming increasingly necessary. In this paper we report on ongoing work which aims at providing automated support for the drafting and use of regulations using logic modelling techniques. We highlight the support that can be provided by logic modelling, describe the technical foundation of our project, and report on the status of the project and the next steps.}}
@ARTICLE{Mylopoulos_1995,title={Modelling strategic relationships for process reengineering},year={1995},author={John Mylopoulos and John Mylopoulos and Eric Yu and Eric Yu},doi={null},pmid={null},pmcid={null},mag_id={1524500851},journal={null},abstract={Existing models for describing a process (such as a business process or a software development process) tend to focus on the "what" or the "how" of the process. For example, a health insurance claim process would typically be described in terms of a number of steps for assessing and approving a claim. In trying to improve or redesign a process, however, one also needs to have an understanding of the "why"--for example, why do physicians submit treatment plans to insurance companies before giving treatment? and why do claims managers seek medical opinions when assessing treatment plans? An understanding of the motivations and interests of process participants is often crucial to the successful redesign of processes.
This thesis proposes a modelling framework i* (pronounced i-star) consisting of two modelling components. The Strategic Dependency (SD) model describes a process in terms of intentional dependency relationships among agents. Agents depended on each other for goals to be achieved, tasks to be performed, and resources to be furnished. Agents are intentional in that they have desires and wants, and strategic in that they are concerned about opportunities and vulnerabilities. The Strategic Rationale (SR) model describes the issues and concerns that agents have about existing processes and proposed alternatives, and how they might be addressed, in terms of a network of means-ends relationships. An agent's routines for carrying out a process can be analyzed for their ability, workability, viability and believability. Means-ends rules are used to suggest methods for addressing issues, related issues to be raised, and assumptions to be challenged. The models are represented in the conceptual modelling language Telos. The modelling concepts are axiomatically characterized.
The utility of the framework is illustrated each of four application areas: requirements engineering, business process reengineering, organizational impacts analysis, and software process modelling. Advantage of i* over existing modelling techniques in each of these areas are described.}}
@ARTICLE{Bench‐Capon_1997,title={Argument in Artificial Intelligence and Law},year={1997},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1023/a:1008242417011},pmid={null},pmcid={null},mag_id={1527525914},journal={Artificial Intelligence and Law},abstract={In this paper I shall discuss the notion of argument, and the importanceof argument in AI and Law. I shall distinguish four areas where argument hasbeen applied: in modelling legal reasoning based on cases; in thepresentation and explanation of results from a rule based legal informationsystem; in the resolution of normative conflict and problems ofnon-monotonicity; and as a basis for dialogue games to support the modellingof the process of argument. The study of argument is held to offer prospectsof real progress in the field of AI and law, and the purpose of this paperis to provide an overview of work, and the connection between the various strands.}}
@ARTICLE{Norman_1988,title={The Design of Everyday Things},year={1988},author={Donald A. Norman and Donald A. Norman},doi={null},pmid={null},pmcid={null},mag_id={1528027857},journal={null},abstract={Revealing how smart design is the new competitive frontier, this innovative book is a powerful primer on how--and why--some products satisfy customers while others only frustrate them.}}
@ARTICLE{McLachlan_2004,title={Analyzing Microarray Gene Expression Data},year={2004},author={Geoffrey J. McLachlan and Geoffrey J. McLachlan and Kim‐Anh Do and Kim Anh Do and Christophe Ambroise and Christophe Ambroise},doi={null},pmid={null},pmcid={null},mag_id={1528177290},journal={null},abstract={Preface. 1. Microarrays in Gene Expression Studies. 2. Cleaning and Normalization. 3. Some Cluster Analysis Methods. 4. Clustering of Tissue Samples. 5. Screening and Clustering of Genes. 6. Discriminant Analysis. 7. Supervised Classification of Tissue Samples. 8. Linking Microarray Data with Survival Analysis. References. Author Index. Subject Index.}}
@ARTICLE{Braak_2010,title={Sensemaking software for crime analysis},year={2010},author={S.W. van den Braak and S.W. van den Braak},doi={null},pmid={null},pmcid={null},mag_id={1528476210},journal={null},abstract={Criminal investigation is a difficult and laborious process that is prone to error as teams of investigators may be subject to tunnel vision, groupthink, and confirmation bias. As a result, miscarriages of justice may ensue. To overcome these problems, in the Dutch law enforcement organization, crime analysts have been given a more important role. It is now their task to critically evaluate the investigation that is going on. They have to make sense of the vast amount of evidence available in a case by generating plausible scenarios about what might have happened. Subsequently, they have to assess the quality of their scenarios and choose the best alternative. Due to the difficulty of this process, a great need exists for software that supports crime analysts in their task. However, current support tools for crime analysis do not allow analysts to record scenarios and their relation to the evidence and as a result the most important part of the analysis process remains in the analysts' minds. Therefore, they may benefit from so-called sensemaking systems that allow them to make their reasoning process explicit by visualizing scenarios and the reasons why these scenarios are supported by the evidence. Nevertheless, such sensemaking tools for crime analysis are relatively sparse and often do not incorporate a logical model of reasoning with evidence in the context of crime analysis. This thesis aims to fill this gap by proposing sensemaking software that has specifically been designed for crime analysis. Such a tool should be rationally well-founded, natural, useful, usable, and effective. To this aid, a proof-of-concept application called AVERs (Argument Visualization for Evidential Reasoning based on stories) was built that implements a rationally well-founded and natural model of the reasoning that takes place in crime analysis. In this way a standard of rational reasoning is encouraged and errors may be reduced. Using AVERs analysts are able to create visual representations of scenarios and evidential arguments. Scenarios are represented as causal networks of events, while evidential arguments are arguments based on the evidential data in the case. Such arguments are based on argumentation schemes that often come with critical questions. These questions make the analysts more aware of possible sources of doubt and encourage them to critically examine the evidence. Evidential arguments can be used to support or attack scenarios with the available evidence. In this way, this software allows the analysts to reason about scenarios and to critically evaluate them. Moreover, it provides features that can be used to compare alternative scenarios. A series of empirical studies has confirmed that the design and implementation of AVERs fulfills all five criteria to a certain degree. This means that it is useful to crime analysts and satisfies their desires, while it may improve their analysis of the case and the communication of their results to the investigators working on the case, and ensures that rational analyses are performed. Therefore, through this software in the future biases in the crime analysis process may be avoided.}}
@ARTICLE{Francesconi_2010,title={Integrating a bottom–up and top–down methodology for building semantic resources for the multilingual legal domain},year={2010},author={Enrico Francesconi and Enrico Francesconi and Simonetta Montemagni and Simonetta Montemagni and Wim Peters and Wim Peters and Daniela Tiscornia and Daniela Tiscornia},doi={10.1007/978-3-642-12837-0_6},pmid={null},pmcid={null},mag_id={1528607527},journal={null},abstract={This article presents a methodology for multilingual legal knowledge acquisition and modelling. It encompasses two comlementary strategies. On the one hand, there is the top–down definition of the conceptual structure of the legal domain under consideration on the basis of expert jugdment. This structure is language–independent, modeled as an ontology, and can be aligned with other ontologies that capture similar or complementary knowledge, in order to provide a wider conceptual embedding. Another top–down approach is the exploitation of the explicit structure of legal texts, which enables the targeted identification of text spans that play an ontological role and their subsequent inclusion in the knowledge model.

On the other hand, the linguistically motivated, text-based bottom–up population and incremental refinement of this conceptual structure using (semi-)automatic NLP techniques, maximizes the completeness and domain-specificity of the resulting knowledge.

The proposed methodology is concerned with the relation between these two differently derived types of knowledge, and defines a framework for interfacing lexical and ontological knowledge, the result of which offers various perspectives on multilingual legal knowledge.

Two case-studies combining bottom-up and top-down methodologies for knowledge modelling and learning are presented as illustrations of the methodology.}}
@ARTICLE{Boer_2008,title={MetaLex XML and the Legal Knowledge Interchange Format},year={2008},author={Alexander Boer and Alexander Boer and Radboud Winkels and Radboud Winkels and Fabio Vitali and Fabio Vitali},doi={10.1007/978-3-540-85569-9_2},pmid={null},pmcid={null},mag_id={1529183035},journal={null},abstract={Electronic government invariably involves XML and electronic law: legislation is as essential to public administration as the ball is to a ball game. This paper gives an overview of two XML standard proposals dealing with two complementary aspects of electronic legislation --- the documents themselves as a carrier, and an institutional reality they represent --- in a coherent way: MetaLex XML and the Legal Knowledge Interchange format (LKIF). MetaLex XML is well on its way to becoming formal and de facto standard for legislation in XML. LKIF is yet to be submitted as a proposed standard. LKIF includes some interesting innovations from an AI & Law perspective.}}
@ARTICLE{Manning_2005,title={Introduction to Information Retrieval},year={2005},author={Christopher D. Manning and Christopher D. Manning and Prabhakar Raghavan and Prabhakar Raghavan and Hinrich Schütze and Hinrich Schütze},doi={null},pmid={null},pmcid={null},mag_id={1532325895},journal={null},abstract={Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.}}
@ARTICLE{Nurmuliani_2004,title={Using card sorting technique to classify requirements change},year={2004},author={N. Nurmuliani and N. Nurmuliani and Didar Zowghi and Didar Zowghi and Susan P. Williams and Susan P. Williams},doi={10.1109/re.2004.55},pmid={null},pmcid={null},mag_id={1533096022},journal={null},abstract={Requirements volatility is considered to be a major source of risk to the management of large and complex software projects. The ability to characterise the nature and origins of requirements change during software development is important and can lead organisations towards more effective management of changing requirements. This work focuses on a study to establish how practitioners classify requirements change requests. We used the card sorting method to identify categories of change requests that software developers use in practice. Card sorting is a knowledge elicitation method that is commonly used for capturing information about different ways of representing domain knowledge. This study has allowed us to get valuable insights into the way practitioners classify change requests and to understand their perspectives on classification. This classification is a valuable source of information in prioritizing change requests and assessing their impact. Our findings from the card sorting exercise further reveal that the criteria used for categorization are related to the role the practitioner plays in the software development team and the nature and extent of their responsibilities.}}
@ARTICLE{Verheij_1995,title={Arguments and Defeat in Argument-Based Nonmonotonic Reasoning},year={1995},author={Bart Verheij and Bart Verheij},doi={10.1007/3-540-60428-6_18},pmid={null},pmcid={null},mag_id={1535377479},journal={null},abstract={Argument-based formalisms are gaining popularity as models of non-monotonic reasoning. Central in such formalisms is a notion of argument. Arguments are formal reconstructions of how a conclusion is supported. Generally, an argument is defeasible. This means that an argument supporting a conclusion does not always justify its conclusion: the argument can be defeated. Whether a conclusion supported by an argument is justified depends on the structure of the argument and on the other arguments available.}}
@ARTICLE{Hays_1973,title={Statistics for the social sciences},year={1973},author={William L. Hays and William L. Hays and William Lee Hays},doi={null},pmid={null},pmcid={null},mag_id={1536791815},journal={null},abstract={null}}
@ARTICLE{Duda_1981,title={MODEL DESIGN IN THE PROSPECTOR CONSULTANT SYSTEM FOR MINERAL EXPLORATION},year={1981},author={Richard O. Duda and Richard O. Duda and Richard O. Duda and John Gaschnig and John Gaschnig and P. D’Arcy Hart and Peter E. Hart},doi={10.1016/b978-0-934613-03-3.50028-3},pmid={null},pmcid={null},mag_id={1536861783},journal={null},abstract={null}}
@ARTICLE{Valente_1995,title={Legal knowledge engineering : a modelling approach},year={1995},author={André Valente and A. Valente},doi={null},pmid={null},pmcid={null},mag_id={1537848071},journal={null},abstract={Although the field of Artificial Intelligence and Law has matured considerably, there is still no comprehensive view on the field, its achievements, and no agenda or clear direction for research. Moreover, present approaches to the development of legal knowledge-based systems (LKBS) - such as the use of rule-based systems, case-based systems, or logics - have obtained somewhat limited theoretical and practical results. This book provides a critical overview of the field by describing present approaches and analysing their problems in detail. A new "modelling approach" to legal knowledge engineering is proposed to address these problems and provide an agenda for research and development. This approach applies recent developments in knowledge modelling to the law domain. The book's central premise, that the development of LK BS should be centred on the elaboration of explicit models of law, is well demonstrated, it is an extremely worthwhile read for anyone interested in the theoretical foundations of AI and law and knowledge representation in particular.}}
@ARTICLE{Lloyd_1984,title={Foundations of logic programming},year={1984},author={John W. Lloyd and John W. Lloyd},doi={null},pmid={null},pmcid={null},mag_id={1541434994},journal={null},abstract={This is the second edition of an account of the mathematical foundations of logic programming. Its purpose is to collect, in a unified and comprehensive manner, the basic theoretical results of the field, which have previously only been available in widely scattered research papers. In addition to presenting the technical results, the book also contains many illustrative examples and problems. The text is intended to be self-contained, the only prerequisites being some familiarity with PROLOG and knowledge of some basic undergraduate mathematics. The material is suitable either as a reference book for researchers or as a textbook for a graduate course on the theoretical aspects of logic programming and deductive database systems.}}
@ARTICLE{Corcho_2005,title={Building legal ontologies with METHONTOLOGY and WebODE},year={2005},author={Óscar Corcho and Oscar Corcho and Mariano Fernández-López and Mariano Fernández-López and Asuncíon Gómez-Pérez and Asunción Gómez-Pérez and Angel López-Cima and Angel López-Cima},doi={10.1007/978-3-540-32253-5_9},pmid={null},pmcid={null},mag_id={1541543414},journal={null},abstract={This paper presents how to build an ontology in the legal domain following the ontology development methodology METHONTOLOGY and using the ontology engineering workbench WebODE. Both of them have been widely used to develop ontologies in many other domains. The ontology used to illustrate this paper has been extracted from an existing class taxonomy proposed by Breuker, and adapted to the Spanish legal domain.}}
@ARTICLE{Pedrycz_2007,title={Fuzzy Systems Engineering: Toward Human-Centric Computing},year={2007},author={Witold Pedrycz and Witold Pedrycz and Fernando Gomide and Fernando Gomide},doi={null},pmid={null},pmcid={null},mag_id={1542883383},journal={null},abstract={A self-contained treatment of fuzzy systems engineering, offering conceptual fundamentals, design methodologies, development guidelines, and carefully selected illustrative material Forty years have passed since the birth of fuzzy sets, in which time a wealth of theoretical developments, conceptual pursuits, algorithmic environments, and other applications have emerged. Now, this reader-friendly book presents an up-to-date approach to fuzzy systems engineering, covering concepts, design methodologies, and algorithms coupled with interpretation, analysis, and underlying engineering knowledge. The result is a holistic view of fuzzy sets as a fundamental component of computational intelligence and human-centric systems. Throughout the book, the authors emphasize the direct applicability and limitations of the concepts being discussed, and historical and bibliographical notes are included in each chapter to help readers view the developments of fuzzy sets from a broader perspective. A radical departure from current books on the subject, Fuzzy Systems Engineering presents fuzzy sets as an enabling technology whose impact, contributions, and methodology stretch far beyond any specific discipline, making it applicable to researchers and practitioners in engineering, computer science, business, medicine, bioinformatics, and computational biology. Additionally, three appendices and classroom-ready electronic resources make it an ideal textbook for advanced undergraduate- and graduate-level courses in engineering and science.}}
@ARTICLE{Palmirani_2012,title={AI approaches to the complexity of legal systems : models and ethical challenges for legal systems, legal language and legal ontologies, argumentation and software agents},year={2012},author={Monica Palmirani and Monica Palmirani and Ugo Pagallo and Ugo Pagallo and Pompeu Casanovas and Pompeu Casanovas and Giovanni Sartor and Giovanni Sartor},doi={10.1007/978-3-642-35731-2},pmid={null},pmcid={null},mag_id={1545712609},journal={null},abstract={International Workshop AICOL-III, Held as Part of the 25th IVR Congress, Frankfurt am Main, Germany, August 15-16, 2011. Revised Selected Papers}}
@ARTICLE{Stevens_1998,title={Systems engineering : coping with complexity},year={1998},author={Richard Stevens},doi={null},pmid={null},pmcid={null},mag_id={1545878190},journal={null},abstract={In an age of shrinking development cycles, it is harder than ever to bring the right product to market at the right time. Good product, especially complex products, is underpinned by good systems, and systems engineering itself is recognised as the key tool to product development. This book covers the principles of systems design in an easy to read format. The authors have decades of practical industrial experience, and the material is ideal for industrial project teams. For academic courses, the book acts as a component for graduate and undergraduate engineering studies, particularly those on systems engineering. It covers how to handle requirements, architectural design, integration and verification, starting from the perspective of a simple linear lifecycle. The book then gradually introduces recent work on the complexity of real world systems, with issues such as multi-level systems, and iterative development. There is also coverage of the impact of systems engineering at the organsational level.}}
@ARTICLE{Flick_1998,title={An Introduction to Qualitative Research},year={1998},author={Uwe Flick and Uwe Flick},doi={null},pmid={null},pmcid={null},mag_id={1547253197},journal={null},abstract={PART ONE: FRAMEWORK Guide to this Book Qualitative Research: Why And How to Do It Qualitative and Quantitative Research Approaches to Qualitative Research Ethics of Qualitative Research PART TWO: THEORY IN QUALITATIVE RESEARCH Using the Existing Literature Theories Underlying Qualitative Research Texts as Data in Qualitative Research PART THREE: RESEARCH DESIGN Designing Qualitative Research The Qualitative Research Process Research Questions Entering the Field Sampling Triangulation PART FOUR: VERBAL DATA Collecting Verbal Data Interviews Focus Groups Using Narrative Data PART FIVE: DATA BEYOND TALK Collecting Data Beyond Talk Observation and Ethnography Visual Data: Photography, Film & Video Using Documents as Data PART SIX: QUALITATIVE DATA ANALYSIS Qualitative Data Analysis Transcription and Data Management Grounded Theory Coding Thematic Coding and Content Analysis Naturally Occuring Data: Conversation, Discourse, and Hermeneutic Analysis Using Software in Qualitative Data Analysis PART SEVEN: GROUNDING, WRITING AND OUTLOOK Quality of Qualitative Research: Criteria and Beyond Writing Qualitative Research State of the Art and the Future}}
@ARTICLE{Prakken_2004,title={Argumentation schemes and burden of proof},year={2004},author={Henry Prakken and Henry Prakken and Chris Reed and Chris Reed and Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={1547524103},journal={null},abstract={This paper analyzes the phenomenon of a shift of the burden of proof in persuasion dialogues in which arguments are constructed according to argumentation schemes. Some sample di- alogues are analyzed with arguments from expert opinion, revealing that some critical questions of this scheme carry with them a burden of proof on the questioner while others do not, and that the burden of proof can become the subject of debate during a dialogue. Then these dialogues are diagrammed with the argument visualization software Araucaria, and a simple formal protocol is proposed of persuasion dialogues with embedded burden-of-proof dialogues. The objective of this paper is to analyze the phenomenon of a shift of the burden of proof in persuasion dialogues where arguments are constructed according to argumentation schemes. Suppose an argu- ment instantiating some scheme has been put forward and a criti- cal question matching the scheme for that argument has been asked. Does merely asking the question make the argument default, or is the burden on the questioner to provide evidence? In this paper we take the view that the answer to this question depends on domain-specific issues and that the domain context in which the dialogue takes place will often leave room for disagreement on allocations of the burden of proof. Accordingly, our aim is to present a protocol for two-person persuasion dialogues in which the burden of proof can become the issue under dispute. In developing our model, we will first analyze some sample dialogues using the argumentation scheme from expert opinion, then semiformally diagram these dialogues with the argu- ment visualization software Araucaria (10), and finally present a for- mal protocol. The protocol will be simple, intended to give a first idea of how protocols for burden-of-proof dialogues can be formalized. As for previous research, most dialogue systems for persuasion}}
@ARTICLE{Burdon_2010,title={Australian data breach notification: avoiding the state/federal overlap},year={2010},author={Mark Burdon and Mark Burdon and Paul Von Nessen and Paul von Nessen and Jason Reid and Jason Reid and Bill Lane and William B. Lane and Bill Lane},doi={null},pmid={null},pmcid={null},mag_id={1547575177},journal={null},abstract={Mandatory data breach notification has become a matter of increasing concern for law reformers. In Australia, this issue was recently addressed as part of a comprehensive review of privacy law conducted by the Australian Law Reform Commission (ALRC) which recommended a uniform national regime for protecting personal information applicable to both the public and private sectors. As in all federal systems, the distribution of powers between central and state governments poses problems for national consistency. In the authors’ view, a uniform approach to mandatory data breach notification has greater merit than a ‘jurisdiction specific’ approach epitomized by US state-based laws. The US response has given rise to unnecessary overlaps and inefficiencies as demonstrated by a review of different notification triggers and encryption safe harbors. Reviewing the US response, the authors conclude that a uniform approach to data breach notification is inherently more efficient.}}
@ARTICLE{Hu_2009,title={Experience with Establishment of Reusable and Certifiable Safety Lifecycle Model within ABB},year={2009},author={Zaijun Hu and Zaijun Hu and Carlos Bilich and Carlos G. Bilich},doi={10.1007/978-3-642-04468-7_12},pmid={null},pmcid={null},mag_id={1548081801},journal={null},abstract={One basic requirement for a functional safety development project is to establish a SIL-compliant safety lifecycle model. For a company with a big family of safety-related products and a great number of development projects like ABB, it would be very time-consuming and cost-intensive for each safety development project to develop a safety lifecycle model. One approach for managing the corresponding costs and effort is to create a common lifecycle model that fulfills the SIL requirements and can be reused by safety-related projects. In this paper we are going to present such a common safety lifecycle model, its structure and components, and our experience on how to establish and apply it in safety-related product development projects. The paper analyzes the design constraints for the development of a common safety lifecycle model such as complexity, flexibility, simplicity, conformity and the safety integrity. It shows how these constraints drive the design of the safety lifecycle model to be developed. Our design concept, design considerations, development strategy, and our experience in establishing such a common safety lifecycle model will also be discussed in the paper.}}
@ARTICLE{Eemeren_2004,title={A systematic theory of argumentation},year={2004},author={F.H. van Eemeren and Frans H. van Eemeren and Frans H. van Eemeren},doi={null},pmid={null},pmcid={null},mag_id={1548116827},journal={null},abstract={null}}
@ARTICLE{Buchanan_1985,title={Rule-based expert systems : the MYCIN experiments of the Stanford Heuristic Programming Project},year={1985},author={Bruce G. Buchanan and Edward H. Shortliffe},doi={null},pmid={null},pmcid={null},mag_id={1549872659},journal={null},abstract={null}}
@ARTICLE{Alexy_1978,title={Theorie der juristischen Argumentation : die Theorie des rationalen Diskurses als Theorie der juristischen Begründung},year={1978},author={Robert Alexy and Robert Alexy and Robert Alexy},doi={null},pmid={null},pmcid={null},mag_id={1550719106},journal={null},abstract={null}}
@ARTICLE{Christie_2000,title={The Notion of an Ideal Audience in Legal Argument},year={2000},author={George C. Christie and George C. Christie},doi={null},pmid={null},pmcid={null},mag_id={1550961429},journal={null},abstract={This book examines in some detail how our concepts of an ideal or "universal" audience influence legal argument. It shows how asking what are the arguments and what are the forms of argumentation that we believe would be accepted by such an audience is a useful analytical tool. The book explores what, if any, are the constraints that our vision of an ideal audience imposes on public discourse and particularly on legal discourse. Some visions of an ideal or universal audience are widely shared; others are only shared within particular political and legal cultures. Stylistic preferences can have as important an influence on legal decision making as do substantive preferences. In some cultures and legal systems there is a preference for resort to broad general principles; in others there is a preference for a more circumscribed and particular mode of legal argument. Different legal cultures have different idealized notions as to the role of the judge and these different conceptions of the role of the judge will influence many aspects of legal decision making, including how statutes and other authoritative official instruments should be interpreted. The book also illustrates, by concrete examples, how the resolution of all these questions will be influenced by how a particular legal culture envisions the common or public good and by how tolerant a particular legal culture is of diverse outcomes, that is by how much discretion superior legal decision makers are prepared to grant inferior decision makers.}}
@ARTICLE{Quaresma_2010,title={Using linguistic information and machine learning techniques to identify entities from juridical documents},year={2010},author={Paulo Quaresma and Paulo Quaresma and Paulo Quaresma and Teresa Gonçalves and Teresa Gonçalves},doi={10.1007/978-3-642-12837-0_3},pmid={null},pmcid={null},mag_id={1551115015},journal={null},abstract={Information extraction from legal documents is an important and open problem. A mixed approach, using linguistic information and machine learning techniques, is described in this paper. In this approach, top-level legal concepts are identified and used for document classification using Support Vector Machines. Named entities, such as, locations, organizations, dates, and document references, are identified using semantic information from the output of a natural language parser. This information, legal concepts and named entities, may be used to populate a simple ontology, allowing the enrichment of documents and the creation of high-level legal information retrieval systems.

The proposed methodology was applied to a corpus of legal documents - from the EUR-Lex site – and it was evaluated. The obtained results were quite good and indicate this may be a promising approach to the legal information extraction problem.}}
@ARTICLE{Greenwood_2003,title={Structuring dialogue between the people and their representatives},year={2003},author={Katie Greenwood and Katie Greenwood and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Peter McBurney and Peter McBurney},doi={10.1007/10929179_9},pmid={null},pmcid={null},mag_id={1551755235},journal={null},abstract={Conversations between citizens and their representatives may take a number of forms. In this paper, we consider one of these – letters between citizens and representatives – and explore the application of a well-known model of dialogue types to these. We provide a method to give these types a precise characterization in terms of the initial beliefs and desires of the participants, and then explore one type, persuasion dialogues. This work commences the formal modeling of citizen-representative interactions necessary for a fully electronic democracy.}}
@ARTICLE{Boddy_2009,title={Statistical Methods in Practice: For Scientists and Technologists},year={2009},author={Richard Boddy and Richard Boddy and Gordon Smith and Gordon Smith},doi={null},pmid={null},pmcid={null},mag_id={1551841473},journal={null},abstract={Preface. 1 Samples and populations. Introduction. What a lottery! No can do. Nobody is listening to me. How clean is my river? Discussion. 2 What is the true mean? Introduction. Presenting data. Averages. Measures of variability. Relative standard deviation . Degrees of freedom. Confidence interval for the population mean. Sample sizes. How much moisture is in the raw material? Problems. 3 Exploratory data analysis. Introduction. Histograms: is the process capable of meeting specifications? Box plots: how long before the lights go out? The box plot in practice. Problems. 4 Significance testing. Introduction. The one-sample t -test. The significance testing procedure. Confidence intervals as an alternative to significance testing. Confidence interval for the population standard deviation. F -test for ratio of standard deviations. Problems. 5 The normal distribution. Introduction. Properties of the normal distribution. Example. Setting the process mean. Checking for normality. Uses of the normal distribution. Problems. 6 Tolerance intervals. Introduction. Example. Confidence intervals and tolerance intervals. 7 Outliers. Introduction. Grubbs' test. Warning. 8 Significance tests for comparing two means. Introduction. Example: watching paint lose its gloss. The two-sample t -test for independent samples. An alternative approach: a confidence intervals for the difference between population means. Sample size to estimate the difference between two means. A production example. Confidence intervals for the difference between the two suppliers. Sample size to estimate the difference between two means. Conclusions. Problems. 9 Significance tests for comparing paired measurements. Introduction. Comparing two fabrics. The wrong way. The paired sample t -test. Presenting the results of significance tests. One-sided significance tests. Problems. 10 Regression and correlation. Introduction. Obtaining the best straight line. Confidence intervals for the regression statistics. Extrapolation of the regression line. Correlation coefficient. Is there a significant relationship between the variables? How good a fit is the line to the data? Assumptions. Problems. 11 The binomial distribution. Introduction. Example. An exact binomial test. A quality assurance example. What is the effect of the batch size? Problems. 12 The Poisson distribution. Introduction. Fitting a Poisson distribution. Are the defects random? The Poisson distribution. Poisson dispersion test. Confidence intervals for a Poisson count. A significance test for two Poisson counts. How many black specks are in the batch? How many pathogens are there in the batch? Problems. 13 The chi-squared test for contingency tables. Introduction. Two-sample test for percentages. Comparing several percentages. Where are the differences? Assumptions. Problems. 14 Non-parametric statistics. Introduction. Descriptive statistics. A test for two independent samples: Wilcoxon-Mann-Whitney test. A test for paired data: Wilcoxon matched-pairs sign test. What type of data can be used? Example: cracking shoes. Problems. 15 Analysis of variance: Components of variability. Introduction. Overall variability. Analysis of variance. A practical example. Terminology. Calculations. Significance test. Variation less than chance? When should the above methods not be used? Between- and within-batch variability. How many batches and how many prawns should be sampled? Problems. 16 Cusum analysis for detecting process changes. Introduction. Analysing past data. Intensity. Localised standard deviation. Significance test. Yield. Conclusions from the analysis. Problem. 17 Rounding of results. Introduction. Choosing the rounding scale. Reporting purposes: deciding the amount of rounding. Reporting purposes: rounding of means and standard deviations. Recording the original data and using means and standard deviations in statistical analysis. References. Solutions to Problems. Statistical Tables. Index .}}
@ARTICLE{Poulin_1993,title={Contradiction and Confirmation},year={1993},author={Daniel Poulin and Daniel Poulin and Pierre St-Vincent and Pierre St-Vincent and Paul Bratley and Paul Bratley},doi={10.1007/3-540-57234-1_51},pmid={null},pmcid={null},mag_id={1551937168},journal={null},abstract={In most expert systems, efforts are made to keep the rules in the knowledge base free from contradictions., because a logical system that generates even a single contradiction may collapse. We argue that not only can contradictions be tolerated, but in fact they are useful. An excellent test of an argument is to compare it with the best argument for the opposing view. Accordingly, we propose a multilevel architecture, where the object level may include contradictory rules, and the metalevels resolve these conflicts. Once such an architecture exists, there are advantages to allowing contradictions in wider contexts. In a legal context we may want to examine both sides of an argument. In administrative applications, we may need systems that ‘look over a clerk's shoulder’ to check that he is following one of several plausible, but not necessarily compatible, approaches. We are currently implementing these ideas.}}
@ARTICLE{Schwitter_2003,title={Dynamic semantics at work},year={2003},author={Rolf Schwitter and Rolf Schwitter and Marc Tilbrook and Marc Tilbrook},doi={10.1007/978-3-540-71009-7_39},pmid={null},pmcid={null},mag_id={1552595853},journal={null},abstract={In this case study we show how an unambiguous semantic representation can be constructed dynamically in left-to-right order while a text is written in PENG, a controlled natural language designed for knowledge representation. PENG can be used in contexts where precise texts (e.g. software specifications, axioms for formal ontologies, legal documents) need to be composed. Texts written in PENG look seemingly informal and are easy to write and to read for humans but have first-order equivalent properties that make these texts computer-processable.}}
@ARTICLE{Hage_2004,title={Comparing Alternatives in the law},year={2004},author={Jaap Hage and Jaap Hage},doi={10.1007/s10506-005-6926-z},pmid={null},pmcid={null},mag_id={1553881162},journal={Artificial Intelligence and Law},abstract={This paper argues the thesis that a particular style of reasoning, qualitative comparative reasoning (QCR), plays a role in at least three areas of legal reasoning that are central in AI and law research, namely legal theory construction, case-based reasoning in the form of case comparison, and legal proof. The paper gives an informal exposition of one particular way to deal with QCR, based on the author's previous work on reason-based logic (RBL). Then it contains a substantially adapted formalisation of RBL, to make RBL suitable for dealing with QCR. The paper concludes with a brief discussion of related work.}}
@ARTICLE{Peek_1997,title={Representing Law in Partial Information Structures},year={1997},author={Niels Peek and Niels Peek and Niels Peek and Niels Peek},doi={10.1023/a:1008238332032},pmid={null},pmcid={null},mag_id={1554205848},journal={Artificial Intelligence and Law},abstract={This paper presents a new language for isomorphic representations of legalknowledge in feature structures. The language includes predefinedstructures based on situation theory for common-sense categories, andpredefined structures based on Van Kralingen`s (1995) frame-based conceptualmodelling language for legal rules. It is shown that the flexibility of thefeature-structure formalism can exploited to allow for structure-preservingrepresentations of non-primitive concepts, and to enable various types ofinteraction and cross-reference between language elements. A fragment of theDutch Opium Act is used to illustrate how modelling and reasoning proceed in practice.}}
@ARTICLE{Bench‐Capon_2009,title={Argumentation in Legal Reasoning},year={2009},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1007/978-0-387-98197-0_18},pmid={null},pmcid={null},mag_id={1555368823},journal={null},abstract={A popular view of what Artificial Intelligence can do for lawyers is that it can do no more than deduce the consequences from a precisely stated set of facts and legal rules. This immediately makes many lawyers sceptical about the usefulness of such systems: this mechanical approach seems to leave out most of what is important in legal reasoning. A case does not appear as a set of facts, but rather as a story told by a client. For example, a man may come to his lawyer saying that he had developed an innovative product while working for Company A. Now Company B has made him an offer of a job, to develop a similar product for them. Can he do this? The lawyer firstly must interpret this story, in the context, so that it can be made to fit the framework of applicable law. Several interpretations may be possible. In our example it could be seen as being governed by his contract of employment, or as an issue in Trade Secrets law. Next the legal issues must be identified and the pros and cons of the various interpretations considered with respect to them. Does his contract include a non-disclosure agreement? If so, what are its terms? Was he the sole developer of the product? Did Company A support its development? Does the product use commonly known techniques? Did Company A take measures to protect the secret? Some of these will favour the client, some the Company. Each interpretation will require further facts to be obtained. For example, do the facts support a claim that the employee was the sole developer of the product? Was development work carried out in his spare time? What is the precise nature of the agreements entered into? Once an interpretation has been selected, the argument must be organised into the form considered most likely to persuade, both to advocate the client’s position and to rebut anticipated objections. Some precedents may point to one result and others to another. In that case, further arguments may be produced to suggest following the favourable precedent and ignoring the unfavourable one. Or the rhetorical presentation of the facts may prompt one interpretation rather than the other. Surely all this requires the skill, experience and judgement of a human being? Granted that this is true, much effort has been made to design computer programs that will help people in these tasks, and it is the purpose of this chapter to describe the progress that has been made in modelling and supporting this kind of sophisticated legal reasoning.We will review systems that can store conflicting interpretations and that can propose alternative solutions to a case based on these interpretations. We will also describe systems that can use legal precedents to generate arguments by drawing analogies to or distinguishing precedents. We will discuss systems that can argue why a rule should not be applied to a case even though all its conditions are met. Then there are systems that can act as a mediator between disputing parties by structuring and recording their arguments and responses. Finally we look at systems that suggest mechanisms and tactics for forming arguments.}}
@ARTICLE{Miles_1994,title={Qualitative Data Analysis: An Expanded Sourcebook},year={1994},author={Matthew B. Miles and A. Michael Huberman},doi={null},pmid={null},pmcid={null},mag_id={1556808170},journal={null},abstract={null}}
@ARTICLE{Popple_1990,title={Legal Expert Systems: The Inadequacy of a Rule-Based Approach},year={1990},author={James Popple and James Popple},doi={10.2139/ssrn.1335646},pmid={null},pmcid={null},mag_id={1558046767},journal={null},abstract={The two different categories of legal AI system are described, and legal analysis systems are chosen as objects of study. So-called judgment machines are discussed, but it is decided that research in legal AI systems would be best carried-out in the area of legal expert systems. A model of legal reasoning is adopted, and two different methods of legal knowledge representation are examined: rule-based systems and case-based systems. It is argued that a rule-based approach to legal expert systems is inadequate given the requirements of lawyers and the nature of legal reasoning about cases. A new, eclectic approach is proposed, incorporating both rule-based and case-based knowledge representation. It is claimed that such an approach can form the basis of an effective and useful legal expert system.}}
@ARTICLE{Gómez-Pérez_2004,title={Ontological Engineering: with examples from the areas of Knowledge Management, e-Commerce and the Semantic Web},year={2004},author={Asuncíon Gómez-Pérez and Asunción Gómez-Pérez and Mariano Fernández-López and Mariano Fernández-López and Óscar Corcho and Oscar Corcho},doi={null},pmid={null},pmcid={null},mag_id={1563421732},journal={null},abstract={Theoretical Foundations of Ontologies.- The Most Outstanding Ontologies.- Methodologies and Methods for Building Ontologies.- Languages for Building Ontologies.- Ontology Tools.}}
@ARTICLE{Alexy_2010,title={A Theory of Legal Argumentation: The Theory of Rational Discourse as Theory of Legal Justification},year={2010},author={Robert Alexy and Robert Alexy and Ruth Adler and Ruth Adler and Neil MacCormick and Neil MacCormick and Ruth Adler},doi={null},pmid={null},pmcid={null},mag_id={1564062619},journal={null},abstract={Introduction: the problem of the justification of legal decisions topic theory and its limits towards assessing whether contemporary methodological discussions reveal a need for a theory of rational legal argumentation. Part 1 Reflections on some theories of practical discourse: practical discourse in analytic moral philosophy - naturalism and intuitionism, emotivism, practical discourse as a rule-governed activity including Wittgenstein and Austin, Hare's theory, Toulmin's theory, Baier's theory some interim results Habermas' consensus theory of truth - Habermas' critique of the correspondence theory of truth, combining speech act theory and a theory of truth, distinguishing between action and discourse, the justification of normative statements, the logic of discourse, the ideal speech situation, critical discussion of Habermas' theory the theory of practical deliberation of the Erlangen School - the programme of the constructive method, the presupposed purpose of constructivist ethics, the principles of constructivist ethics, the critical genesis of norms Chaim Perelman's theory of argumentation - the theory of argumentation as a logical theory, argumentation as a function of audience, demonstration and argumentation, the concept of the universal audience, persuading and convincing, Perelman's analysis of the structure of argumentation, the rationality of argumentation. Part 2 Outline of a theory of general rational practical discourse: the problem of the justification of normative statements possible theories of discourse, the justification of rules of discourse the rules and forms of general practice discourse - the rationality rules, rules for allocating the burden of argument, the justification rules, the transition rules the limits of general practical discourse. Part 3 A theory of legal argumentation: legal discourse as a special case of general practical discourse the outline of a theory of legal argumentation - internal justification, external justification legal and general practical discourse.}}
@ARTICLE{Gelder_2003,title={Enhancing deliberation through computer supported argument visualization},year={2003},author={Tim van Gelder and Tim van Gelder},doi={10.1007/978-1-4471-0037-9_5},pmid={null},pmcid={null},mag_id={1565182730},journal={null},abstract={As this is being written, the Governor General of Australia, Dr. Peter Hollingworth, has not resigned. Yet over the previous weeks and months he must have been thinking about it long and hard. He has been under intense pressure from various quarters, based on allegations that in previous positions of leadership he had not handled some sexual abuse incidents appropriately. In pondering what he should do, he must have been considering the many and varied arguments on both sides of the case. He must, in short, have been deliberating about his future.}}
@ARTICLE{Jackson_1997,title={The meaning of requirements},year={1997},author={Michael Jackson and Michael Jackson},doi={10.1023/a:1018990005598},pmid={null},pmcid={null},mag_id={1567369818},journal={Annals of Software Engineering},abstract={We use the term requirements to denote what are often called functional requirements. Requirements are located in the environment, which is distinguished from the machine to be built. A requirement is a condition over phenomena of the environment. A specification is a restricted form of requirement, providing enough information for the implementer to build the machine (by programming it) without further environment knowledge. To describe requirements appropriately we must fit our descriptions into an appropriate structure. This structure must respect the distinction between the machine and the environment, and the distinction between those environment properties that are given (indicative descriptions) and those that must be achieved by the machine (optative descriptions). Formalisation is a fundamental problem of requirements engineering. Since most environments are parts of the physical world, and therefore informal, the formalisation task is inescapable. Some techniques are discussed for tackling this task. In particular, the use of designations is explained, and the distinction between definition and assertion. By using the smallest possible set of designated terms, augmented by appropriate definitions, the developer can create a narrow bridge between the environment and its descriptions in the requirements. In this way a sufficiently faithful approximation to the informal reality can be obtained.}}
@ARTICLE{Oliveira_2008,title={i* Diagnoses: A Quality Process for Building i* Models.},year={2008},author={Anderson Oliveira and Antonio de Pádua Albuquerque Oliveira and Julio César Sampaio do Prado Leite and Julio Cesar Sampaio do Prado Leite and Luiz Marcio Cysneiros and Luiz Marcio Cysneiros and Carlos José Pereira de Lucena and Carlos José Pereira de Lucena},doi={null},pmid={null},pmcid={null},mag_id={1569598451},journal={null},abstract={Modeling with i* is not a trivial task. Our work describes i* Diagnoses Framework, a quality oriented process to analyze i* models. Our process is similar to some of the reading techniques of inspection methods and bears some similarity with the inquiry based requirement analysis approach. Our process focuses on defect prevention considering both the efficiency and effectiveness of Multi-Agent System development.}}
@ARTICLE{Walton_1995,title={Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning},year={1995},author={Douglas Walton and Douglas Walton and Erik C. W. Krabbe and Erik C. W. Krabbe},doi={null},pmid={null},pmcid={null},mag_id={1569903868},journal={null},abstract={null}}
@ARTICLE{Peczenik_1989,title={On law and reason},year={1989},author={Aleksander Peczenik and Aleksander Peczenik and Aleksander Peczenik and Aleksander Peczenik},doi={null},pmid={null},pmcid={null},mag_id={1570099351},journal={null},abstract={The Dilemma of Legal Reasoning: Moral Evaluation or Description of the Law?.- Rationality of Moral Judgments.- Rationality of Legal Reasoning.- The Ultimate Justification of Moral and Legal Reasoning.- What is Valid Law?.- The Doctrine of the Sources of the Law.- The Methods of Legal Reasoning.}}
@ARTICLE{Loucopoulos_1995,title={System Requirements Engineering},year={1995},author={Pericles Loucopoulos and Pericles Loucopoulos and V. Karakostas and Vassilios Karakostas},doi={null},pmid={null},pmcid={null},mag_id={1570543497},journal={null},abstract={From the Publisher:
System Requirements Engineering presents a balanced view of the issues, concepts, models, techniques and tools found in requirements engineering research and practice. Requirements engineering is presented from business, behavioural and software engineering perspectives and a general framework is established at the outset. This book considers requirements engineering as a combination of three concurrent and interacting processes: eliciting knowledge related to a problem domain, ensuring the validity of such knowledge and specifying the problem in a formal way. Particular emphasis is given to requirements elicitation techniques and there is a fully integrated treatment of the development of requirements specifications through enterprise modelling, functional requirements and non-functional requirements.}}
@ARTICLE{Egly_2008,title={ASPARTIX: Implementing Argumentation Frameworks Using Answer-Set Programming},year={2008},author={Uwe Egly and Uwe Egly and Sarah Alice Gaggl and Sarah Alice Gaggl and Stefan Woltran and Stefan Woltran},doi={10.1007/978-3-540-89982-2_67},pmid={null},pmcid={null},mag_id={1570778371},journal={null},abstract={The system ASPARTIX is a tool for computing acceptable extensions for a broad range of formalizations of Dung's argumentation framework and generalizations thereof. ASPARTIX relies on a fixed disjunctive datalog program which takes an instance of an argumentation framework as input, and uses the answer-set solver DLV for computing the type of extension specified by the user.}}
@ARTICLE{Ashley_1987,title={But, See, Accord: Generating Blue Book Citations in HYPO.},year={1987},author={Kevin D. Ashley and Kevin D. Ashley and Edwina L. Rissland and Edwina L. Rissland},doi={null},pmid={null},pmcid={null},mag_id={1570894769},journal={null},abstract={null}}
@ARTICLE{Ellis-Braithwaite_2012,title={Modelling the Strategic Alignment of Software Requirements using Goal Graphs},year={2012},author={Richard Ellis-Braithwaite and Richard Ellis-Braithwaite and Russell Lock and Russell Lock and Ray Dawson and Ray Dawson and Badr Haque and Badr Haque},doi={null},pmid={null},pmcid={null},mag_id={1571018340},journal={arXiv: Software Engineering},abstract={This paper builds on existing Goal Oriented Requirements Engineering (GORE) research by presenting a methodology with a supporting tool for analysing and demonstrating the alignment between software requirements and business objectives. Current GORE methodologies can be used to relate business goals to software goals through goal abstraction in goal graphs. However, we argue that unless the extent of goal-goal contribution is quantified with verifiable metrics and confidence levels, goal graphs are not sufficient for demonstrating the strategic alignment of software requirements. We introduce our methodology using an example software project from Rolls-Royce. We conclude that our methodology can improve requirements by making the relationships to business problems explicit, thereby disambiguating a requirement's underlying purpose and value.}}
@ARTICLE{Gaaevic_2006,title={Model Driven Architecture and Ontology Development},year={2006},author={Dragan Gaaevic and Dragan Gaaevic and Dragan Djurić and Dragan Djuric and Vladan Devedžić and Vladan Devedzic and Bran Selić and Bran Selic},doi={null},pmid={null},pmcid={null},mag_id={1572188237},journal={null},abstract={Part I: Basics - Knowledge Representation - Ontologies - Semantic Web - Model Driven Architecture - Modeling Spaces.- Part II: Model Driven Architecture and Ontologies - Software Engineering Approaches for Ontology Development - MDA-Based Ontology Infrastructure - Ontology Definition Metamodel - Ontology UML Profile - Mappings of MDA Based Languages and Ontologies.- Part III: Application - Using UML Tools for Ontology Modeling - MDA Based Ontology Platform: AIR - Ontology Examples.}}
@ARTICLE{Watson_1988,title={Legal Transplants: An Approach to Comparative Law},year={1988},author={Alan Watson and Alan Watson},doi={null},pmid={null},pmcid={null},mag_id={1573382455},journal={null},abstract={A reprint of the 1974 edition (Scottish Academic Press). Highly controversial then and now, Watson's argument is that a society's laws do not usually develop from within, but are borrowed from other societies. A new (12 pp.) afterword by Watson places the study in the context of recent scholarship,}}
@ARTICLE{Manning_1999,title={Foundations of Statistical Natural Language Processing},year={1999},author={Christopher D. Manning and Christopher D. Manning and Hinrich Schütze and Hinrich Schütze},doi={null},pmid={null},pmcid={null},mag_id={1574901103},journal={null},abstract={Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.}}
@ARTICLE{Gîrba_2005,title={How developers drive software evolution},year={2005},author={Tudor Gîrba and Tudor Gîrba and Adrian Kuhn and Adrian Kuhn and Manfred Seeberger and M. Seeberger and Sté́phane Ducasse and Stéphane Ducasse},doi={10.1109/iwpse.2005.21},pmid={null},pmcid={null},mag_id={1576040511},journal={null},abstract={As systems evolve their structure change in ways not expected upfront. As time goes by, the knowledge of the developers becomes more and more critical for the process of understanding the system. That is, when we want to understand a certain issue of the system we ask the knowledgeable developers. Yet, in large systems, not every developer is knowledgeable in all the details of the system. Thus, we would want to know which developer is knowledgeable in the issue at hand. In this paper we make use of the mapping between the changes and the author identifiers (e.g., user names) provided by versioning repositories. We first define a measurement for the notion of code ownership. We use this measurement to define the ownership map visualization to understand when and how different developers interacted in which way and in which part of the system. We report the results we obtained on several large systems.}}
@ARTICLE{Prakken_2010,title={A logical analysis of burdens of proof},year={2010},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={null},pmid={null},pmcid={null},mag_id={1576106079},journal={null},abstract={The starting point of this article is the claim that logics for defeasible argumentation provide the means to logically characterise the difference between several kinds of proof burdens, but only if they are embedded in a dynamic setting that captures the various stages of a legal proceeding. It is also argued that estandardi argumentation logics for AI must be adapted in order to model shifts in burdens of proof. Thus this analysis illustrates in two ways that logics cannot be simply imposed on the law but that features of legal systems must be taken into account. First there is the claim that the burden of persuasion, which legally is the burden to prove a statement to a specified degree (the standard of proof) on the penalty of losing on the issue, can be verified by applying an argumentation logic to the evidence available at the final stage of a proceeding. Then a precise distinction is made between two burdens that are sometimes confused, namely the burden of production and the tactical burden. In this analysis, the tactical burden of proof is automatically induced by the defeasible nature of the reasoning. The burden of production, by contrast, concerns the legal question whether an issue can be submitted to trial or must be decided as a matter of law against the one who fails to produce any evidence. Finally the issue is raised to what extent this account can be generalised to statistical and story-based approaches.}}
@ARTICLE{Austin_1962,title={How to do things with words},year={1962},author={John Austin and John Langshaw Austin and J. L. Austin},doi={null},pmid={null},pmcid={null},mag_id={1576632330},journal={null},abstract={* Lecture I * Lecture II * Lecture III * Lecture IV * Lecture V * Lecture VI * Lecture VII * Lecture VIII * Lecture IX * Lecture X * Lecture XI * Lecture XII}}
@ARTICLE{Boehm_1978,title={Characteristics of software quality},year={1978},author={Barry Boehm and John R Brown and Hans Kaspar},doi={null},pmid={null},pmcid={null},mag_id={1577034617},journal={null},abstract={null}}
@ARTICLE{Barabucci_2009,title={Multi-layer markup and ontological structures in Akoma Ntoso},year={2009},author={Gioele Barabucci and Gioele Barabucci and Luca Cervone and Luca Cervone and Monica Palmirani and Monica Palmirani and Silvio Peroni and Silvio Peroni and Fabio Vitali and Fabio Vitali},doi={10.1007/978-3-642-16524-5_9},pmid={null},pmcid={null},mag_id={1579225698},journal={null},abstract={The XML documents that represent legal resources contain information and legal knowledge that belong to many distinct conceptual layers. This paper shows how the Akoma Ntoso standard keeps these layers well separated while providing ontological structures on top of them. Additionally, this paper illustrates how Akoma Ntoso allows multiple interpretations, provided by different agents, over the same set of texts and concepts and how current semantic technologies can use these interpretations to reason on the underlying legal texts.}}
@ARTICLE{Garrett_2000,title={Legislation and statutory interpretation},year={2000},author={Elizabeth Garrett and Elizabeth Garrett},doi={null},pmid={null},pmcid={null},mag_id={1580929996},journal={null},abstract={Suitable for students or practitioners, this authoritative overview of the legislative process and statutory interpretation moves smoothly and understandably between the theoretical and the practical. You'll find in-depth discussion of such topics as theories of legislation and representation, electoral and legislative structures, extrinsic sources for statutory interpretation, and substantive canons of statutory interpretation. Reap the benefits of the authors' experience, opinions, and insight and gain a working knowledge of the area.}}
@ARTICLE{Hastie_1993,title={Inside the juror: The psychology of juror decision making.},year={1993},author={Reid Hastie and Reid Hastie},doi={10.1017/cbo9780511752896},pmid={null},pmcid={null},mag_id={1581825063},journal={null},abstract={Series preface Editor's preface Acknowledgments Part I. Models of Juror Decision Making: 1. Introduction Reid Hastie 2. Some steps between attitudes and verdicts Phoebe C. Ellsworth 3. The influence of outcome information and attitudes on juror decision making in search and seizure cases Jonathan D. Casper and Kennette M. Benedict 4. Algebraic models of juror decision processes Reid Hastie 5. Stochastic models of juror decision making Norbert Kerr 6. Formal and empirical research on cascaded inference in jurisprudence David A. Schum and Anne W. Martin 7. Argument structuring and evidence evaluation David A. Schum 8. The story model for juror decision making Nancy Pennington and Reid Hastie Part II. Commentaries: 9. Notes on the sampling of stimulus cases and the measurement of responses in research on juror decision making Robyn M. Davies 10. Sausages and the law: juror decisions in the much larger justice system Joseph B. Kadane 11. A rational game theory framework for the analysis of legal and criminal decision making Ehud Kalai 12. Why do jury research? Richard O. Lempert 13. Two conceptions of the juror Lola Lopes 14. A mathematician comments on models of juror decision making Sandy Zabell Index of names Index of subjects.}}
@ARTICLE{Karlsson_1996,title={Software requirements prioritizing},year={1996},author={Jenny Karlsson and Joachim Karlsson},doi={10.1109/icre.1996.491435},pmid={null},pmcid={null},mag_id={1583369417},journal={null},abstract={The importance of candidate software requirements can vary by orders of magnitude, yet most software providers do not have accurate and efficient means for selecting among them. The paper describes a case study at Ericsson Radio Systems AB of two techniques for software requirements prioritizing as a means for determining the importance of candidate requirements, a pairwise comparison technique and a numeral assignment technique. The results from the case study indicate that the pairwise comparison technique is an efficient informative and accurate means for finding the candidate requirements importance. We therefore recommend the pairwise comparison technique for software requirements prioritizing. At Ericsson we have extended its use in other development projects.}}
@ARTICLE{Valente_2005,title={Types and roles of legal ontologies},year={2005},author={André Valente and Andre Valente},doi={10.1007/978-3-540-32253-5_5},pmid={null},pmcid={null},mag_id={1584251685},journal={null},abstract={In this paper, we propose a number of basic types and roles of ontologies, and use them as a basis to analyze several legal ontologies in the AI and Law literature. We discuss some dimensions in which to distinguish types of ontologies, for example considering their level of structure. We propose five main roles of ontologies in general: (a) organize and structure information; (b) reasoning and problem solving; (c) se-mantic indexing and search; (d) semantics integration and interoperation; and (e) understanding the domain. We then discuss example of works that have exploited each of these roles in the AI and Law literature. Further, we discuss some of the consequences of using ontologies to play each of these roles in terms of the level of structure of the knowledge represented in the ontologies, the kinds of knowledge representation formalisms they use, and the reasoning methods they employ.}}
@ARTICLE{Yalçinalp_1989,title={An integrated interpreter for explaining Prolog's successes and failures},year={1989},author={L. Ümit Yalçinalp and L. U. Yalçinalp and L. U. Yalçinalp and L.U. Yalcinalp and Leon Sterling and Leon Sterling and L. Sterling and L. Sterling},doi={null},pmid={null},pmcid={null},mag_id={1585349532},journal={null},abstract={null}}
@ARTICLE{Cai_2011,title={Computational Intelligence and Intelligent Systems},year={2011},author={Zhihua Cai and Zhihua Cai and Zhihua Cai and Zhenhua Li and Zhenhua Li and Zhuo Kang and Zhuo Kang and Yong Liu and Yong Liu},doi={null},pmid={null},pmcid={null},mag_id={1586750735},journal={null},abstract={ISICA 2017, conference proceedings on swarm intelligence, cooperative Search, swarm optimization, complex systems modeling, system dynamic, multimedia simulation, intelligent information systems, information retrieval, e-commerce platforms, artificial intelligence and robotics, virtualization.}}
@ARTICLE{Paine_2005,title={Up to code: does your company's conduct meet world-class standards?},year={2005},author={Lynn S. Paine and Lynn S. Paine and Rohit Deshpandé and Rohit Deshpandé and Joshua D. Margolis and Joshua D. Margolis and Kim Eric Bettcher and Kim Bettcher},doi={null},pmid={16334587},pmcid={null},mag_id={1589027071},journal={Harvard Business Review},abstract={Codes of conduct have long been a feature of corporate life. Today, they are arguably a legal necessity--at least for public companies with a presence in the United States. But the issue goes beyond U.S. legal and regulatory requirements. Sparked by corruption and excess of various types, dozens of industry, government, investor, and multisector groups worldwide have proposed codes and guidelines to govern corporate behavior. These initiatives reflect an increasingly global debate on the nature of corporate legitimacy. Given the legal, organizational, reputational, and strategic considerations, few companies will want to be without a code. But what should it say? Apart from a handful of essentials spelled out in Sarbanes-Oxley regulations and NYSE rules, authoritative guidance is sorely lacking. In search of some reference points for managers, the authors undertook a systematic analysis of a select group of codes. In this article, they present their findings in the form of a "codex," a reference source on code content. The Global Business Standards Codex contains a set of overarching principles as well as a set of conduct standards for putting those principles into practice. The GBS Codex is not intended to be adopted as is, but is meant to be used as a benchmark by those wishing to create their own world-class code. The provisions of the codex must be customized to a company's specific business and situation; individual companies' codes will include their own distinctive elements as well. What the codex provides is a starting point grounded in ethical fundamentals and aligned with an emerging global consensus on basic standards of corporate behavior.}}
@ARTICLE{Desprès_2006,title={TERMINAE method and integration process for legal ontology building},year={2006},author={Sylvie Desprès and Sylvie Desprès and Sylvie Szulman and Sylvie Szulman},doi={10.1007/11779568_108},pmid={null},pmcid={null},mag_id={1592087104},journal={null},abstract={This paper describes the contruction method of a legal application ontology. This method is based on the merging of micro-ontologies built from European community directives. The terminae construction method from texts enhanced by an alignment process with a core legal ontology is used for building micro-ontologies. A merging process allows constructing the legal ontology.}}
@ARTICLE{Walter_1988,title={Computer Power and Legal Language: The Use of Computational Linguistics, Artificial Intelligence, and Expert Systems in the Law},year={1988},author={Charles Walter and C. Walter},doi={null},pmid={null},pmcid={null},mag_id={1594096799},journal={null},abstract={Introduction by Charles Walter Precise Meaning and Open Texture in Legal Writing and Reading by Peter Linzer Elements of Legal Language by Charles Walter Toward a Model of Legal Argumentation by Donald Berman and Charles Walter A Brief Introduction to Logic Programming and Its Applications in Law by Marek Sergot Toward A Rule-Based Representation of Open Texture in Law by Trevor Bench-Capon and Marek Sergot A Semantic Representation of the Pre-Contractual and Contractual Verbs of Exchange by J. Hook The Discourse Properties of the Criminal Status by Michael Hoey The Implementation of CCLIPS by George Cross, Cary deBessonet, Teri Bradshaw, Glynn Durham, Rittick Gupta, and Mohammed Nasiruddin Representing Contractual Situations by Seth Goldman, Michael Dyer, and Margot Flowers The Text Retrieval System as a Conversation Factor by Jon Bing Natural Language Interfaces by Michael Hoey and Charles Walter Semiotic Orders in Law by Michael Heather Distinguishing Legal Language-Types for Conceptual Retrieval by Cary deBessonet and George Cross The Basic Logic for the Interpetation of Legal Texts by Hector-Neri Castaneda Obstacles to the Development of Logic-Based Models of Legal Reasoning by Donald Berman and Carole Hafner The Relation between Language Studies and Expert Systems by J. C. Gardin An Experiment with Normalized Statutes in an Emycin Expert System by Grayfred Gray Exploring Computer-Aided Generation of Question for Normalizing Legal Rules by Layman Allen and Charles Saxon Expert System Shells and the Judicial Process: An Evaluation by Bethany Dumas and Charles Walter Expert Systems for Law by Charles Walter Toward a Legal Expert System Shell: A Prolog Implementation by C. Duncan MacRae and Elizabeth Chase MacRae State of the Art of Computerization in Law Practice by Charles Walter Index}}
@ARTICLE{Governatori_2010,title={A Logic Framework of Normative-based Contract Management},year={2010},author={Guido Governatori and Guido Governatori},doi={null},pmid={null},pmcid={null},mag_id={1595102515},journal={null},abstract={We explore of the feasibility of the computationally oriented institutional agency framework proposed by Governatori and Rotolo testing it against an industrial strength scenario. In particular we show how to encode in defeasible logic the dispute resolution policy described in Article 67 of FIDIC.}}
@ARTICLE{Uyttendaele_1998,title={Salomon: Automatic Abstracting of Legal Cases for Effective Access to Court Decisions},year={1998},author={Caroline Uyttendaele and Caroline Uyttendaele and Marie‐Francine Moens and Marie-Francine Moens and Jos Dumortier and Jos Dumortier},doi={10.1023/a:1008256030548},pmid={null},pmcid={null},mag_id={1599637927},journal={Artificial Intelligence and Law},abstract={The SALOMON project is a contribution to the automatic processing of legal texts. Its aim is to automatically summarise Belgian criminal cases in order to improve access to the large number of existing and future cases. Therefore, techniques are developed for identifying and extracting relevant information from the cases. A broader application of these techniques could considerably simplify the work of the legal profession.

A double methodology was used when developing SALOMON: the cases are processed by employing additional knowledge to interpret structural patterns and features on the one hand and by way of occurrence statistics of index terms on the other. As a result, SALOMON performs an initial categorisation and structuring of the cases and subsequently extracts the most relevant text units of the alleged offences and of the opinion of the court. The SALOMON techniques do not themselves solve any legal questions, but they do guide the user effectively towards relevant texts.}}
@ARTICLE{Visser_1998,title={A Comparison of Four Ontologies for the Design of Legal Knowledge Systems},year={1998},author={Pepijn R. S. Visser and Pepijn R. S. Visser and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1023/a:1008251913710},pmid={null},pmcid={null},mag_id={1600311062},journal={Artificial Intelligence and Law},abstract={There is a growing interest in how people conceptualise the legal domain for the purpose of legal knowledge systems. In this paper we discuss four such conceptualisations (referred to as ontologies): McCarty's language for legal discourse, Stamper's norma formalism, Valente's functional ontology of law, and the ontology of Van Kralingen and Visser. We present criteria for a comparison of the ontologies and discuss the strengths and weaknesses of the ontologies in relation to these criteria. Moreover, we critically review the criteria.}}
@ARTICLE{Tosatto_2012,title={Visualizing normative systems: an abstract approach},year={2012},author={Silvano Colombo Tosatto and Silvano Colombo Tosatto and Guido Boella and Guido Boella and Leendert van der Torre and Leendert van der Torre and Serena Villata and Serena Villata},doi={10.1007/978-3-642-31570-1_2},pmid={null},pmcid={null},mag_id={1601298413},journal={null},abstract={Abstract normative systems allow to reason with norms even when their content is not detailed. In this paper, we propose a our preliminary results to visualize abstract normative systems, in such a way that we are able to reason with institutional facts, obligations and permissions. Moreover, we detect meaningful patterns emerging from the proposed visualization, and we show how these patterns can be used to define commonly used reusable solutions.}}
@ARTICLE{Kaptein_2009,title={Legal Evidence and Proof : Statistics, Stories, Logic},year={2009},author={H.J.R. Kaptein and H.J.R. Kaptein and Henry Prakken and Henry Prakken and Bart Verheij and Bart Verheij},doi={10.4324/9781315592015},pmid={null},pmcid={null},mag_id={1601828931},journal={null},abstract={As a result of recent scandals concerning evidence and proof in the administration of criminal justice – ranging from innocent people on death row in the United States to misuse of statistics leading to wrongful convictions in The Netherlands and elsewhere – inquiries into the logic of evidence and proof have taken on a new urgency both in an academic and practical sense. This study presents a broad perspective on logic by focusing on inference not just in isolation but as embedded in contexts of procedure and investigation. With special attention being paid to recent developments in Artificial Intelligence and the Law, specifically related to evidentiary reasoning, this book provides clarification of problems of logic and argumentation in relation to evidence and proof. As the vast majority of legal conflicts relate to contested facts, rather than contested law, this volume concerning facts as prime determinants of legal decisions presents an important contribution to the field for both scholars and practitioners. Contents: Preface; General Introduction; Burdens of evidence and proof: why bear them? A plea for principled opportunism in (leaving) legal fact-finding (alone), Hendrik Kaptein; The fabrication of facts: the lure of the credible coincidence, Ton Derksen and Monica Meijsing; Decision-making in the forensic arena, Ton Broeders; Analysing stories using schemes, Floris Bex; The evaluation of evidence: differences between legal systems, Marijke Malsch and Ian Freckelton; Inference to the best legal explanation, Amalia Amaya; Accepting the truth of a story about the facts of a criminal case, Bart Verheij and Floris Bex; Rigid anarchic principles of evidence and proof: anomist panaceas against legal pathologies of proceduralism, Hendrik Kaptein; A logical analysis of burdens of proof, Henry Prakken and Giovanni Sartor; 12 angry men or one good woman? Asymmetric relations in evidentiary reasoning, Burkhard Schafer; Index.}}
@ARTICLE{Gunning_1968,title={The Technique of Clear Writing.},year={1968},author={Robert C. Gunning and Robert Gunning},doi={null},pmid={null},pmcid={null},mag_id={1602368597},journal={null},abstract={null}}
@ARTICLE{Moe_2007,title={Understanding lacking trust in global software teams: a multi-case study},year={2007},author={Nils Brede Moe and Nils Brede Moe and Darja Šmite and Darja Šmite},doi={10.1007/978-3-540-73460-4_6},pmid={null},pmcid={null},mag_id={1602815829},journal={null},abstract={Many organizations have turned toward globally distributed software development in their quest for higher-quality software delivered cheaply and quickly. But this kind of development has often been reported as problematic and complex to manage. One of the fundamental factors in determining the success and failure of globally distributed software teams is trust. The aim of our work has therefore been to describe the key factors causing lack of trust, and the main effects of lacking trust in such teams. From studying 4 projects, all located in two different countries, with trust problems we found the key factors to be poor socialization and socio-cultural fit, lack of face-to-face meetings, missing conflict handling and cognitive based trust, increased monitoring and too little communication. The effect of lacking trust was a decrease in productivity, quality, information exchange, feedback and morale among the employees; the monitoring increased and the employees doubted negative feedback from manager.}}
@ARTICLE{Philipps_1999,title={Introduction: from legal theories to neural networks and fuzzy reasoning},year={1999},author={Lothar Philipps and Lothar Philipps and Giovanni Sartor and Giovanni Sartor},doi={10.1023/a:1008371600675},pmid={null},pmcid={null},mag_id={1603330466},journal={Artificial Intelligence and Law},abstract={Computational approaches to the law have frequently been characterized as being formalistic implementations of the syllogistic model of legal cognition: using insufficient or contradictory data, making analogies, learning through examples and experiences, applying vague and imprecise standards. We argue that, on the contrary, studies on neural networks and fuzzy reasoning show how AI & law research can go beyond syllogism, and, in doing that, can provide substantial contributions to the law.}}
@ARTICLE{Yoshino_1997,title={On the logical foundations of compound predicate formulae for legal knowledge representation},year={1997},author={Hideaki Yoshino and Hajime Yoshino},doi={10.1023/a:1008289826410},pmid={null},pmcid={null},mag_id={1603618493},journal={Artificial Intelligence and Law},abstract={In order to represent legal knowledge adequately, it is vital to create a formal device that can freely construct an individual concept directly from a predicate expression. For this purpose, a Compound Predicate Formula (CPF) is formulated for use in legal expert systems. In this paper, we willattempt to explain the nature of CPFs by rigorous logical foundation, i.e., establishing their syntax and semantics precisely through the use of appropriate examples. We note the advantages of our system over other such systems and discuss the significance of CPFs with regard to the formalization of legal reasonings using examples from the United Nations Convention for the International Sale of Goods.}}
@ARTICLE{Kamsties_2005,title={Taming Ambiguity in Natural Language Requirements},year={2005},author={Erik Kamsties and Erik Kamsties and E. Kamsties},doi={null},pmid={null},pmcid={null},mag_id={1605228128},journal={null},abstract={One of the main tasks of requirements engineering (RE) is the creation of a requirements document that precisely, consistently, and completely describes the functional and non-functional properties of the system to be built. At some point during the RE process, the requirements are written down using a natural language or a requirements specification language. On one hand, natural language is flexible, universal, and wide-spread. On the other hand, natural language requirements are recognized widely as being incomplete, inconsistent, and inherently ambiguous. Semi-formal and formal requirements specification techniques, such as UML [13] or SCR [5], have been proposed to overcome these deficiencies. Completeness and consistency of these specifications can be tackled to some degree mechanically by tools. However, as a recent study shows, ambiguity rarely surfaces during the development of a requirements model [10]. Thus, since a requirements specification technique enforces precision, the resulting requirements model becomes unambiguously wrong. In this paper, we show how to detect ambiguities in natural language requirements using a checklist. We distinguish between linguistic and RE-specific ambiguities. Linguistic ambiguities are those ambiguities that are usually discussed in RE textbooks such as ambiguous pronoun references. RE-specific ambiguities occur with respect to the RE context, which includes the application domain and the system domain. For example, the requirement (1) If the bank customer maintains a minimum balance in his or her checking account, there is no monthly service charge. is ambiguous not because of its linguistic representation, but because of the operational environment and application domain. In the requirements documents that we have investigated, RE-specific ambiguities account for the majority of ambiguities, while pure linguistic ambiguities played a less significant role. In this paper, we present also an approach that allows identifying types of RE-specific ambiguity from a metamodel, e.g., of a requirements specification technique. Based on the identified types of ambiguity, we have developed an improved inspection technique for natural language requirements on the basis of checklists and scenario-based reading.}}
@ARTICLE{Callan_1992,title={The INQUERY Retrieval System},year={1992},author={James P. Callan and James P. Callan and W. Bruce Croft and W. Bruce Croft and Stephen M. Harding and Stephen M. Harding},doi={10.1007/978-3-7091-7557-6_14},pmid={null},pmcid={null},mag_id={1605873790},journal={null},abstract={As larger and more heterogeneous text databases become available, information retrieval research will depend on the development of powerful, efficient and flexible retrieval engines. In this paper, we describe a retrieval system (INQUERY) that is based on a probabilistic retrieval model and provides support for sophisticated indexing and complex query formulation. INQUERY has been used successfully with databases containing nearly 400,000 documents.}}
@ARTICLE{Schweighofer_1993,title={Legal Expert System KONTERM - Automatic Representation of Document Structure and Contents},year={1993},author={Erich Schweighofer and Erich Schweighofer and Werner Winiwarter and Werner Winiwarter},doi={10.1007/3-540-57234-1_49},pmid={null},pmcid={null},mag_id={1606063821},journal={null},abstract={Our legal expert system KONTERM contains a selective thesaurus and a knowledge base for the automatic representation of the structure and the contents of the document. The thesaurus takes into account the necessary degree of formalism of legal language and therefore overcomes the untidiness of natural language and represents automatically the expert knowledge of a lawyer about legal terminology. The required selectivity of the individual descriptors is achieved by distinguishing between precise legal terms and words with fuzzy meanings as well as by detecting hidden word senses. This thesaurus, together with the expert rules for structuring the document, provides the user with an analytical document representation. In addition to that, hypertext links will be supplemented in order to offer the lawyer a convenient tool for efficient information retrieval.}}
@ARTICLE{Lehman_2002,title={Software Evolution and Software Evolution Processes},year={2002},author={M. M. Lehman and Meir M. Lehman and Juan F. Ramil and Juan F. Ramil},doi={10.1023/a:1020557525901},pmid={null},pmcid={null},mag_id={1606433013},journal={Annals of Software Engineering},abstract={Most of the software in regular use in businesses and organisations all over the world cannot be completely specified. It cannot be implemented, once and for all. Both the original implementation and the inevitable subsequent evolution (maintenance) are a continual learning experience driven, i>inter alia, by feedback from the results of the behaviour under execution of the software, as perceived by various stakeholders, by advances and growth in the user organisations and by adaptation to changes in the external world, both independent and as a result of installation and use of the software. Real world, termed type-i>E, software is essentially evolutionary in nature. The study of the processes of evolution of such software is of considerable interest, as is that of the domains that co-evolve with the software. After briefly discussing the meaning of the term i>evolution in the context of software, its technology, the software process and related domains, this paper describes some of the facets of the evolution phenomenon and implications to the evolution process as identified during many years of active interest in the topic.}}
@ARTICLE{Vreeswijk_2000,title={Arno R. Lodder, DiaLaw: On Legal Justification and Dialogical Models of Argumentation. Law and Philosophy Library Vol. 42},year={2000},author={Gerard A. W. Vreeswijk and Gerard A. W. Vreeswijk},doi={10.1023/a:1008345320940},pmid={null},pmcid={null},mag_id={1606901355},journal={Artificial Intelligence and Law},abstract={null}}
@ARTICLE{Chieze_2010,title={An automatic system for summarization and information extraction of legal information},year={2010},author={Emmanuel Chieze and Emmanuel Chieze and Atefeh Farzindar and Atefeh Farzindar and Guy Lapalme and Guy Lapalme},doi={10.1007/978-3-642-12837-0_12},pmid={null},pmcid={null},mag_id={1607318503},journal={null},abstract={This paper presents an information system for legal professionals that integrates natural language processing technologies such as text classification and summarization. We describe our experience in the use of a mix of linguistics aware transductor and XML technologies for bilingual information extraction from judgements in both French and English within a legal information and summarizing system. We present the context of the work, the main challenges and how they were tackled by clearly separating language and domain dependent terms and vocabularies. After having been developed on the immigration law domain, the system was easily ported to the intellectual property and tax law domains.}}
@ARTICLE{Lührs_2001,title={Internet, Discourses, and Democracy},year={2001},author={Rolf Lührs and Rolf Lührs and Thomas Malsch and Thomas Malsch and Kimberly Wilmot Voss and K. Voss},doi={10.1007/3-540-45548-5_9},pmid={null},pmcid={null},mag_id={1607779331},journal={Lecture Notes in Computer Science},abstract={It is the very purpose of the DEMOS1 project — the subject of this paper2 — to exploit novel forms of computer mediated communication in order to support democracy on-line (‘e-democracy’) and to enhance citizen participation in modern societies.}}
@ARTICLE{Otto_2009,title={Managing Legal Texts in Requirements Engineering},year={2009},author={Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1007/978-3-540-92966-6_21},pmid={null},pmcid={null},mag_id={1621472773},journal={null},abstract={Laws and regulations are playing an increasingly important role in requirements engineering and systems development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, legal compliance is critical in systems development, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new statutes, regulations, and case law. Requirements engineers and compliance auditors must be able to identify relevant legal texts, extract requirements and other key concepts, and monitor compliance. This chapter surveys research efforts over the past 50 years in handling legal texts for systems development. This survey can aid requirements engineers and auditors to better specify, test, and monitor systems for compliance.}}
@ARTICLE{Gruber_1991,title={The role of common ontology in achieving sharable, reusable knowledge bases},year={1991},author={Tobias Gruber and Thomas R. Gruber},doi={null},pmid={null},pmcid={null},mag_id={1622687797},journal={null},abstract={null}}
@ARTICLE{Lin_1998,title={An Information-Theoretic Definition of Similarity},year={1998},author={Dekang Lin and Dekang Lin},doi={null},pmid={null},pmcid={null},mag_id={1647729745},journal={null},abstract={Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.}}
@ARTICLE{Strauss_1998,title={Basics of qualitative research : techniques and procedures for developing grounded theory},year={1998},author={Anselm L. Strauss and Anselm L. Strauss and Juliet Corbin and Juliet Corbin and Juliet Corbin and Juliet M. Corbin},doi={null},pmid={null},pmcid={null},mag_id={1658908529},journal={null},abstract={Part I: Introduction to Grounded Theory of Anselm Strauss Chapter 1: Inspiration and Background Chapter 2: Theoretical Foundations Chapter 3: Practical Considerations for Getting Started Chapter 4: Prelude to Analysis Chapter 5: Strategies for Qualitative Data Analysis Chapter 6: Memos and Diagrams Chapter 7: Theoretical Sampling Chapter 8: Context Chapter 9: Process Chapter 10: Techniques for Achieving Theoretical Integration Chapter 11: The Use of Computer Programs in Qualitative Data Analysis Part II: Research Demonstration Project Chapter 12 Open Coding: Identifying Concepts Chapter 13: Developing Concepts in Terms of Their Properties and Dimensions Chapter 14: Analyzing Data for Context Chapter 15: Bringing Process Into the Analysis Chapter 16: Integrating Categories Part III: Finishing the Research Project Chapter 17: Writing Theses, Monographs, and Dissertations, and Giving Talks About Your Research Chapter 18: Criteria for Evaluation Chapter 19: Student Questions and Answers}}
@ARTICLE{Gordon_2006,title={Pierson vs. Post RevisitedA Reconstruction using the Carneades Argumentation Framework},year={2006},author={Thomas F. Gordon and Thomas F. Gordon and Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={1660713213},journal={null},abstract={The Pierson vs. Post case [1] has become an important benchmark in the field of AI and Law for computational models of argumentation. In [2], Bench-Capon used Pierson vs. Post to motivate the use of values and value preferences in his theory-construction account of legal argument. And in a more a recent paper by Atkinson, Bench-Capon and McBurney [3], it was used to illustrate a formalization of an argumentation scheme for practical reasoning. Here we offer yet another reconstruction of Pierson vs. Post, using our Carneades Argumentation Framework, a formal mathematical model of argument structure and evaluation based on Walton's theory of argumentation [4], and compare it to this prior work. Carneades, named in honor of the Greek skeptic philosopher who emphasized the importance of plausible reasoning, applies proof standards [5] to determine the defensibility of arguments and the acceptability of statements on an issue-by-issue basis.}}
@ARTICLE{Hashmi_2012,title={Business process data compliance},year={2012},author={Mustafa Hashmi and Mustafa Hashmi and Guido Governatori and Guido Governatori and Moe Thandar Wynn and Moe Thandar Wynn},doi={10.1007/978-3-642-32689-9_4},pmid={null},pmcid={null},mag_id={1672671307},journal={null},abstract={Most approaches to business process compliance are restricted to the analysis of the structure of processes. It has been argued that full regulatory compliance requires information on not only the structure of processes but also on what the tasks in a process do. To this end Governatori and Sadiq [2007] proposed to extend business processes with semantic annotations. We propose a methodology to automatically extract one kind of such annotations; in particular the annotations related to the data schema and templates linked to the various tasks in a business process.}}
@ARTICLE{Palmirani_2009,title={Model regularity of legal language in active modifications},year={2009},author={Monica Palmirani and Monica Palmirani and Raffaella Brighi and Raffaella Brighi},doi={10.1007/978-3-642-16524-5_5},pmid={null},pmcid={null},mag_id={1686502315},journal={null},abstract={One of the main emerging challenges in legal documentation is to capture the meaning and the semantics of normative content using NLP techniques, and to isolate the relevant part of the linguistic speech. The last five years have seen an explosion in XML schemas and DTDs whose focus in modelling legal resources their focus was on structure. Now that the basic elements of textual descriptiveness are well formalized, we can use this knowledge to proceed with content. This paper presents a detailed methodology for classifying modificatory provisions in depth and providing all the necessary information for semiautomatically managing the consolidation process. The methodology is based on an empirical legal analysis of about 29,000 Italian acts, where we bring out regularities in the language associated with some modifications, and where we define patterns of proprieties for each type of modificatory provision. The list of verbs and the frames inferred through this empirical legal analysis have been used by the NLP group at the University of Turin to refine a syntactical NLP parser for isolating and representing the sentences as syntactic trees, and the pattern will be used by the light semantic interpreter module to indentify the parameters of modificatory provisions.}}
@ARTICLE{Völker_2008,title={Supporting the Construction of Spanish Legal Ontologies with Text2Onto},year={2008},author={Johanna Völker and Johanna Völker and Sergi Fernández and Sergi Fernandez Langa and York Sure and York Sure},doi={10.1007/978-3-540-85569-9_7},pmid={null},pmcid={null},mag_id={1690600408},journal={null},abstract={The IST project SEKT (Semantically Enabled Knowledge Technologies) aims at developing semantic technologies by integrating knowledge management, text mining, and human language technology. Tools and methodologies implemented in the SEKT project are employed and optimized in three case studies, one of them being concerned with intelligent integrated decision support for legal professionals. The main goal of this case study is to offer decision support to newly appointed judges in Spain by means of iFAQ, an intelligent Frequently Asked Questions system based on a complex ontology of the legal domain. Building this ontology is a tedious and time-consuming task requiring profound knowledge of legal documents and language. Therefore, any kind of automatic support can significantly increase the efficiency of the knowledge acquisition process. In this paper we present Text2Onto, an open-source tool for ontology learning, and our experiments with legal case study data. The previously existing English version of Text2Onto has been adapted to support the linguistic analysis of Spanish texts, including language-specific algorithms for the extraction of ontological concepts, instances and relations. Text2Onto greatly facilitated the automatic generation of the initial version of the Spanish legal ontology from a given collection of Spanish documents. In further iterative steps which included a mixture of learning and manual effort the ontology has been refined and applied to the real-world case study.}}
@ARTICLE{Brüninghaus_2005,title={Reasoning with textual cases},year={2005},author={Stefanie Brüninghaus and Stefanie Brüninghaus and Kevin D. Ashley and Kevin D. Ashley},doi={10.1007/11536406_13},pmid={null},pmcid={null},mag_id={1695915310},journal={null},abstract={This paper presents methods that support automatically finding abstract indexing concepts in textual cases and demonstrates how these cases can be used in an interpretive CBR system to carry out case-based argumentation and prediction from text cases. We implemented and evaluated these methods in SMILE+IBP, which predicts the outcome of legal cases given a textual summary. Our approach uses classification-based methods for assigning indices. In our experiments, we compare different methods for representing text cases, and also consider multiple learning algorithms. The evaluation shows that a text representation that combines some background knowledge and NLP combined with a nearest neighbor algorithm leads to the best performance for our TCBR task.}}
@ARTICLE{Hunter_1999,title={Out of their minds: legal theory in neural networks},year={1999},author={Dan Hunter and Dan Hunter},doi={10.1023/a:1008301122056},pmid={null},pmcid={null},mag_id={1722548028},journal={Artificial Intelligence and Law},abstract={This paper examines the use of connectionism (neural networks) in modelling legal reasoning. I discuss how the implementations of neural networks have failed to account for legal theoretical perspectives on adjudication. I criticise the use of neural networks in law, not because connectionism is inherently unsuitable in law, but rather because it has been done so poorly to date. The paper reviews a number of legal theories which provide a grounding for the use of neural networks in law. It then examines some implementations undertaken in law and criticises their legal theoretical naivete. It then presents a lessons from the implementations which researchers must bear in mind if they wish to build neural networks which are justified by legal theories.}}
@ARTICLE{Christensen_2007,title={The Paradox of Legal Expertise: A Study of Experts and Novices Reading the Law},year={2007},author={Leah M. Christensen and Leah M. Christensen and Leah M. Christensen},doi={null},pmid={null},pmcid={null},mag_id={1722589996},journal={Brigham Young University Education and Law Journal},abstract={What strategies do lawyers and judges use to read the law? The study described in this article examined the way in which 10 legal experts (8 lawyers and 2 judges) and 10 novices (law students in the top 50% of their class) read a judicial opinion. Whereas the experts read efficiently (taking less overall time), the beginning law students read less efficiently. Where the experts read the text flexibly, moving back and forth between different parts of the opinion, the novices read inflexibly. The experts connected to the purpose of their reading more consistently than the novices and drew upon their prior knowledge and experience with the law. The results of this study suggest that we can give our students the following advice in order to read like legal experts: (1) Read with a purpose; (2) Use background knowledge to situate the case; (3) Establish the context of the case before beginning to read; (4) Evaluate the case and have an opinion about its outcome; and (5) Read flexibly; skim and skip when appropriate. By examining the actual transcripts of lawyers and judges reading a case, this article illustrates how we can teach our students to read like legal experts. The earlier they achieve these skills, the better for the individual students, the more likely their success in law school and the better for the legal profession as a whole.}}
@ARTICLE{Brewka_2011,title={Relating the semantics of abstract dialectical frameworks and standard AFs},year={2011},author={Gerhard Brewka and Gerd Brewka and Paul E. Dunne and Paul E. Dunne and Stefan Woltran and Stefan Woltran},doi={10.5591/978-1-57735-516-8/ijcai11-137},pmid={null},pmcid={null},mag_id={1727543676},journal={null},abstract={One criticism often advanced against abstract argumentation frameworks (AFs), is that these consider only one form of interaction between atomic arguments: specifically that an argument attacks another. Attempts to broaden the class of relationships include bipolar frameworks, where arguments support others, and abstract dialectical frameworks (ADFs). The latter, allow "acceptance" of an argument, x, to be predicated on a given propositional function, Cx, dependent on the corresponding acceptance of its parents, i.e. those y for which 〈y, x〉 occurs. Although offering a richly expressive formalism subsuming both standard and bipolar AFs, an issue that arises with ADFs is whether this expressiveness is achieved in a manner that would be infeasible within standard AFs. Can the semantics used in ADFs be mapped to some AF semantics? How many arguments are needed in an AF to "simulate" an ADF? We show that (in a formally defined sense) any ADF can be simulated by an AF of similar size and that this translation can be realised by a polynomial time algorithm.}}
@ARTICLE{Crain_2005,title={The impact of regulatory costs on small firms},year={2005},author={Nicole V. Crain and Nicole V. Crain and W. Mark Crain and W. Mark Crain},doi={null},pmid={null},pmcid={null},mag_id={1738334925},journal={null},abstract={This research updates and delineates thedisproportionality of federal regulations' cost burden on small business. Thereport divides federal regulations into four categories: economic, workplace,environmental, and tax compliance. The two main components of economicregulation are estimated separately. The estimated costs of economic regulations affecting international tradeare mainly derived from the report issued by the U.S. International TradeCommission in 2004. Other data are from the Organization for EconomicCooperation and Development, with gaps in the baseline filled with Office ofManagement and Budget (OMB) estimates. The costs of workplace regulations are based on updated estimates from astudy by Johnson (Office of Advocacy, 2005). Environmental regulationsestimates are mainly based on OMB reports. Tax compliance costs are based on the 2002 report from the Tax Foundation.The allocation of costs across employment classes was derived from census datapublished by the Office of Advocacy. In the face of higher costs of federal regulations, findings show that smallbusinesses continue to bear a disproportionate share of the federal regulatoryburden, totaling $1.1 trillion or $7,647 per employee for firms with fewer thantwenty employees. (Abstract compiled from introductory matter)}}
@ARTICLE{Wagenaar_1994,title={Anchored Narratives: The Psychology of Criminal Evidence},year={1994},author={Willem A. Wagenaar and Willem A. Wagenaar and P.J. van Koppen and P.J. van Koppen and Peter J. van Koppen and H.F.M. Crombag and H.F.M. Crombag},doi={null},pmid={null},pmcid={null},mag_id={1744974913},journal={null},abstract={The theory of anchored narratives proof by narrative only the quality of anchors investigation and proof confessions identification witnesses expert testimony defense tactics selection of evidence.}}
@ARTICLE{Kirschner_2012,title={Visualizing Argumentation: Software Tools for Collaborative and Educational Sense-Making},year={2012},author={Paul A. Kirschner and Paul A. Kirschner and Simon Buckingham Shum and Simon Buckingham Shum and Chad S. Carr and Chad Carr},doi={null},pmid={null},pmcid={null},mag_id={1752174299},journal={null},abstract={About the book: Computer Supported Argument Visualization is attracting attention across education, science, public policy and business. More than ever, we need sense-making tools to help negotiate understanding in the face of multi-stakeholder, ill-structured problems. In order to be effective, these tools must support human cognitive and discursive processes, and provide suitable representations, services and user interfaces.

Visualizing Argumentation is written by practitioners and researchers for colleagues working in collaborative knowledge media, educational technology and organizational sense-making. It will also be of interest to theorists interested in software tools which embody different argumentation models. Particular emphasis is placed on the usability and effectiveness of tools in different contexts.}}
@ARTICLE{Breukers_2004,title={Epistemology and ontology in core ontologies: FOLaw and LRI-Core, two core ontologies for law},year={2004},author={J.A.P.J. Breukers and J.A.P.J. Breukers and Rinke Hoekstra and Rinke Hoekstra},doi={null},pmid={null},pmcid={null},mag_id={1752804396},journal={null},abstract={For more than a decade constructing ontologies for legal domains, we, at the Leibniz Center for Law, felt really the need to develop a core ontology for law that would enable us to re-use the common denominator of the various legal domains. In this paper we present two core ontologies for law. The first one was the result of a PhD thesis by [Valente, 1995], called FOLaw. FOLaw speci- fies functional dependencies between types of knowledge involved in legal reasoning. Despite the fact that FOLaw was the starting point for a number of ontologies and legal reasoning systems in various (European) projects, it is rather an epistemological framework than a (core) ontology. We are not the only ones who easily confound epistemology with ontology. In the paper we present some examples and discuss whether this epistemological promiscuity in (core) ontology development is a serious problem. It is to some extent, as it limits the scope of re-use (if not leading to confusion). Therefore, we started about four years ago the development of a 'real' core-ontology for law based upon notions of common sense. The reason for a common-sense foundation is that domain independent concepts of law - the common denominator - are still tainted with a strong common-sense flavor. Moreover, domains of law refer to social activities which are generally governed by common-sense notions. This core ontology, called LRI-Core, consists of five major portions ('worlds'): physical, mental and abstract classes; roles and occurrences.}}
@ARTICLE{Palmirani_2011,title={LegalRuleML: XML-based rules and norms},year={2011},author={Monica Palmirani and Monica Palmirani and Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Said Tabet and Said Tabet and Harold Boley and Harold Boley and Adrian Paschke and Adrian Paschke},doi={10.1007/978-3-642-24908-2_30},pmid={null},pmcid={null},mag_id={1752934815},journal={null},abstract={Legal texts are the foundational resource where to discover rules and norms that feed into different concrete (often XML-based) Web applications. Legislative documents provide general norms and specific procedural rules for eGovernment and eCommerce environments, while contracts specify the conditions of services and business rules (e.g. service level agreements for cloud computing), and judgments provide information about the legal argumentation and interpretation of norms to concrete case-law. Such legal knowledge is an important source that should be detected, properly modeled and expressively represented in order to capture all the domain particularities. This paper provides an extension of RuleML called LegalRuleML for fostering the characteristics of legal knowledge and to permit its full usage in legal reasoning and in the business rule domain. LegalRuleML encourages the effective exchange and sharing of such semantic information between legal documents, business rules, and software applications.}}
@ARTICLE{Dag_2004,title={Speeding up requirements management in a product software company: linking customer wishes to product requirements through linguistic engineering},year={2004},author={Johan Natt och Dag and J. Natt och Dag and Vincenzo Gervasi and Vincenzo Gervasi and Sjaak Brinkkemper and Sjaak Brinkkemper and Björn Regnell and Björn Regnell},doi={10.1109/re.2004.47},pmid={null},pmcid={null},mag_id={1772478219},journal={null},abstract={Developing large complex software products aimed for a broad market involves a great flow of wishes and requirements. The former are elicited from customers while the latter are brought forth by the developing organization. These are preferably kept separated to preserve the different perspectives. The interrelationships should however be identified and maintained to enable well-founded decisions. Unfortunately, the current manual linkage is cumbersome, time-consuming, and error-prone. This work presents a pragmatic linguistic engineering approach to how statistical natural language processing may be used to support the manual linkage between customer wishes and product requirements by suggesting potential links. An evaluation with real requirements from industry is presented. It shows that in a realistic setting, automatic support could make linkage faster for at least 50% of the links. An estimation based on our evaluation also shows that considerable time savings are possible. The results, together with the identified enhancement, are promising for improving software quality and saving time in industrial requirements engineering.}}
@ARTICLE{Modgil_2011,title={Revisiting preferences and argumentation},year={2011},author={Sanjay Modgil and Sanjay Modgil and Henry Prakken and Henry Prakken},doi={10.5591/978-1-57735-516-8/ijcai11-175},pmid={null},pmcid={null},mag_id={1780443464},journal={null},abstract={The ASPIC+ framework is intermediate in abstraction between Dung's argumentation framework and concrete instantiating logics. This paper generalises ASPIC+ to accommodate classical logic instantiations, and adopts a new proposal for evaluating extensions: attacks are used to define the notion of conflict-free sets, while the defeats obtained by applying preferences to attacks, are exclusively used to determine the acceptability of arguments. Key properties and rationality postulates are then shown to hold for the new framework.}}
@ARTICLE{Sadiq_2015,title={Managing Regulatory Compliance in Business Processes},year={2015},author={Shazia Sadiq and Shazia Sadiq and Guido Governatori and Guido Governatori},doi={10.1007/978-3-642-45103-4_11},pmid={null},pmcid={null},mag_id={1783586525},journal={null},abstract={The ever-increasing obligations of regulatory compliance are presenting a new breed of challenges for organizations across several industry sectors. Aligning control objectives that stem from regulations and legislation with business objectives devised for improved business performance is a foremost challenge. The organizational as well as IT structures for the two classes of objectives are often distinct and potentially in conflict. In this chapter, we present an overarching methodology for aligning business and control objectives. The various phases of the methodology are then used as a basis for discussing state-of-the-art in compliance management. Contributions from research and academia as well as industry solutions are discussed. The chapter concludes with a discussion on the role of BPM as a driver for regulatory compliance and a presentation of open questions and challenges.}}
@ARTICLE{Taghiabadi_2013,title={Diagnostic information for compliance checking of temporal compliance requirements},year={2013},author={Elham Ramezani Taghiabadi and Elham Ramezani Taghiabadi and Dirk Fahland and Dirk Fahland and Boudewijn F. van Dongen and Boudewijn F. van Dongen and Wil M. P. van der Aalst and Wil M. P. van der Aalst},doi={10.1007/978-3-642-38709-8_20},pmid={null},pmcid={null},mag_id={1793272377},journal={null},abstract={Compliance checking is gaining importance as today's organizations need to show that operational processes are executed in a controlled manner while satisfying predefined (legal) requirements or service level agreements. Deviations may be costly and expose an organization to severe risks. Compliance checking is of growing importance for the business process management and auditing communities. This paper presents an approach for checking compliance of observed process executions recorded in an event log to temporal compliance requirements, which restrict when particular activities may or may not occur. We show how temporal compliance requirements discussed in literature can be unified and formalized using a generic temporal compliance rule. To check compliance with respect to a temporal rule, the event log describing the observed behavior is aligned with the rule. The alignment then shows which events occurred out of order and which events deviated by which amount of time from the prescribed behavior. This approach integrates with an existing approach for control-flow compliance checking, allowing for multi-perspective diagnostic information in case of compliance violations. We show the feasibility of our technique by checking temporal compliance rules of real life event logs.}}
@ARTICLE{Abdullah_2010,title={Emerging challenges in information systems research for regulatory compliance management},year={2010},author={Norris Syed Abdullah and Norris Syed Abdullah and Shazia Sadiq and Shazia Sadiq and Marta Indulska and Marta Indulska},doi={10.1007/978-3-642-13094-6_21},pmid={null},pmcid={null},mag_id={1798675754},journal={null},abstract={Managing regulatory compliance is increasingly challenging and costly for organizations world-wide. While such efforts are often supported by information technology (IT) and information systems (IS) tools, there is evidence that the current solutions are inadequate and do not fully address the needs of organizations. Often such discrepancy stems from a lack of alignment between the needs of the industry and the focus of academic research efforts. In this paper, we present the results of an empirical study that investigates challenges in managing regulatory compliance, derived from expert professionals in the Australian compliance industry. The results provide insights into problematic areas within the compliance management domain, as related to regulatees, regulations and IT compliance management solutions. By relating the identified challenges to existing activity in IS research, this exploratory paper highlights the inadequacy of current research and presents the first industry-relevant compliance management research agenda for IS researchers.}}
@ARTICLE{Kiyavitskaya_2007,title={Annotating Accommodation Advertisements Using CERNO},year={2007},author={Nadzeya Kiyavitskaya and Nadzeya Kiyavitskaya and Nicola Zeni and Nicola Zeni and Luisa Mich and Luisa Mich and James R. Cordy and James R. Cordy and John Mylopoulos and John Mylopoulos},doi={10.1007/978-3-211-69566-1_36},pmid={null},pmcid={null},mag_id={1801770420},journal={null},abstract={There has been great interest in applying Semantic Web technologies to the tourism sector ever since Tim Berners-Lee introduced his vision. Unfortunately, there is a major obstacle in realizing such applications: tourist (or other) information on the Web has to be semantically annotated, and this happens to be a very time- and resource-consuming process. In this work we present the application of a lightweight automated approach for the annotation of accommodation advertisements. The annotation tool, called Cerno, allows for annotation of text according to a predefined conceptual schema. Resulting annotations are stored in a database, allowing users to quickly find the best match to personal requirements. To evaluate our framework, we have conducted a series of experiments that support the efficacy of our proposal with respect to annotation quality and fulfilment of user information needs.}}
@ARTICLE{Prakken_2001,title={Modelling Defeasibility in Law: Logic or Procedure?},year={2001},author={Henry Prakken and Henry Prakken},doi={null},pmid={null},pmcid={null},mag_id={1803159072},journal={null},abstract={This paper investigates whether current nonmonotonic logics are suitable for formalising the defeasibility of legal reasoning. It does so by studying the role of burden of proof in legal argument, in particular how allocations of burden of proof determine the required strength of counterarguments. It is argued that the two currently available modelling approaches both have some shortcomings. On the one hand, techniques for modelling burden of proof in nonmonotonic logics do not allow for shifts of the burden of proof from one party to the other. On the other hand, current procedural models of legal argument are too rigid, in that every counterargument induces a shift of proof burdens; this fails to respect that in legal reasoning burden shifts only occur in some cases. It is then shown how current dialectical models of defeasible reasoning can be adapted to overcome these shortcomings.}}
@ARTICLE{Ceci_2016,title={An OWL ontology library representing judicial interpretations},year={2016},author={Marcello Ceci and Marcello Ceci and Marcello Ceci and Aldo Gangemi and Aldo Gangemi},doi={10.3233/sw-140146},pmid={null},pmcid={null},mag_id={1811929041},journal={Sprachwissenschaft},abstract={The article introduces JudO, an OWL2 ontology library of legal knowledge that relies on the metadata contained in judicial documents. JudO represents the interpretations performed by a judge while conducting legal reasoning towards the adjudication of a case. To the aim of this application, judicial interpretation is intended in the restricted sense of the acts of judicial subsumption performed by the judge when he considers a material instance (token in Searle's terminology), and assigns it to an abstract category (type). The ontology library is based on a theoretical model and on some specific patterns that exploit some new features introduced by OWL2. JudO provides meaningful legal semantics, while retaining a strong connection to source documents (fragments of legal texts). The application task is to enable detection and modeling of jurisprudence-related information directly from the text, and to perform shallow reasoning on the resulting knowledge base. The ontology library is also supposed to support a defeasible rule set for legal argumentation on the groundings of judicial decisions.}}
@ARTICLE{Searle_2001,title={Rationality in Action},year={2001},author={John R. Searle and Francois Recanati and John R. Searle},doi={null},pmid={null},pmcid={null},mag_id={1812422352},journal={null},abstract={The study of rationality and practical reason, or rationality in action, has been central to Western intellectual culture. In this invigorating book, John Searle lays out six claims of what he calls the Classical Model of rationality and shows why they are false. He then presents an alternative theory of the role of rationality in thought and action. A central point of Searle's theory is that only irrational actions are directly caused by beliefs and desires -- for example, the actions of a person in the grip of an obsession or addiction. In most cases of rational action, there is a gap between the motivating desire and the actual decision making. The traditional name for this gap is "freedom of the will." According to Searle, all rational activity presupposes free will. For rationality is possible only where one has a choice among various rational as well as irrational options. Unlike many philosophical tracts, Rationality in Action invites the reader to apply the author's ideas to everyday life. Searle shows, for example, that contrary to the traditional philosophical view, weakness of will is very common. He also points out the absurdity of the claim that rational decision making always starts from a consistent set of desires. Rational decision making, he argues, is often about choosing between conflicting reasons for action. In fact, humans are distinguished by their ability to be rationally motivated by desire-independent reasons for action. Extending his theory of rationality to the self, Searle shows how rational deliberation presupposes an irreducible notion of the self. He also reveals the idea of free will to be essentially a thesis of how the brain works.}}
@ARTICLE{Breuker_2005,title={Use and reuse of legal ontologies in knowledge engineering and information management},year={2005},author={Joost Breuker and J.A.P.J. Breuker and André Valente and Andre Valente and Radboud Winkels and Radboud Winkels},doi={10.1007/978-3-540-32253-5_4},pmid={null},pmcid={null},mag_id={1827110031},journal={null},abstract={In this article we present an overview of legal ontological modeling over a period of more than a decade. Most of the research reported concerns results from mid-size (European) projects aimed at the development of legal reasoning and information management tools and systems. In these projects we developed ontologies for several legal or regulation domains. However, the main thread of this article is provided by fundamental research performed by us or under our supervision (e.g. PhD theses by [1], [2], [3]), leading to more abstract legal ‘core' ontologies and legal reasoning architectures. The major insights we have obtained from these experiences can be summarized as follows:

Legal sources contain and assume non-legal, common-sense based domain knowledge. Therefore, a legal core ontology should be rooted in highly abstract common sense concepts as a foundational ontology. This notion is worked out in the LRI-Core ontology, presented in this article.

What remains as typical legal knowledge to be modelled is normative and legal responsability knowledge. However, in this article we only summarize our work in this area, refering to [4] and to [5] on these issues.

As law and legal theory is focussed on questions of justification, legal (core) ontologies, e.g. FOLaw, are epistemological frameworks, describing legal reasoning, rather than legal ontologies, explaining knowledge resources.}}
@ARTICLE{Scacchi_2006,title={Understanding Open Source Software Evolution 181},year={2006},author={Walt Scacchi and Walt Scacchi},doi={10.1002/0470871822.ch9},pmid={null},pmcid={null},mag_id={1838610867},journal={null},abstract={This chapter contains sections titled: Introduction Empirical Studies of Software Evolution Evolutionary Patterns in Open Source Software Evolution Models and Theories Do We Need New or Revised Models, Laws or Theories for Open Source Software Evolution? Conclusions Acknowledgements References}}
@ARTICLE{Smith_1970,title={Derivation and Validation of the Automated Readability Index for Use with Technical Materials},year={1970},author={Edgar A. Smith and Edgar A. Smith and J. Peter Kincaid and J. Peter Kincaid and J. Peter Kincaid},doi={10.1177/001872087001200505},pmid={null},pmcid={null},mag_id={1841018194},journal={Human Factors},abstract={The utility of technical materials is influenced to a marked extent by their reading level or readability. This article describes the derivation and validation of the Automated Readability Index (ARI) for use with technical materials. The method allows for the easy, automatic collection of data as narrative material is typed on a slightly modified electric typewriter. Data collected includes word length (a measure of word difficulty) and sentence length (a measure of sentence difficulty). Appropriate weightings of these factors in a multiple regression equation result in an index of reading difficulty. Uses of the index for evaluating and controlling the readability of large quantities of technical material are described.}}
@ARTICLE{Roth_2004,title={Cases and Dialectical Arguments – An Approach to Case-Based Reasoning},year={2004},author={Bram Roth and Bram Roth and Bart Verheij and Bart Verheij},doi={10.1007/978-3-540-30470-8_75},pmid={null},pmcid={null},mag_id={1844793792},journal={Lecture Notes in Computer Science},abstract={Case-based reasoning in the law is a reasoning strategy in which legal conclusions are supported by decisions made by judges. If the case at hand is analogous to a settled case, then by judicial authority one can argue that the settled case should be followed. Case-based reasoning is a topic where ontology meets logic since one’s conception of cases determines one’s conception of reasoning with cases. In the paper, it is shown how reasoning with cases can be modelled by comparing the corresponding dialectical arguments. A unique characteristic thereby is the explicit recognition that it is in principle contingent which case features are relevant for case comparison. This contigency gives rise to some typical reasoning patterns. The present work is compared to other existing approaches to reasoning by case comparison, and some work on legal ontologies is briefly discussed regarding the role attributed to cases.}}
@ARTICLE{Wyner_2010,title={Towards Annotating and Extracting Textual Legal Case Elements},year={2010},author={Adam Wyner and Adam Wyner},doi={null},pmid={null},pmcid={null},mag_id={1851605424},journal={null},abstract={In common law contexts, legal cases are decided with respect to precedents rather than legislation as in civil law contexts. Legal professionals must find, analyse, and reason with and about cases drawn from a set of cases (a case base). A range of particular textual elements of a case may be relevant to query and extract. Commercial providers of legal information allow legal professionals to search a case base by keywords and meta data. However, the case base and the search tools are proprietary, of limited, non-extensible functionality, and are restricted access. Moreover, no provider applies natural language processing techniques to the cases for text analysis, XML annotation, or information acquisition. In this paper, we discuss an initial experiment in developing and applying natural language processing tools to cases to produce annotated text which can then support information extraction.}}
@ARTICLE{Yu_1993,title={Modeling organizations for information systems requirements engineering},year={1993},author={Eric Yu and Eric Yu},doi={10.1109/isre.1993.324839},pmid={null},pmcid={null},mag_id={1859760514},journal={Requirements Engineering},abstract={In attempting to understand information system environments during requirements engineering, it is often helpful to have an understanding of the 'whys' as well as the 'whats' about the environment. A natural way to answer why questions is by tracing them to goals. In an organizational environment, however, the whys do not originate from a single set of given goals. Organizational agents depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. A requirements model that captures knowledge about an organizational environment can be enriched by including the network of dependency relationships among agents. A set of intentional operators for modeling dependencies among agents is proposed, and a preliminary axiomatic characterization is presented. >}}
@ARTICLE{Berenbach_2009,title={Software & Systems Requirements Engineering: In Practice},year={2009},author={Brian Berenbach and Brian Berenbach and Daniel J. Paulish and Daniel J. Paulish and Juergen Kazmeier and Juergen Kazmeier and Arnold Rudorfer and Arnold Rudorfer and Arnold Rudorfer},doi={null},pmid={null},pmcid={null},mag_id={1861088438},journal={null},abstract={Proven Software & Systems Requirements Engineering Techniques
"Requirements engineering is a discipline used primarily for large and complex applications. It is more formal than normal methods of gathering requirements, and this formality is needed for many large applications. The authors are experienced requirements engineers, and this book is a good compendium of sound advice based on practical experience." --Capers Jones, Chief Scientist Emeritus, Software Productivity Research
Deliver feature-rich products faster, cheaper, and more reliably using state-of-the-art SSRE methods and modeling procedures. Written by global experts, Software & Systems Requirements Engineering: In Practice explains how to effectively manage project objectives and user needs across the entire development lifecycle. Gather functional and quality attribute requirements, work with models, perform system tests, and verify compliance. You will also learn how to mitigate risks, avoid requirements creep, and sidestep the pitfalls associated with large, complex projects.
Define and prioritize customer expectations using taxonomies
Elicit and analyze functional and quality attribute requirements
Develop artifact models, meta-models, and prototypes
Manage platform and product line development requirements
Derive and generate test cases from UML activity diagrams
Deploy validation, verification, and rapid development procedures
Handle RE for globally distributed software and system development projects 
Perform hazard analysis, risk assessment, and threat modeling


Table of contents
Chapter 1. Introduction
Chapter 2. Requirements Engineering Artifact Modeling
Chapter 3. Eliciting Requirements
Chapter 4. Requirements Modeling
Chapter 5. Quality Attribute Requirements
Chapter 6. Requirements Engineering for Platforms
Chapter 7. Requirements Management
Chapter 8. Requirements-Driven System Testing
Chapter 9. Rapid Development Techniques for Requirements Evolution
Chapter 10. Distributed Requirements Engineering
Chapter 11. Hazard Analysis and Threat Modeling
Chapter 12. Conclusion
Appendix A. Configuring and Managing a Requirements Database
Index}}
@ARTICLE{Kayed_2005,title={Building e-laws ontology: new approach},year={2005},author={Ahmad Kayed and Ahmad Kayed},doi={10.1007/11575863_104},pmid={null},pmcid={null},mag_id={1865238256},journal={null},abstract={Semantic Web provides tools for expressing information in a machine accessible form where agents (human or software) can understand it. Ontology is required to describe the semantics of concepts and properties used in web documents. Ontology is needed to describe products, services, processes and practices in any e-commerce application. Ontology plays an essential role in recognizing the meaning of the information in Web documents. This paper attempts to deploy these concepts in an e-law application. E-laws ontology has been built using existing resources. It has been shown that extracting concepts is less hard than building relationships among them. A new algorithm has been proposed to reduce the number of relationships, so the domain knowledge expert (i.e. lawyer) can refine these relationships.}}
@ARTICLE{Allen_1957,title={SYMBOLIC LOGIC: A RAZOR-EDGED TOOL FOR DRAFTING AND INTERPRETING LEGAL DOCUMENTS},year={1957},author={Layman E. Allen and Layman E. Allen},doi={10.2307/794073},pmid={null},pmcid={null},mag_id={1868697350},journal={Yale Law Journal},abstract={A large amount of the litigation based on written instruments?whether statute, contract, will, conveyance or regulation?can be traced to the draftsman's failure to convey his meaning clearly. Frequently, of course, certain items may purposely be left ambiguous, but often the question in issue is due to an inadvertent ambiguity that could have been avoided had the draftsman clearly expressed what he intended to say. In this Article it is suggested that a new approach to drafting, using certain elementary notions of symbolic logic, can go a long way towards eliminating such inadvertent ambiguity. This new approach makes available to draftsmen a technique that achieves some of the clarity, precision and efficiency of analysis that symbolic logic provides. In addition, it can be a valuable aid in moving towards a more comprehensive and systematic method of interpretation,1 as well as drafting. This approach is a compromise between expression in ordinary prose and expression in the mathematical notation of symbolic logic?enough like ordi? nary prose to be understood easily by any careful reader, enough like sym? bolic logic to achieve some of its important advantages. It represents an effort to adapt some of the techniques of symbolic logic to make more systematic what is now best described as the "art" of drafting. The first section will explain six elementary logical connectives: implication, conjunction, coimplication, exclusive disjunction, inclusive disjunction and negation. In order to simplify this exposition, trivial examples will be used for purposes of illustration. In the second section the proposed system will be applied to actual legal problems of drafting, interpretation, simplification and comparison. Six Elementary Logical Connectives 2 1.0 Implication The development of a more systematic method of drafting will enable the lawyer to communicate his intended meaning more effectively. That is the basic proposition to which this Article is addressed. This same proposition can be stated in a different form:}}
@ARTICLE{Kiyavitskaya_2006,title={Text mining through semi automatic semantic annotation},year={2006},author={Nadzeya Kiyavitskaya and Nadzeya Kiyavitskaya and Nicola Zeni and Nicola Zeni and Luisa Mich and Luisa Mich and James R. Cordy and James R. Cordy and John Mylopoulos and John Mylopoulos},doi={10.1007/11944935_13},pmid={null},pmcid={null},mag_id={1870211331},journal={null},abstract={The Web is the greatest information source in human history. Unfortunately, mining knowledge out of this source is a laborious and error-prone task. Many researchers believe that a solution to the problem can be founded on semantic annotations that need to be inserted in web-based documents and guide information extraction and knowledge mining. In this paper, we further elaborate a tool-supported process for semantic annotation of documents based on techniques and technologies traditionally used in software analysis and reverse engineering for large-scale legacy code bases. The outcomes of the paper include an experimental evaluation framework and empirical results based on two case studies adopted from the Tourism sector. The conclusions suggest that our approach can facilitate the semi-automatic annotation of large document bases.}}
@ARTICLE{Antón_1996,title={Goal-based requirements analysis},year={1996},author={Annie I. Antón and Annie I. Antón},doi={10.1109/icre.1996.491438},pmid={null},pmcid={null},mag_id={1875598785},journal={null},abstract={Goals are a logical mechanism for identifying, organizing and justifying software requirements. Strategies are needed for the initial identification and construction of goals. We discuss goals from the perspective of two themes: goal analysis and goal evolution. We begin with an overview of the goal-based method we have developed and summarize our experiences in applying our method to a relatively large example. We illustrate some of the issues that practitioners face when using a goal-based approach to specify the requirements for a system and close the paper with a discussion of needed future research on goal-based requirements analysis and evolution.}}
@ARTICLE{Fornara_2009,title={Ontology and time evolution of obligations and prohibitions using semantic web technology},year={2009},author={Nicoletta Fornara and Nicoletta Fornara and Marco Colombetti and Marco Colombetti},doi={10.1007/978-3-642-11355-0_7},pmid={null},pmcid={null},mag_id={1890315360},journal={null},abstract={The specification and monitoring of conditional obligations and prohibitions with starting points and deadlines is a crucial aspect in the design of open interaction systems. In this paper we regard such obligations and prohibitions as cases of social commitment, and propose to model them in OWL, the logical language recommended by the W3C for Semantic Web applications. In particular we propose an application-independent ontology of the notions of social commitment, temporal proposition, event, agent, role and norms that can be used in the specification of any open interaction system. We then delineate a hybrid solution that uses the OWL ontology, SWRL rules, and a Java program to dynamically monitor or simulate the temporal evolution of social commitments, due to the elapsing of time and to the actions performed by the agents interacting within the system.}}
@ARTICLE{Prakken_2008,title={More on Presumptions and Burdens of Proof},year={2008},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.2139/ssrn.1317348},pmid={null},pmcid={null},mag_id={1919804909},journal={null},abstract={This paper extends our previous logical analysis of presumptions and burden of proof by studying the force of a presumption once counterevidence has been offered. In the jurisprudential literature different accounts of this issue have been given: some have argued that a presumption is nullified by counterarguments while others have maintained that this gives presumptions a force that is too slight. We argue that these differences largely are not a matter of logic but of legal policy, and we show how the various accounts can be logically formalised.}}
@ARTICLE{Dahlman_2013,title={Legal argumentation theory: Cross-disciplinary perspectives},year={2013},author={Christian Dahlman and Christian Dahlman and Eveline T. Feteris and Eveline T. Feteris},doi={10.1007/978-94-007-4670-1},pmid={null},pmcid={null},mag_id={1920913053},journal={null},abstract={Table of Contents.- Introduction Christian Dahlman and Eveline Feteris.- Reasoning by Consequences: Applying Different Argumentation Structures to the Analysis of Consequentialist Reasoning in Judicial Decisions Flavia Carbonell.- On the Argumentum ad Absurdum Statutory Interpretation: its Uses and Normative Significance Thomas Bustamante.- Why precedent in law (and elsewhere) is not totally (or even substantially) about analogy Frederick Schauer.- Fallacies in Ad Hominem Arguments Christian Dahlman, David Reidhav and Lena Wahlberg.- The Rule of Law and the Ideal of a Critical Discussion Harm Kloosterhuis.- Strategic Maneuvering with the Argumentative Role of Legal Principles in the Case of the "Unworthy Spouse" Eveline Feteris.- Legal argumentation and the normativity of legal Norms Carlos Bernal.- Weighing and Balancing in the Light of Deliberation and Expression Bruce Anderson.- Construction or reconstruction? On the Function of Argumentation in the Law Jaap Hage.- The Argument from Psychological Typology for a Mild Separation between the Context of Discovery and the Context of Justification Marko Novak.- Constitutive Rules and Coherence in Legal Argumentation: The Case of Extensive and Restrictive Interpretation Antonino Rotolo and Corrado Roversi.- Is Balancing a Method of Rational Justification sui generis? Jan Sieckmann.- Arguing on Facts. Truth, Trials and Adversary Procedures. Giovanni Tuzet.- Index.}}
@ARTICLE{Pressman_1982,title={Software Engineering: A Practitioner's Approach},year={1982},author={Roger S. Pressman},doi={null},pmid={null},pmcid={null},mag_id={1936022305},journal={null},abstract={null}}
@ARTICLE{Ven_2008,title={Automated Legal Assessment in OWL 2},year={2008},author={Saskia van de Ven and Saskia van de Ven and Joost Breuker and Joost Breuker and Rinke Hoekstra and Rinke Hoekstra and Lars Wortel and Lars Wortel},doi={10.3233/978-1-58603-952-3-170},pmid={null},pmcid={null},mag_id={1947337586},journal={null},abstract={In this paper we describe HARNESS, a legal knowledge-based system that is developed in the context of ESTRELLA. This system is primarily aimed at the task of legal assessment, i.e. determining whether some case violates and/or complies with legal norms. We explain how the sound and complete reasoning provided by OWL-DL reasoners is exploited by a careful representation of norms, using examples from law on taxation of gifts. We describe a plugin for Protege 4 that enables easy experimentation for this system, powered by Pellet.}}
@ARTICLE{DesRoches_2008,title={Electronic Health Records in Ambulatory Care — A National Survey of Physicians},year={2008},author={Catherine M. DesRoches and Catherine M. DesRoches and Eric G. Campbell and Eric G. Campbell and Sowmya R. Rao and Sowmya R. Rao and Karen Donelan and Karen Donelan and Timothy G. Ferris and Timothy G. Ferris and Ashish K. Jha and Ashish K. Jha and Rainu Kaushal and Rainu Kaushal and Douglas E. Levy and Douglas E. Levy and Sara Rosenbaum and Sara J. Rosenbaum and Alexandra E. Shields and Alexandra E. Shields and David Blumenthal and David Blumenthal},doi={10.1056/nejmsa0802005},pmid={18565855},pmcid={null},mag_id={1963918827},journal={The New England Journal of Medicine},abstract={Background Electronic health records have the potential to improve the delivery of health care services. However, in the United States, physicians have been slow to adopt such systems. This study assessed physicians' adoption of outpatient electronic health records, their satisfaction with such systems, the perceived effect of the systems on the quality of care, and the perceived barriers to adoption. Methods In late 2007 and early 2008, we conducted a national survey of 2758 physicians, which represented a response rate of 62%. Using a definition for electronic health records that was based on expert consensus, we determined the proportion of physicians who were using such records in an office setting and the relationship between adoption and the characteristics of individual physicians and their practices. Results Four percent of physicians reported having an extensive, fully functional electronic-records system, and 13% reported having a basic system. In multivariate analyses, primary care physicians a...}}
@ARTICLE{Amgoud_2008,title={On bipolarity in argumentation frameworks},year={2008},author={Leïla Amgoud and Leila Amgoud and Claudette Cayrol and Claudette Cayrol and Claudette Cayrol and Marie-Christine Lagasquie-Schiex and Marie-Christine Lagasquie-Schiex and Pierre Livet and P. Livet},doi={10.1002/int.v23:10},pmid={null},pmcid={null},mag_id={1964816496},journal={International Journal of Intelligent Systems},abstract={In this article, we propose a survey of the use of bipolarity in argumentation frameworks. On the one hand, the notion of bipolarity relies on the presence of two kinds of entities that have a diametrically opposed nature and that represent repellent forces (a positive entity and a negative entity). The notion exists in various domains (for example with the representation of preferences in artificial intelligence, or in cognitive psychology). On the other hand, argumentation process is a promising approach for reasoning, based on the construction and the comparison of arguments. It follows five steps: building the arguments, defining the interactions between these arguments, valuating the arguments, selecting the most acceptable arguments and, finally, drawing a conclusion. Using the nomenclature proposed by Dubois and Prade, this article shows on various applications, and with some formal definitions, that bipolarity appears in argumentation (in some cases if not always) and can be used in each step of this process under different forms. © 2008 Wiley Periodicals, Inc.}}
@ARTICLE{Zou_2010,title={Improving automated requirements trace retrieval: a study of term-based enhancement methods},year={2010},author={Xuchang Zou and Xuchang Zou and Raffaella Settimi and Raffaella Settimi and Jane Cleland‐Huang and Jane Cleland-Huang},doi={10.1007/s10664-009-9114-z},pmid={null},pmcid={null},mag_id={1965177473},journal={Empirical Software Engineering},abstract={Automated requirements traceability methods that utilize Information Retrieval (IR) methods to generate and maintain traceability links are often more efficient than traditional manual approaches, however the traces they generate are imprecise and significant human effort is needed to evaluate and filter the results. This paper investigates and compares three term-based enhancement methods that are designed to improve the performance of a probabilistic automated tracing tool. Empirical studies show that the enhancement methods can be effective in increasing the accuracy of the retrieved traces; however the effectiveness of each method varies according to specific project characteristics. The analysis of such characteristics has lead to the development of two new project-level metrics which can be used to predict the effectiveness of each enhancement method for a given data set. A procedure to automatically extract critical keywords and phrases from a set of traceable artifacts is also presented to enhance the automated trace retrieval algorithm. The procedure is tested on two new datasets.}}
@ARTICLE{McGee_2012,title={Towards an understanding of the causes and effects of software requirements change: two case studies},year={2012},author={Sharon McGee and Sharon McGee and Des Greer and Des Greer},doi={10.1007/s00766-012-0149-0},pmid={null},pmcid={null},mag_id={1965638906},journal={Requirements Engineering},abstract={Changes to software requirements not only pose a risk to the successful delivery of software applications but also provide opportunity for improved usability and value. Increased understanding of the causes and consequences of change can support requirements management and also make progress towards the goal of change anticipation. This paper presents the results of two case studies that address objectives arising from that ultimate goal. The first case study evaluated the potential of a change source taxonomy containing the elements ‘market’, ‘organisation’, ‘vision’, ‘specification’, and ‘solution’ to provide a meaningful basis for change classification and measurement. The second case study investigated whether the requirements attributes of novelty, complexity, and dependency correlated with requirements volatility. While insufficiency of data in the first case study precluded an investigation of changes arising due to the change source of ‘market’, for the remainder of the change sources, results indicate a significant difference in cost, value to the customer and management considerations. Findings show that higher cost and value changes arose more often from ‘organisation’ and ‘vision’ sources; these changes also generally involved the co-operation of more stakeholder groups and were considered to be less controllable than changes arising from the ‘specification’ or ‘solution’ sources. Results from the second case study indicate that only ‘requirements dependency’ is consistently correlated with volatility and that changes coming from each change source affect different groups of requirements. We conclude that the taxonomy can provide a meaningful means of change classification, but that a single requirement attribute is insufficient for change prediction. A theoretical causal account of requirements change is drawn from the implications of the combined results of the two case studies.}}
@ARTICLE{Amardeilh_2005,title={Document annotation and ontology population from linguistic extractions},year={2005},author={Florence Amardeilh and Florence Amardeilh and Philippe Laublet and Philippe Laublet and Jean-Luc Minel and Jean-Luc Minel and Jean-Luc Minel and Jean-Luc Minel},doi={10.1145/1088622.1088651},pmid={null},pmcid={null},mag_id={1965790416},journal={null},abstract={In this paper, we present a workbench for semi-automatic ontology population from textual documents. It provides an environment for mapping the linguistic extractions with the domain ontology thanks to knowledge acquisition rules. Those rules are activated when a pertinent linguistic tag is reached. Those linguistic tags are then mapped to a concept, one of its attributes or even a semantic relation between several concepts. The rules instantiate these concepts, attributes and relations in the knowledge base constrained by the domain ontology. This paper deals with the underlying knowledge capture process and presents the first experiments realized on a real client application from the legal publishing domain.}}
@ARTICLE{Aleven_1993,title={What law students need to know to WIN},year={1993},author={Vincent Aleven and Vincent Aleven and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/158976.158995},pmid={null},pmcid={null},mag_id={1966158111},journal={null},abstract={To make legal arguments, one needs certain information about how to use cases effectively - dialectical information. In the broadest sense, dialectical information includes strategies for employing cases to justify legal conclusions (and responding to such justifications) and criteria for finding cases and deciding which cases to use. Making dialectical information explicit is important for teaching case-based argument. It is our experience that typically, law students do not have a very good set of dialectical strategies nor are they aware of the criteria. Even the most sophisticated legal information retrieval tools do not make such dialectical information explicit and assume that users have already learned it.  For purposes of instruction, we have identified some useful dialectical information comprising a flexible argument plan, a set of eight argument moves that embody standard ways for using cases as examples in an argument, factors for representing factual strengths and weaknesses in cases, and an annotated Claim Lattice for organizing cases for the purpose of selecting and making argument moves to implement the plan. Our tutorial program CATO makes this dialectical information explicit through a combination of information retrieval tools and graphical display. In this paper, an extended example illustrated the utility, for an expert, of this dialectical information for constructing a sophisticated legal argument from the argument moves. We believe that practice with the CATO program will make students aware of the existence of argumentative strategies and criteria and help them to apply this dialectical information to construct better arguments.}}
@ARTICLE{Jain_2010,title={A systems integration framework for process analysis and improvement},year={2010},author={Rashmi Jain and Anithashree Chandrasekaran and Ozgur Erol},doi={10.1002/sys.v13:3},pmid={null},pmcid={null},mag_id={1966374171},journal={Systems Engineering},abstract={Systems Integration (SI) is an important element of Systems Engineering. It involves the integration of hardware, software, products, services, processes, and humans. The ever-increasing scale of complexity of systems and its impact on the business requires that we revisit the processes involved in the development and integration of a system. This paper proposes a Systems Integration Process Model (SIPM) based on a comprehensive lifecycle view of systems integration. As part of the ongoing SI research at Stevens Institute of Technology, the authors have developed a Systems Integration Framework (SIF) which incorporates the relevant aspects of integration from a lifecycle perspective and sets a foundation to an end-to-end approach to SI. Our end-to-end approach focuses on how integration issues can be addressed up-front to minimize integration related complexities and challenges later on in the system engineering process. This paper discusses the merits and benefits of applying the SIPM to evaluate and improve current SI processes in organizations. The paper provides, in addition to an overview of the SI framework, the activities included in the model. The model was pilot tested to evaluate the SI processes at a government agency. The results were used to provide recommendations for SI process reengineering. © 2009 Wiley Periodicals, Inc. Syst Eng}}
@ARTICLE{Flesch_1948,title={A new readability yardstick.},year={1948},author={Rudolph Flesch and Rudolf Franz Flesch and Rudolph Flesch},doi={10.1037/h0057532},pmid={18867058},pmcid={null},mag_id={1967390364},journal={Journal of Applied Psychology},abstract={null}}
@ARTICLE{Osborn_1999,title={JUSTICE: a judicial search tool using intelligent concept extraction},year={1999},author={James Osborn and James Osborn and Leon Sterling and Leon Sterling},doi={10.1145/323706.323792},pmid={null},pmcid={null},mag_id={1968052172},journal={null},abstract={A legal knowledge based system called JUSTICE is presented which provides conceptual information retrieval for legal cases. JUSTICE can identify heterogeneous representations of concepts across all major Australian jurisdictions. The knowledge representation scheme used for legal and common sense concepts is inspired by human processes for the identification of concepts and the expected order and location of concepts. These are supported by flexible search functions and various string utilities. JUSTICE is a client-based legal software agent which works with both plaintext and HTML representations of legal cases over file systems, and the World Wide Web. In creating JUSTICE an ontology for legal cases was developed, and is implicit within JUSTICE. Further, the identification of concepts within data is shown to be a process enabling conceptual information retrieval and search, conceptualised summarisation, automated statistical analysis, and the conversion of informal documents into formalised semi-structured representations. JUSTICE was tested on the precision, recall and usefulness of its concept identifications; achieving good results. The results show the promise of the approach and establish JUSTICE as an intelligent legal research aid offering improved multifaceted access to the concepts within legal cases.}}
@ARTICLE{Price_2009,title={Using semantic components to search for domain-specific documents: An evaluation from the system perspective and the user perspective},year={2009},author={Susan Price and Susan Price and Marianne Lykke and Marianne Lykke Nielsen and Lois Delcambre and Lois Delcambre and Peter Vedsted and Peter Vedsted and Jeremy Steinhauer and Jeremy Steinhauer},doi={10.1016/j.is.2009.04.005},pmid={null},pmcid={null},mag_id={1968580477},journal={Information Systems},abstract={null}}
@ARTICLE{Hachey_2005,title={Automatic legal text summarisation: experiments with summary structuring},year={2005},author={Ben Hachey and Ben Hachey and Claire Grover and Claire Grover},doi={10.1145/1165485.1165498},pmid={null},pmcid={null},mag_id={1968776705},journal={null},abstract={We describe a set of experiments using machine learning techniques for the task of extractive summarisation. The research is part of a summarisation project for which we use a corpus of judgments of the UK House of Lords. We present classification results for naive Bayes and maximum entropy and we explore methods for scoring the summary-worthiness of a sentence. We present sample output from the system, illustrating the utility of rhetorical status information, which provides a means for structuring summaries and tailoring them to different types of users.}}
@ARTICLE{Edwards_2009,title={7.3.1 The Impact of Technical Regulation on the Technical Integrity of Complex Engineered Systems},year={2009},author={Michael Edwards and Michael Edwards},doi={10.1002/j.2334-5837.2009.tb01007.x},pmid={null},pmcid={null},mag_id={1968994799},journal={null},abstract={The explicit aim of the application of technical regulation is to ensure that the requisite Technical Integrity of the developed system is achieved. Technical Integrity encompasses fitness for service, system safety and minimised environmental impact. The beginnings of a program of research to test the hypothesis that “The application of technical regulation yields improved technical integrity of the complex engineered system” are established. It is shown that only the intended emergent properties of a complex engineered system and its unintended hazards are directly addressed by technical regulation. A candidate set of measures to assess technical integrity is proposed. The methods that may enhance technical integrity of complex systems are reviewed. The degrees to which technical regulatory frameworks actually enforce methods known to have a contribution to technical integrity are also reviewed resulting in a list of corollary questions to be considered by future research.}}
@ARTICLE{Juristo_1998,title={Common framework for the evaluation process of KBS and conventional software},year={1998},author={Natália Juristo and Natalia Juristo and J. L. Morant and J. L. Morant and J. L. Morant and J. L. Morant},doi={10.1016/s0950-7051(98)00047-1},pmid={null},pmcid={null},mag_id={1969593469},journal={Knowledge Based Systems},abstract={null}}
@ARTICLE{Appelt_1998,title={THE COMMON PATTERN SPECIFICATION LANGUAGE},year={1998},author={Douglas E. Appelt and Douglas E. Appelt and Boyan Onyshkevych and Boyan Onyshkevych and Boyan Onyshkevych and Boyan Onyshkevych},doi={10.3115/1119089.1119095},pmid={null},pmcid={null},mag_id={1969595233},journal={null},abstract={This paper describes the Common Pattern Specification Language (CPSL) that was developed during the TIPSTER program by a committee of researchers from the TIPSTER research sites. Many information extraction systems work by matching regular expressions over the lexical features of input symbols. CPSL was designed as a language for specifying such finite-state grammars for the purpose of specifying information extraction rules in a relatively system-independent way. The adoption of such a common language would enable the creation of shareable resources for the development of rule-based information extraction systems.}}
@ARTICLE{Dunne_2003,title={Two party immediate response disputes: properties and efficiency},year={2003},author={Paul E. Dunne and Paul E. Dunne and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1016/s0004-3702(03)00076-6},pmid={null},pmcid={null},mag_id={1969721969},journal={Artificial Intelligence},abstract={Two Party Immediate Response Disputes (TPI-disputes) are one class of dialogue or argument game in which the protagonists take turns producing counter arguments to the 'most recent' argument advanced by their opponent. Argument games have been found useful as a means of modelling dialectical discourse and in providing semantic bases for proof theoretic aspects of reasoning. In this article we consider a formalisation of TPI-disputes in the context of finite Argument Systems. Our principal concern may, informally, be phrased as follows: given a specific argument system, H, and argument x within H, what can be stated concerning the number of moves a dispute might take for one of its protagonists to accept that x has some defence respectively cannot be defended?}}
@ARTICLE{Tennent_1976,title={The denotational semantics of programming languages},year={1976},author={R. D. Tennent and R. D. Tennent},doi={10.1145/360303.360308},pmid={null},pmcid={null},mag_id={1969857501},journal={Communications of The ACM},abstract={This paper is a tutorial introduction to the theory of programming language semantics developed by D. Scott and C. Strachey. The application of the theory to formal language specification is demonstrated and other applications are surveyed. The first language considered, LOOP, is very elementary and its definition merely introduces the notation and methodology of the approach. Then the semantic concepts of environments, stores, and continuations are introduced to model classes of programming language features and the underlying mathematical theory of computation due to Scott is motivated and outlined. Finally, the paper presents a formal definition of the language GEDANKEN.}}
@ARTICLE{Gibiec_2010,title={Towards mining replacement queries for hard-to-retrieve traces},year={2010},author={Marek Gibiec and Marek Gibiec and Adam Czauderna and Adam Czauderna and Jane Cleland‐Huang and Jane Cleland-Huang},doi={10.1145/1858996.1859046},pmid={null},pmcid={null},mag_id={1970264769},journal={null},abstract={Automated trace retrieval methods can significantly reduce the cost and effort needed to create and maintain requirements traces. However, the set of generated traces is generally quite imprecise and must be manually evaluated by analysts. In applied settings when the retrieval algorithm is unable to find the relevant links for a given query, a human user can improve the trace results by manually adding additional search terms and filtering out unhelpful ones. However, the effectiveness of this approach is largely dependent upon the knowledge of the user. In this paper we present an automated technique for replacing the original query with a new set of query terms. These query terms are learned through seeding a web-based search with the original query and then processing the results to identify a set of domain-specific terms. The query-mining algorithm was evaluated and fine-tuned using security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications.}}
@ARTICLE{Palmirani_2003,title={Automated extraction of normative references in legal texts},year={2003},author={Monica Palmirani and Monica Palmirani and Raffaella Brighi and Raffaella Brighi and Matteo Massini and Matteo Massini},doi={10.1145/1047788.1047815},pmid={null},pmcid={null},mag_id={1972019838},journal={null},abstract={Italian Ministry of Justice, with the contributions of the researcher centres, universities and public bodies, are presently engaged in an effort to work out shared standards with which to represent legal texts. Documents standardised under uniform formats and structures make it possible to link up distinct bodies of norms, and this in turn makes it easier to find and look up norms and design tools with which to process them, as when doing legal drafting and bringing out consolidated texts. This function is enabled by marking up the different parts of a legal text: its identification data (indicating text type, text number, date of delivery, and the like), its partitions (e.g., the articles and sections that make up its layout), and the normative references it contains.}}
@ARTICLE{Mitchell_2009,title={A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development: A systematic review},year={2009},author={Susan M. Mitchell and Susan M. Mitchell and Carolyn Seaman and Carolyn Seaman},doi={10.1109/esem.2009.5314228},pmid={null},pmcid={null},mag_id={1972105418},journal={null},abstract={The objective of this study is to present a body of evidence that will assist software project managers to make informed choices about software development approaches for their projects. In particular, two broadly defined competing approaches, the traditional “waterfall” approach and iterative and incremental development (IID), are compared with regards to development cost and duration, and resulting product quality. The method used for this comparison is a systematic literature review. The small set of studies we located did not demonstrate any identifiable cost, duration, or quality trends, although there was some evidence suggesting the superiority of IID (in particular XP). The results of this review indicate that further empirical studies, both quantitative and qualitative, on this topic need to be undertaken. In order to effectively compare study results, the research community needs to reach a consensus on a set of comparable parameters that best assess cost, duration, and quality.}}
@ARTICLE{Chisholm_1963,title={Contrary-To-Duty Imperatives and Deontic Logic},year={1963},author={Roderick M. Chisholm and Roderick M. Chisholm},doi={10.1093/analys/24.2.33},pmid={null},pmcid={null},mag_id={1972568059},journal={Analysis},abstract={Journal Article Contrary-To-Duty Imperatives and Deontic Logic Get access Roderick M. Chisholm Roderick M. Chisholm Brown University Search for other works by this author on: Oxford Academic Google Scholar Analysis, Volume 24, Issue 2, December 1963, Pages 33–36, https://doi.org/10.1093/analys/24.2.33 Published: 01 December 1963}}
@ARTICLE{Branting_1993,title={A reduction-graph model of ratio decidendi},year={1993},author={L. Karl Branting and L. Karl Branting},doi={10.1145/158976.158981},pmid={null},pmcid={null},mag_id={1972709490},journal={null},abstract={This paper proposes a model of  ration decidendi  as a justification structure consisting of a series of reasoning steps, some of which relate abstract predicates to other abstract predicates and some of which relate abstract predicates to specific facts. This model satisfies four adequacy criteria for  ratio decidendi  identified from the jurisprudential literature. In particular, the model shows how the theory under which a case is decided controls its precedential effect. By contrast, a purely case-based model of  ratio  fails to account for the dependency of precedential effect on the theory of decision.}}
@ARTICLE{Jakobovits_1999,title={Dialectic semantics for argumentation frameworks},year={1999},author={Hadassa Jakobovits and Hadassa Jakobovits and Dirk Vermeir and Dirk Vermeir},doi={10.1145/323706.323715},pmid={null},pmcid={null},mag_id={1972753218},journal={null},abstract={We provide a formalism for the study of dialogues, where a dialogue is a two-person game, initiated by the proponent who defends a proposed thesis. We examine several different winning criteria and several different dialogue types, where a dialogue type is determined by a set of positions, an attack relation between positions and a legal-move function. We examine two proof theories, where a proof theory is determined by a dialogue type and a winning criterion. For each of the proof theories we supply a corresponding declarative semantics.}}
@ARTICLE{Governatori_2008,title={A computational framework for institutional agency},year={2008},author={Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo},doi={10.1007/s10506-007-9056-y},pmid={null},pmcid={null},mag_id={1973162402},journal={Artificial Intelligence and Law},abstract={This paper provides a computational framework, based on defeasible logic, to capture some aspects of institutional agency. Our background is Kanger-Lindahl-Porn account of organised interaction, which describes this interaction within a multi-modal logical setting. This work focuses in particular on the notions of counts-as link and on those of attempt and of personal and direct action to realise states of affairs. We show how standard defeasible logic (DL) can be extended to represent these concepts: the resulting system preserves some basic properties commonly attributed to them. In addition, the framework enjoys nice computational properties, as it turns out that the extension of any theory can be computed in time linear to the size of the theory itself.}}
@ARTICLE{Martinek_2005,title={Dynamics of legal provisions and its representation},year={2005},author={Jacek Martinek and Jacek Martinek and Jolanta Cybulka and Jolanta Cybulka},doi={10.1145/1165485.1165489},pmid={null},pmcid={null},mag_id={1973275031},journal={null},abstract={Legal provisions are considered as interrelated fragments of a text with some structural relations which hold between them. Some provisions are treated as meta-provisions, in case they are used to enact, repeal or amend the substantial provisions. The meaning of a meta-provision is described by meta-norms. Every meta-norm is conditioned by a certain event, and it describes the action which should be executed in order to obtain the current properties of the provision, such as its current text content or the current structural relations holding between the provisions. The presented model of the legal provisions dynamics is based on the event calculus, which is used to represent how the provision properties change in time. The model can be easily implemented in Prolog.}}
@ARTICLE{Thompson_2004,title={6.1.1 Compliance Mapping of Industry Standards to the Rockwell Collins' Technical Consistent Process Using a Requirements Database},year={2004},author={Anthony J. Thompson and Anthony J. Thompson and John F. Povacz and John F. Povacz and Richard A. Bennett and Richard A. Bennett},doi={10.1002/j.2334-5837.2004.tb00562.x},pmid={null},pmcid={null},mag_id={1973668980},journal={null},abstract={The growth in quantity, variety, and merging of industry standards, process capability models, regulatory requirements, and customer request has inundated many Original Equipment Manufactures' (OEMs) process groups. Customer requests for objective evidence corroborating capability and maturity of documented internal design and development processes are necessary to demonstrate compliance to these source requirements. This paper discusses Rockwell Collins' approach to manage these changes using a requirements database to assist projects in tailoring Enterprise standard design and development processes and processes assets to meet the customer requirements.}}
@ARTICLE{Hosmer_1989,title={Applied Logistic Regression},year={1989},author={David W. Hosmer and David W. Hosmer and David W. Hosmer and Stanley Lemeshow},doi={null},pmid={null},pmcid={null},mag_id={1973948212},journal={null},abstract={"A new edition of the definitive guide to logistic regression modeling for health science and other applicationsThis thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data. A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion. Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines"--}}
@ARTICLE{Berenbach_2010,title={Contract-based requirements engineering},year={2010},author={Brian Berenbach and Brian Berenbach and Ren‐Yi Lo and Ren-Yi Lo and Bill Sherman and Bill Sherman},doi={10.1109/relaw.2010.5625354},pmid={null},pmcid={null},mag_id={1974258029},journal={null},abstract={The authors have observed that traditional requirements engineering practices are inadequate to support large projects that are defined by a contract. Unlike a product development effort, contract requirements are not at a uniform level so the typical “V” model for tracing does not work well. Other important attributes of contracts such as penalty clauses, contract options, incentive payments, regulatory codes and standards, cross-cutting and project execution requirements all need to be considered. This paper describes some of the challenges and recommended best practices for the management of requirements on large contract-based projects as described from the perspective of the lead supplier or prime contractor.}}
@ARTICLE{Loureiro_2004,title={A systems engineering framework for integrated automotive development},year={2004},author={Geilson Loureiro and Geilson Loureiro and Paul Leaney and P.G. Leaney and Mike Hodgson and Mike Hodgson},doi={10.1002/sys.20001},pmid={null},pmcid={null},mag_id={1974300295},journal={Systems Engineering},abstract={Automotive development faces tightening regulatory requirements, shortening development cycle times, and growing complexity. To cope with such an environment, it is moving from a traditional evolutionary to a systems engineering approach. Great effort is being made for a shift from the traditional component focus, which has been enhanced by concurrent engineering, to a more broadened view supported by systems thinking. This broader view, however, is in practice strongly focused on the product elements of the system. This paper proposes a systems engineering framework for integrated automotive development—the total view approach. It is a modeling framework that integrates the product, its life cycle processes and their associated organizations throughout the requirements, functional and physical analysis processes, at all levels of the product breakdown structure, deriving attributes as emergent properties of a whole integrated system. The paper justifies the framework through a review of traditional and current automotive development and two case studies. A major benefit of the application of the framework is the ability to investigate early in the product development process the interactions between requirements and attributes not only of the product, but also of its life cycle processes and their associated organisations. This can lead to better product quality, lower life cycle cost, shorter development time, and manageable complexity. © 2004 Wiley Periodicals, Inc. Syst Eng 7: 153–166, 2004}}
@ARTICLE{Loui_1997,title={Progress on Room 5: a testbed for public interactive semi-formal legal argumentation},year={1997},author={Ronald P. Loui and Ronald P. Loui and Jeff Norman and Jeff Norman and Joe Altepeter and Joe Altepeter and Dan Pinkard and Dan Pinkard and Dan Craven and Dan Craven and Jessica Linsday and Jessica Linsday and Mark A. Foltz and Mark Foltz},doi={10.1145/261618.261655},pmid={null},pmcid={null},mag_id={1974572827},journal={null},abstract={null}}
@ARTICLE{Torkar_2012,title={REQUIREMENTS TRACEABILITY: A SYSTEMATIC REVIEW AND INDUSTRY CASE STUDY},year={2012},author={Richard Torkar and Richard Torkar and Tony Gorschek and Tony Gorschek and Robert Feldt and Robert Feldt and Mikael Svahnberg and Mikael Svahnberg and Uzair Akbar Raja and Uzair Akbar Raja and Kashif Kamran and Kashif Kamran},doi={10.1142/s021819401250009x},pmid={null},pmcid={null},mag_id={1975058921},journal={International Journal of Software Engineering and Knowledge Engineering},abstract={Requirements traceability enables software engineers to trace a requirement from its emergence to its fulfillment. In this paper we examine requirements traceability definitions, challenges, tools  ...}}
@ARTICLE{Sage_1998,title={Systems integration and architecting: An overview of principles, practices, and perspectives},year={1998},author={Andrew P. Sage and Andrew P. Sage and Charles L. Lynch and Charles L. Lynch},doi={10.1002/(sici)1520-6858(1998)1:3<176::aid-sys3>3.0.co;2-l},pmid={null},pmcid={null},mag_id={1975235610},journal={Systems Engineering},abstract={Systems Integration is an activity omnipresent in almost all of systems engineering and management. Often the term lacks precise definition and is used in different ways and for different purposes in the engineering of systems. This paper presents an overview of systems integration at all levels of system engineering and management. The goal of the paper is to present the various views of systems integration including lifecycle, architecture, process, interface, and enterprise, as well as product integration involving software and hardware. We examine systems engineering process, including several process models in an attempt to reveal the activities associated with integration during the engineering of systems. We review the characteristics of interfaces between systems of systems and users that generally give rise to many systems integration concerns. Without very careful effort at developing an appropriate architecture for a system, there will be little hope of integration. We discuss the role of systems management and knowledge integration in architecting for systems integration. Also presented are several contemporary perspectives on system architectures and their role in the engineering and integration of systems. © 1998 John Wiley & Sons, Inc. Syst Eng 1: 176–227, 1998}}
@ARTICLE{Yan_2006,title={Ontology Modeling for Contract: Using OWL to Express Semantic Relations},year={2006},author={Yunjun Yan and Yalan Yan and Yalan Yan and Jinlong Zhang and Jinlong Zhang and Mi Yan and Mi Yan},doi={10.1109/edoc.2006.37},pmid={null},pmcid={null},mag_id={1976659263},journal={null},abstract={There exist the generic concepts and semantic relationships in the description of procedural knowledge and strategic knowledge in contract management. In order that contract information could be understood and processed by machine automatically, more semantic descriptions are needed. Ontology has become a promising technology to express semantics. At present, Web ontology languages have been standardized. However, building domain ontology based on OWL (Ontology Web Language) just begins and has a long way to go. In building domain ontology, people should explore the rationality in expressing domain concepts and relations between the concepts, considering the capabilities of OWL (not the capability of natural languages). In this paper, we put forward a framework of contract ontology after considering capabilities to express semantic relations provided by OWL. Then we use Prot?g?-2000 to build OWL representations of the contract ontology and make some analyses. Our exploration might not standardize contract ontology. We explore with the intention to help people better understand the rationality in expressing concepts and relations between the concepts in contract management in order that OWL contract ontology and other domain ontology are improved and standardized as soon as possible.}}
@ARTICLE{Moens_2007,title={Automatic detection of arguments in legal texts},year={2007},author={Marie‐Francine Moens and Marie-Francine Moens and Erik Boiy and Erik Boiy and Raquel Mochales Palau and Raquel Mochales Palau and Chris Reed and Chris Reed},doi={10.1145/1276318.1276362},pmid={null},pmcid={null},mag_id={1977155386},journal={null},abstract={This paper provides the results of experiments on the detection of arguments in texts among which are legal texts. The detection is seen as a classification problem. A classifier is trained on a set of annotated arguments. Different feature sets are evaluated involving lexical, syntactic, semantic and discourse properties of the texts. The experiments are a first step in the context of automatically classifying arguments in legal texts according to their rhetorical type and their visualization for convenient access and search.}}
@ARTICLE{Mazzei_2009,title={NLP-based extraction of modificatory provisions semantics},year={2009},author={Alessandro Mazzei and Alessandro Mazzei and Daniele Paolo Radicioni and Daniele P. Radicioni and Raffaella Brighi and Raffaella Brighi},doi={10.1145/1568234.1568241},pmid={null},pmcid={null},mag_id={1978727198},journal={null},abstract={In this paper we illustrare a research based on NLP techniques aimed at automatically annotate modificatory provisions. We propose an approach which pairs deep syntactic parsing with rule-based shallow semantic analysis relying on a fine-grained taxonomy of modificatory provisions. The implemented system is evaluated on a large dataset hand-crafted by legal experts; the results are discussed and future directions of the research outlined.}}
@ARTICLE{Hamdaqa_2011,title={An approach based on citation analysis to support effective handling of regulatory compliance},year={2011},author={Mohammad Hamdaqa and Mohammad Hamdaqa and Abdelwahab Hamou-Lhadj and Abdelwahab Hamou-Lhadj},doi={10.1016/j.future.2010.09.007},pmid={null},pmcid={null},mag_id={1979838076},journal={Future Generation Computer Systems},abstract={null}}
@ARTICLE{Bobrow_1984,title={Qualitative reasoning about physical systems: An introduction},year={1984},author={Daniel G. Bobrow and Daniel G. Bobrow},doi={10.1016/0004-3702(84)90036-5},pmid={null},pmcid={null},mag_id={1979882473},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Buse_2008,title={A metric for software readability},year={2008},author={Raymond P.L. Buse and Raymond P.L. Buse and Westley Weimer and Westley Weimer},doi={10.1145/1390630.1390647},pmid={null},pmcid={null},mag_id={1981108270},journal={null},abstract={In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80% effective, and better than a human on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with two traditional measures of software quality, code changes and defect reports. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggests that comments, in of themselves, are less important than simple blank lines to local judgments of readability.}}
@ARTICLE{Jones_1985,title={Ideality, sub-ideality and deontic logic},year={1985},author={Andrew Jones and Andrew J. I. Jones and Ingmar Pörn and Ingmar Pörn},doi={10.1007/bf00869304},pmid={null},pmcid={null},mag_id={1981173442},journal={Synthese},abstract={null}}
@ARTICLE{O’Keefe_1993,title={Expert system verification and validation: a survey and tutorial},year={1993},author={Robert M. O’Keefe and Robert M. O'Keefe and Daniel E. O’Leary and Daniel E. O'Leary},doi={10.1007/bf00849196},pmid={null},pmcid={null},mag_id={1981311312},journal={Artificial Intelligence Review},abstract={Assuring the quality of an expert system is critical. A poor quality system may make costly errors resulting in considerable damage to the user or owner of the system, such as financial loss or human suffering. Hence verification and validation, methods and techniques aimed at ensuring quality, are fundamentally important.}}
@ARTICLE{Spinosa_2009,title={NLP-based metadata extraction for legal text consolidation},year={2009},author={Pierluigi Spinosa and PierLuigi Spinosa and PierLuigi Spinosa and Pierluigi Spinosa and Gerardo Giardiello and Gerardo Giardiello and Manola Cherubini and Manola Cherubini and Simone Marchi and Simone Marchi and Giulia Venturi and Giulia Venturi and Simonetta Montemagni and Simonetta Montemagni},doi={10.1145/1568234.1568240},pmid={null},pmcid={null},mag_id={1982020468},journal={null},abstract={The paper describes a system for the automatic consolidation of Italian legislative texts to be used as a support of an editorial consolidating activity and dealing with the following typology of textual amendments: repeal, substitution and integration. The focus of the paper is on the semantic analysis of the textual amendment provisions and the formalized representation of the amendments in terms of meta-data. The proposed approach to consolidation is metadata--oriented and based on Natural Language Processing (NLP) techniques: we use XML--based standards for metadata annotation of legislative acts and a flexible NLP architecture for extracting metadata from parsed texts. An evaluation of achieved results is also provided.}}
@ARTICLE{Adelman_1994,title={A multifaceted approach to evaluating expert systems},year={1994},author={Leonard Adelman and Leonard Adelman and James Gualtieri and James Gualtieri and Sharon L. Riedel and Sharon L. Riedel},doi={10.1017/s0890060400000974},pmid={null},pmcid={null},mag_id={1982024802},journal={Ai Edam Artificial Intelligence for Engineering Design, Analysis and Manufacturing},abstract={A multifaceted approach to evaluating expert systems is overviewed. This approach has three facets: a technical facet, for “looking inside the black box”; an empirical facet, for assessing the system’s impact on performance; and a subjective facet, for obtaining users’ judgments about the system. Such an approach is required to test the system against the different types of criteria of interest to sponsors and users and is consistent with evolving lifecycle paradigms. Moreover, such an approach leads to the application of different evaluation methods to answer different types of evaluation questions. Different evaluation methods for each facet are overviewed.}}
@ARTICLE{Prakken_2001,title={Modelling reasoning about evidence in legal procedure},year={2001},author={Henry Prakken and Henry Prakken},doi={10.1145/383535.383550},pmid={null},pmcid={null},mag_id={1982904830},journal={null},abstract={This article investigates the modelling of reasoning about evidence in legal procedure. To this end, a dialogue game model of the relevant parts of Dutch civil procedure is developed with three players: two adversaries and a judge. The model aims to be both legally realistic and technically well-founded. Legally, the main achievement is a more realistic account of the judge's role in legal procedures than that provided by current models. Technically, the model aims to preserve the features of an earlier-developed framework for two-player argumentative dialogue systems.}}
@ARTICLE{Tosatto_2015,title={Business Process Regulatory Compliance is Hard},year={2015},author={Silvano Colombo Tosatto and Silvano Colombo Tosatto and Guido Governatori and Guido Governatori and Pierre Kelsen and Pierre Kelsen},doi={10.1109/tsc.2014.2341236},pmid={null},pmcid={null},mag_id={1983339787},journal={IEEE Transactions on Services Computing},abstract={Verifying whether a business process is compliant with a regulatory framework is a difficult task. In the present paper we prove the hardness of the business process regulatory compliance problem by taking into account a sub-problem of the general problem. This limited problem allows to verify only the compliance of structured processes with respect to a regulatory framework composed of a set of conditional obligations including a deadline. Experimental evidence from existing studies shows that compliance is a difficult task. In this paper, despite considering a sub-problem of the general problem, we provide some theoretical evidence of the difficulty of the task. In particular we show that the source of the complexity lies in the core language of verifying conditional obligations with a deadline. We prove that for this simplified case verifying partial compliance belongs to the class of  NP -complete problems, and verifying full compliance belongs to the class of co NP -complete problems. Thus by proving the difficulty of a simplified compliance problem we prove that the general problem of verifying business process regulatory compliance is hard.}}
@ARTICLE{Ziegel_1996,title={A Probabilistic Analysis of the Sacco and Vanzetti Evidence},year={1996},author={Eric R. Ziegel and Joseph B. Kadane and Joseph B. Kadane and David A. Schum and David A. Schum},doi={null},pmid={null},pmcid={null},mag_id={1984161375},journal={null},abstract={Different Wine in an Old Bottle A Standpoint for Our Analysis of the Sacco and Vanzetti Evidence Chains of Reasoning from a Mass of Evidence Grading the Probative Force of the Sacco and Vanzetti Evidence Probabilistic Analyses: Issues and Methods Probabilistic Analyses: Judgments and Stories Probabilistic Analyses of Evidence in Various Disciplines Final Thoughts About Nicola Sacco and Bartolomeo Vanzetti Appendices References Index.}}
@ARTICLE{Rissland_1996,title={The synergistic application of CBR to IR},year={1996},author={Edwina L. Rissland and Edwina L. Rissland and Jody J. Daniels and Jody J. Daniels},doi={10.1007/bf00130694},pmid={null},pmcid={null},mag_id={1984502043},journal={Artificial Intelligence Review},abstract={In this paper we discuss a hybrid approach combining Case-Based Reasoning (CBR) and Information Retrieval (IR) for the retrieval of full-text documents. Our hybrid CBR-IR approach takes as input a standard symbolic representation of a problem case and retrieves texts of relevant cases from a document collection dramatically larger than the case base available to the CBR system. Our system works by first performing a standard HYPO-style CBR analysis and then using the texts associated with certain important classes of cases found in this analysis to “seed” a modified version of INQUERY's relevance feedback mechanism in order to generate a query composed of individual terms or pairs of terms. Our approach provides two benefits: it extends the reach of CBR (for retrieval purposes) to much larger corpora, and it enables the injection of knowledge-based techniques into traditional IR. We describe our CBR-IR approach and report on on-going experiments.}}
@ARTICLE{Tennyson_1988,title={Linking cognitive learning theory to instructional prescriptions},year={1988},author={Robert D. Tennyson and Robert D. Tennyson and Mariana Rasch and Mariana Rasch},doi={10.1007/bf00056222},pmid={null},pmcid={null},mag_id={1985314724},journal={Instructional Science},abstract={This article presents an initial model of instructional design which directly links cognitive learning theory and instructional prescriptions. The major constructs of the model include components of the memory system, allocation of learning times by objectives and specific instructional methods. The model proposes that instructional time be divided so as to reach the educational goals of improvements in higher-order learning and thinking as well as knowledge acquisition. Integrating current methods of instructional strategies provides the framework for bridging the gap between cognitive learning theory and instructional design.}}
@ARTICLE{Hartshorne_1934,title={Collected Papers of Charles Sanders Peirce},year={1934},author={Charles Hartshorne and Paul Weiss},doi={null},pmid={null},pmcid={null},mag_id={1986781756},journal={null},abstract={Charles Sanders Peirce (1839-1914) is best known as the founder of pragmatism - the philosophy that assesses the meaning of what we say by its practical consequences. His writings cover a wide range of subjects and his influence can be seen in ethics, aesthetics, symbolic logic, religion, epistemology and metaphysics, and also scientific topics. The greater part of Peirce's papers were unpublished during his lifetime and upon his death several hundred manuscripts were left to Harvard University. This eight-volume collection also includes Peirce's writings on general philosophy, logic, pragmatism, metaphysics, experimental science, scientific method and philosophy of mind, as well as reviews and correspondence. Paragraphs are numbered for easy reference and contents arranged by subject.}}
@ARTICLE{Elahi_2009,title={Modeling and analysis of security trade-offs - A goal oriented approach},year={2009},author={Golnaz Elahi and Golnaz Elahi and Eric Yu and Eric Yu},doi={10.1016/j.datak.2009.02.004},pmid={null},pmcid={null},mag_id={1987242689},journal={null},abstract={In designing software systems, security is typically only one design objective among many. It may compete with other objectives such as functionality, usability, and performance. Too often, security mechanisms such as firewalls, access control, or encryption are adopted without explicit recognition of competing design objectives and their origins in stakeholders' interests. Recently, there is increasing acknowledgement that security is ultimately about trade-offs. One can only aim for ''good enough'' security, given the competing demands from many parties. This paper investigates the criteria for a conceptual modeling technique for making security trade-offs. We examine how conceptual modeling can provide explicit and systematic support for modeling and analyzing security trade-offs. We examine several existing approaches for dealing with trade-offs and security trade-offs in particular. From analyzing the limitations of existing methods, we propose an extension to the i^* Framework for security trade-off analysis, taking advantage of its multi-agent and goal orientation. The method was applied to several case studies used to exemplify existing approaches. The resulting models developed using different approaches are compared.}}
@ARTICLE{Engers_2003,title={Improving legal quality: an application report},year={2003},author={Tom van Engers and Tom M. van Engers and Margherita Boekenoogen and Margherita Boekenoogen},doi={10.1145/1047788.1047844},pmid={null},pmcid={null},mag_id={1988575564},journal={null},abstract={Problems with legal quality will not only increase effort and costs of the law enforcement organisations, but also undermines the regulating power of the legislator. Unintended use or even abuse of the law may be the result. Governments therefore should improve their legal quality. The complexity of legislation however makes this task a hard one. The Dutch Tax and Customs Administration (DTCA in Dutch: Belastingdienst) has developed a method and supporting tools that support a systematic translation of (new) legislation into the DTCA's processes. This POWER-method and tools help to improve the quality of (new) legislation and codify the knowledge used in the translation processes in which legislation and regulations are transformed into procedures, computer programs and other designs. Thereby the time-to-market of the implementation of legislation will be reduced. In this article we explain some knowledge representation techniques that we use to improve legal quality. We will also show its application and give real-life examples of anomalies detected. In contrast to other knowledge modelling approaches the POWER-approach is focused on modelling legal sources rather than expert knowledge. Expert knowledge however is still needed to find the correct interpretations but also for efficiency reasons. Starting with representing the (legal) experts' knowledge (using scenarios) helps us to find the adequate scope (the legal sources to be analysed). Confronting the expert with differences between the model build out of the experts' knowledge and the ones we make out of the other knowledge sources (specifically the law) causes the legal experts to see things in a different light and has often led to changes in the law.}}
@ARTICLE{Jonsen_1978,title={Do No Harm},year={1978},author={Albert R. Jonsen and Albert R. Jonsen},doi={10.7326/0003-4819-88-6-827},pmid={666143},pmcid={null},mag_id={1988679281},journal={Annals of Internal Medicine},abstract={Abstract This essay considers at length some ethical, philosophical implications of the medical maxim, "Do no harm" (or "Primum non nocere"). It helps to designate the practice of medicine as a mor...}}
@ARTICLE{MacCormick_1998,title={Interpreting precedents : a comparative study},year={1998},author={D. N. MacCormick and D. Neil MacCormick and Neil MacCormick and Robert S. Summers and Robert S. Summers and Robert S. Summers},doi={10.4324/9781315251905},pmid={null},pmcid={null},mag_id={1989986473},journal={American Journal of Comparative Law},abstract={Contents: Introduction, D. Neil MacCormick and Robert S. Summers Precedent in the Federal Republic of Germany, Robert Alexy and Ralph Dreier Precedent in Finland, Aulis Aarnio Precedent in France, Michel Troper and Christophe Grzegorczyk Precedent in Italy, Massimo La Torre and Michele Taruffo Precedent in Norway, Svein Eng Precedent in Poland, Lech Morawski and Marek Zirk-Sadowski Precedent in Spain, Francisco J. Laporta and Alfonso Ruiz Miguel Precedent Interpretation in Sweden, Gunnar Bergholtz and Aleksander Peczenick Precedent in the United Kingdom, Zenon Bankowski, D. Neil MacCormick and Geoffrey Marshall Precedent in the United States (New York State), Robert S. Summers Precedent in European Community Law, John J. BarcelA^3 Institutional Factors Influencing Precedents, Michele Taruffo The Binding Force of Precedent, Aleksander Peczenick Rationales for Precedent, Zenon Bankowski, D. Neil MacCormick, Lech Morawski and Alfonso Ruiz Miguel What is Binding in a Precedent, Geoffrey Marshall Departures from Precedent, Svein Eng and Robert S. Summers Further General Reflections and Conclusions D. Neil MacCormick and Robert S. Summers.}}
@ARTICLE{Ashley_2009,title={Teaching a process model of legal argument with hypotheticals},year={2009},author={Kevin D. Ashley and Kevin D. Ashley},doi={10.1007/s10506-009-9083-y},pmid={null},pmcid={null},mag_id={1990089126},journal={Artificial Intelligence and Law},abstract={The research described here explores the idea of using Supreme Court oral arguments as pedagogical examples in first year classes to help students learn the role of hypothetical reasoning in law. The article presents examples of patterns of reasoning with hypotheticals in appellate legal argument and in the legal classroom and a process model of hypothetical reasoning that relates them to work in cognitive science and Artificial Intelligence. The process model describes the relationships between an advocate's proposed test for deciding a case or issue, the facts of the hypothetical and of the case to be decided, and the often conflicting legal principles and policies underlying the issue. The process model of hypothetical reasoning has been partially implemented in a computerized teaching environment, LARGO ("Legal ARgument Graph Observer") that helps students identify, analyze, and reflect on episodes of hypothetical reasoning in oral argument transcripts. Using LARGO, students reconstruct examples of hypothetical reasoning in the oral arguments by representing them in simple diagrams that focus students on the proposed test, the hypothetical challenge to the test, and the responses to the challenge. The program analyzes the diagrams and provides feedback to help students complete the diagrams and reflect on the significance of the hypothetical reasoning in the argument. The article reports the results of experiments evaluating instruction of first year law students at the University of Pittsburgh using the LARGO program as applied to Supreme Court personal jurisdiction cases. The learning results so far have been mixed. Instruction with LARGO has been shown to help law student volunteers with lower LSAT scores learn skills and knowledge regarding hypothetical reasoning better than a text-based approach, but not when the students were required to participate. On the other hand, the diagrams students produce with LARGO have been shown to have some diagnostic value, distinguishing among law students on the basis of LSAT scores, posttest performance, and years in law school. This lends support to the underlying model of hypothetical argument and suggests using LARGO as a pedagogically diagnostic tool.}}
@ARTICLE{Gómez-Pérez_2013,title={A Formalism and Method for Representing and Reasoning with Process Models Authored by Subject Matter Experts},year={2013},author={José Manuél Gómez-Pérez and Jose Manuel Gomez-Perez and Michael Erdmann and Michael Erdmann and Mark Greaves and Mark Greaves and Óscar Corcho and Oscar Corcho},doi={10.1109/tkde.2012.127},pmid={null},pmcid={null},mag_id={1991525237},journal={IEEE Transactions on Knowledge and Data Engineering},abstract={Enabling Subject Matter Experts (SMEs) to formulate knowledge without the intervention of Knowledge Engineers (KEs) requires providing SMEs with methods and tools that abstract the underlying knowledge representation and allow them to focus on modeling activities. Bridging the gap between SME-authored models and their representation is challenging, especially in the case of complex knowledge types like processes, where aspects like frame management, data, and control flow need to be addressed. In this paper, we describe how SME-authored process models can be provided with an operational semantics and grounded in a knowledge representation language like F-logic to support process-related reasoning. The main results of this work include a formalism for process representation and a mechanism for automatically translating process diagrams into executable code following such formalism. From all the process models authored by SMEs during evaluation 82 percent were well formed, all of which executed correctly. Additionally, the two optimizations applied to the code generation mechanism produced a performance improvement at reasoning time of 25 and 30 percent with respect to the base case, respectively.}}
@ARTICLE{Schmidt_2012,title={Assessing identification of compliance requirements from privacy policies},year={2012},author={Jéssica Schmidt and Jessica Young Schmidt and Annie I. Antón and Annie I. Antón and Julia B. Earp and Julia B. Earp},doi={10.1109/relaw.2012.6347806},pmid={null},pmcid={null},mag_id={1991816117},journal={null},abstract={In the United States, organizations can be held liable by the Federal Trade Commission for the statements they make in their privacy policies. Thus, organizations must include their privacy policies as a source of requirements in order to build systems that are policy-compliant. In this paper, we describe an empirical user study in which we measure the ability of requirements engineers to effectively extract compliance requirements from a privacy policy using one of three analysis approaches—CPR (commitment, privilege, and right) analysis, goal-based analysis, and non-methodassisted (control) analysis. The results of these three approaches were then compared to an expert-produced set of expected compliance requirements. The requirements extracted by the CPR subjects reflected a higher percentage of requirements that were expected compliance requirements as well as a higher percentage of the total expected compliance requirements. In contrast, the goal-based and control subjects produced a higher number of synthesized requirements, or requirements not directly derived from the policy than the CPR subjects. This larger number of synthesized requirements may be attributed to the fact that these two subject groups employed more inquiry-driven approaches than the CPR subjects who relied primarily on focused and direct extraction of compliance requirements.}}
@ARTICLE{Suzuki_2005,title={Additive consolidation for dialogue game},year={2005},author={Yoshitaka Suzuki and Yoshitaka Suzuki and Satoshi Tojo and Satoshi Tojo},doi={10.1145/1165485.1165502},pmid={null},pmcid={null},mag_id={1992132356},journal={null},abstract={Our purpose is to propose postulates about operators which capture dynamic aspects on a legal argumentation, and to construct operators which satisfy the postulates and expand plaintiff and defendant's theories coherently. These ideas are the generalizations of Olsson's additive consolidation. Consolidation is studied in the field of belief revision, and makes an agent's epistemic state coherent. Therefore, our presentation will build a bridge between legal reasoning and belief revision.}}
@ARTICLE{Prakken_2012,title={Reconstructing Popov v. Hayashi in a framework for argumentation with structured arguments and Dungean semantics},year={2012},author={Henry Prakken and Henry Prakken},doi={10.1007/s10506-012-9117-8},pmid={null},pmcid={null},mag_id={1992259715},journal={Artificial Intelligence and Law},abstract={In this article the argumentation structure of the court's decision in the Popov v. Hayashi case is formalised in Prakken's (Argument Comput 1:93---124; 2010) abstract framework for argument-based inference with structured arguments. In this framework, arguments are inference trees formed by applying two kinds of inference rules, strict and defeasible rules. Arguments can be attacked in three ways: attacking a premise, attacking a conclusion and attacking an inference. To resolve such conflicts, preferences may be used, which leads to three corresponding kinds of defeat, after which Dung's (Artif Intell 77:321---357; 1995) abstract acceptability semantics can be used to evaluate the arguments. In the present paper the abstract framework is instantiated with strict inference rules corresponding to first-order logic and with defeasible inference rules for defeasible modus ponens and various argument schemes. The main techniques used in the formal reconstruction of the case are rule-exception structures and arguments about rule validity. Arguments about socio-legal values and the use of precedent cases are reduced to arguments about rule validity. The tree structure of arguments, with explicit subargument relations between arguments, is used to capture the dependency relations between the elements of the court's decision.}}
@ARTICLE{Belzer_1987,title={Legal reasoning in 3-D},year={1987},author={Marvin Belzer and M. Belzer},doi={10.1145/41735.41753},pmid={null},pmcid={null},mag_id={1993308657},journal={null},abstract={This article contains a theory of normative defeasible reasoning based on the modal deontic logic 3-D. The concept of “relative weight” between competing norms is defined, and 3-D is used to formalize two types of legal reasoning (“subsumptive” and “means/end”). A general overview is given of a PROLOG program, 3dpr, that implements the 3-D based theory of normative reasoning.}}
@ARTICLE{Branting_1997,title={Automated drafting of self-explaining documents},year={1997},author={L. Karl Branting and L. Karl Branting and James C. Lester and James C. Lester and Charles Callaway and Charles B. Callaway},doi={10.1145/261618.261635},pmid={null},pmcid={null},mag_id={1995188255},journal={null},abstract={null}}
@ARTICLE{Falessi_2010,title={A comprehensive characterization of NLP techniques for identifying equivalent requirements},year={2010},author={Davide Falessi and Davide Falessi and Giovanni Cantone and Giovanni Cantone and Gerardo Canfora and Gerardo Canfora},doi={10.1145/1852786.1852810},pmid={null},pmcid={null},mag_id={1995444282},journal={null},abstract={Though very important in software engineering, linking artifacts of the same type (clone detection) or of different types (traceability recovery) is extremely tedious, error-prone and requires significant effort. Past research focused on supporting analysts with mechanisms based on Natural Language Processing (NLP) to identify candidate links. Because a plethora of NLP techniques exists, and their performances vary among contexts, it is important to characterize them according to the provided level of support. The aim of this paper is to characterize a comprehensive set of NLP techniques according to the provided level of support to human analysts in detecting equivalent requirements. The characterization consists on a case study, featuring real requirements, in the context of an Italian company in the defense and aerospace domain. The major result from the case study is that simple NLP are more precise than complex ones.}}
@ARTICLE{Tawhid_2012,title={Towards outcome-based regulatory compliance in aviation security},year={2012},author={Rasha Tawhid and Rasha Tawhid and Edna Braun and Edna Braun and Nick Cartwright and Nick Cartwright and Mohammad Alhaj and Mohammad Alhaj and Gunter Mussbacher and Gunter Mussbacher and Azalia Shamsaei and Azalia Shamsaei and Daniel Amyot and Daniel Amyot and Saeed Ahmadi Behnam and Saeed Ahmadi Behnam and Gregory Richards and Greg Richards},doi={10.1109/re.2012.6345813},pmid={null},pmcid={null},mag_id={1996677066},journal={null},abstract={Transport Canada is reviewing its Aviation Security regulations in a multi-year modernization process. As part of this review, consideration is given to transitioning regulations where appropriate from a prescriptive style to an outcome-based style. This raises new technical and cultural challenges related to how to measure compliance. This paper reports on a novel approach used to model regulations with the Goal-oriented Requirement Language, augmented with qualitative indicators. These models are used to guide the generation of questions for inspection activities, enable a flexible conversion of real-world data into goal satisfaction levels, and facilitate compliance analysis. A new propagation mechanism enables the evaluation of the compliance level of an organization. This outcome-based approach is expected to help get a more precise understanding of who complies with what, while highlighting opportunities for improving existing regulatory elements.}}
@ARTICLE{Nair_2014,title={An extended systematic literature review on provision of evidence for safety certification},year={2014},author={Sunil Nair and Sunil Nair and José Luis de la Vara and Jose Luis de la Vara and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand},doi={10.1016/j.infsof.2014.03.001},pmid={null},pmcid={null},mag_id={1997717271},journal={Information & Software Technology},abstract={Abstract   Context  Critical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing undue risks to the user, the public, or the environment. Safety is typically ensured via complying with safety standards. Demonstrating compliance to these standards involves providing evidence to show that the safety criteria of the standards are met.    Objective  In order to cope with the complexity of large critical systems and subsequently the plethora of evidence information required for achieving compliance, safety professionals need in-depth knowledge to assist them in classifying different types of evidence, and in structuring and assessing the evidence. This paper is a step towards developing such a body of knowledge that is derived from a large-scale empirically rigorous literature review.    Method  We use a Systematic Literature Review (SLR) as the basis for our work. The SLR builds on 218 peer-reviewed studies, selected through a multi-stage process, from 4963 studies published between 1990 and 2012.    Results  We develop a taxonomy that classifies the information and artefacts considered as evidence for safety. We review the existing techniques for safety evidence structuring and assessment, and further study the relevant challenges that have been the target of investigation in the academic literature. We analyse commonalities in the results among different application domains and discuss implications of the results for both research and practice.    Conclusion  The paper is, to our knowledge, the largest existing study on the topic of safety evidence. The results are particularly relevant to practitioners seeking a better grasp on evidence requirements as well as to researchers in the area of system safety. As a major finding of the review, the results strongly suggest the need for more practitioner-oriented and industry-driven empirical studies in the area of safety certification.}}
@ARTICLE{Midgley_1992,title={The sacred and profane in critical systems thinking},year={1992},author={Gerald Midgley and Gerald Midgley},doi={10.1007/bf01060044},pmid={null},pmcid={null},mag_id={1997848598},journal={null},abstract={This paper looks at what we mean by being critical about systems. In particular, it seeks to expand our understanding of the process of making boundary judgments so as to explore the relationship these judgments have with values and ethics.}}
@ARTICLE{Schauer_1991,title={Playing by the Rules: A Philosophical Examination of Rule-Based Decision-Making in Law and in Life},year={1991},author={Frederick Schauer and Frederick Schauer},doi={null},pmid={null},pmcid={null},mag_id={1997926766},journal={null},abstract={This is a philosophical but non-technical analysis of the very idea of a rule. Although focused somewhat on the role of rules in the legal system, it is also relevant to the place of rules in morality, religion, etiquette, games, language, and family governance. In both explaining the idea of a rule and making the case for taking rules seriously, the book is a departure both in scope and in perspective from anything that now exists.}}
@ARTICLE{Guitton_2012,title={The immersive impact of meta-media in a virtual world},year={2012},author={Matthieu J. Guitton and Matthieu J. Guitton},doi={10.1016/j.chb.2011.10.016},pmid={null},pmcid={null},mag_id={1998013269},journal={Computers in Human Behavior},abstract={null}}
@ARTICLE{Schlobohm_1989,title={EPS II: estate planning with prototypes},year={1989},author={D. A. Schlobohm and D. A. Schlobohm and L. Thorne McCarty and L. T. McCarty},doi={10.1145/74014.74015},pmid={null},pmcid={null},mag_id={1998280593},journal={null},abstract={Article Free Access Share on EPS II: estate planning with prototypes Authors: D. A. Schlobohm Stanford Law School and Greene, Radovsky, Maloney and Share Stanford Law School and Greene, Radovsky, Maloney and ShareView Profile , L. T. McCarty Computer Science Department and Faculty of Law, Rutgers University Computer Science Department and Faculty of Law, Rutgers UniversityView Profile Authors Info & Claims ICAIL '89: Proceedings of the 2nd international conference on Artificial intelligence and lawMay 1989 Pages 1–10https://doi.org/10.1145/74014.74015Published:01 May 1989Publication History 11citation289DownloadsMetricsTotal Citations11Total Downloads289Last 12 Months30Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{McCarty_1991,title={On the role of prototypes in appellate legal argument (abstract)},year={1991},author={L. Thorne McCarty and L. Thorne McCarty},doi={10.1145/112646.112668},pmid={null},pmcid={null},mag_id={1998756099},journal={null},abstract={Although most of the work on Artificial Intelligence and Law today is oriented towards the development of pract ical systems, there is a small group of researchers who are primarily interested in theoretical questions: How much of legal reasoning can be reduced to reasoning wit h rules? Is this rule-based component significant, or trivial? How is it possible to rewon with cases at all? Are legal concepts just like ordinary common sense concepts, or do they have special characteristics? Is it possible to develop a computational theory of legal argument? The researchers who have investigated these questions include: Anne Gardner [1 I]; Edwina Rissland and her students, Kevin Ashley [38, 1] and David Skalak [39, 41]; Michael Dyer and his student, Seth Goldman [12]; Karl Branting [5]; and Keith Bellairs [2], In addition, researchers such as Richard Susskind [45] and J .C. Smith [43], who have primarily built practical systems, have also been deeply concerned with the jurisprudential foundations of the field.}}
@ARTICLE{Keppens_2007,title={Towards qualitative approaches to Bayesian evidential reasoning},year={2007},author={Jeroen Keppens and Jeroen Keppens},doi={10.1145/1276318.1276322},pmid={null},pmcid={null},mag_id={1998797187},journal={null},abstract={A crucial aspect of evidential reasoning in crime investigation involves comparing the support that evidence provides for alternative hypotheses. Recent work in forensic statistics has shown how Bayesian Networks (BNs) can be employed for this purpose. However, the specification of BNs requires conditional probability tables describing the uncertain processes under evaluation. When these processes are poorly understood, it is necessary to rely on subjective probabilities provided by experts. Accurate probabilities of this type are normally hard to acquire from experts. Recent work in qualitative reasoning has developed methods to perform probabilistic reasoning using coarser representations. However, the latter types of approaches are too imprecise to compare the likelihood of alternative hypotheses. This paper examines this shortcoming of the qualitative approaches when applied to the aforementioned problem, and identifies and integrates techniques to refine them.}}
@ARTICLE{Breaux_2009,title={Identifying vulnerabilities and critical requirements using criminal court proceedings},year={2009},author={Travis D. Breaux and Travis D. Breaux and J. D. Lewis and Jonathan D. Lewis and Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1145/1529282.1529360},pmid={null},pmcid={null},mag_id={2000070919},journal={null},abstract={Information systems governed by laws and regulations are subject to civil and criminal violations. In the United States, these violations are documented in court records, such as complaints, indictments, plea agreements, and verdicts, which thus constitute a source of real-world software vulnerabilities. This paper reports on an exploratory case study to identify legal vulnerabilities and provides guidance to practitioners in the analysis of court documents. As legal violations occur after system deployment, court records reveal vulnerabilities that were likely overlooked during software development. We evaluate established requirements engineering techniques, including sequence and misuse case diagrams and goal models, as applied to criminal court records to identify mitigating requirements that improve privacy protections. These techniques, when properly applied, can help organizations focus their risk-management efforts on emerging legal vulnerabilities. We illustrate our analysis using criminal indictments involving the U.S. Health Insurance Portability and Accountability Act (HIPAA).}}
@ARTICLE{Sherman_1989,title={Expert systems and ICAI in tax law: killing two birds with one AI stone},year={1989},author={David M. Sherman and D. M. Sherman},doi={10.1145/74014.74025},pmid={null},pmcid={null},mag_id={2000518468},journal={null},abstract={The author describes five separate projects he has undertaken in the intersection of computer science and Canadian income tax law. They are:   A computer-assisted instruction (CAI) course for teaching income tax, programmed using conventional CAI techniques;    A “document modeling” computer program for generating the documentation for a tax-based transaction and advising the lawyer-user as to what decisions should be made and what the tax effects will be, programmed in a conventional language;    A prototype expert system for determining the income tax effects of transactions and tax-defined relationships, based on a PROLOG representation of the rules of the Income Tax Act;    An intelligent CAI (ICAI) system for generating infinite numbers of randomized quiz questions for students, computing the answers, and matching wrong answers to particular student errors, based on a PROLOG representation of the rules of the Income Tax Act; and    A Hypercard stack for providing information about income tax, enabling both education and practical research to follow the user's needs path.     The author shows that non-AI approaches are a way to produce packages quickly and efficiently. Their primary disadvantage is the massive rewriting required when the tax law changes. AI approaches based on PROLOG, on the other hand, are harder to develop to a practical level but will be easier to audit and maintain. The relationship between expert systems and CAI is discussed.}}
@ARTICLE{Grover_2003,title={Automatic summarisation of legal documents},year={2003},author={Claire Grover and Claire Grover and Ben Hachey and Ben Hachey and Ian Hughson and Ian Hughson and Chris Korycinski and Chris Korycinski},doi={10.1145/1047788.1047839},pmid={null},pmcid={null},mag_id={2001212975},journal={null},abstract={We report on the SUM project which applies automatic summarisation techniques to the legal domain. We describe our methodology whereby sentences from the text are classified according to their rhetorical role in order that particular types of sentence can be extracted to form a summary. We describe some experiments with judgments of the House of Lords: we have performed automatic linguistic annotation of a small sample set and then hand-annotated the sentences in the set in order to explore the relationship between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to perform the linguistic annotation using XML-based tools and a combination of rule-based and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classifier.}}
@ARTICLE{Ashley_2013,title={Toward constructing evidence-based legal arguments using legal decision documents and machine learning},year={2013},author={Kevin D. Ashley and Kevin D. Ashley and Vern R. Walker and Vern R. Walker},doi={10.1145/2514601.2514622},pmid={null},pmcid={null},mag_id={2001800999},journal={null},abstract={This paper explores how to extract argumentation-relevant information automatically from a corpus of legal decision documents, and how to build new arguments using that information. For decision texts, we use the Vaccine/Injury Project (V/IP) Corpus, which contains default-logic annotations of argument structure. We supplement this with presuppositional annotations about entities, events, and relations that play important roles in argumentation, and about the level of confidence that arguments would be successful. We then propose how to integrate these semantic-pragmatic annotations with syntactic and domain-general semantic annotations, such as those generated in the DeepQA architecture, and outline how to apply machine learning and scoring techniques similar to those used in the IBM Watson system for playing the Jeopardy! question-answer game. We replace this game-playing goal, however, with the goal of learning to construct legal arguments.}}
@ARTICLE{Bing_1987,title={Designing text retrieval systems for conceptual searching},year={1987},author={Jon Bing and J. Bing},doi={10.1145/41735.41741},pmid={null},pmcid={null},mag_id={2002527263},journal={null},abstract={Article Free Access Share on Designing text retrieval systems for conceptual searching Author: J. Bing Univ. of Oslo, Norway Univ. of Oslo, NorwayView Profile Authors Info & Claims ICAIL '87: Proceedings of the 1st international conference on Artificial intelligence and lawDecember 1987 Pages 43–51https://doi.org/10.1145/41735.41741Online:01 December 1987Publication History 23citation389DownloadsMetricsTotal Citations23Total Downloads389Last 12 Months8Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Loui_1998,title={Process and Policy: Resource-Bounded NonDemonstrative Reasoning},year={1998},author={Ronald P. Loui and Ronald P. Loui},doi={10.1111/0824-7935.00055},pmid={null},pmcid={null},mag_id={2002848380},journal={null},abstract={This paper investigates the appropriateness of formal dialectics as a basis for nonmonotonic reasoning and defeasible reasoning that takes computational limits seriously. Rules that can come into conflict should be regarded as policies, which are inputs to deliberative processes. Dialectical protocols are appropriate for such deliberations when resources are bounded and search is serial.



AI, it is claimed here, is now perfectly positioned to correct many misconceptions about reasoning that have resulted from mathematical logic's enormous success in this century: among them, (1) that all reasons are demonstrative, (2) that rational belief is constrained, not constructed, and (3) that process and disputation are not essential to reasoning. AI mainly provides new impetus to formalize the alternative (but older) conception of reasoning, and AI provides mechanisms with which to create compelling formalism that describes the control of processes.



The technical contributions here are: the partial justification of dialectic based on controlling search; the observation that nonmonotonic reasoning can be subsumed under certain kinds of dialectics; the portrayal of inference in knowledge bases as policy reasoning; the review of logics of dialogue and proposed extensions; and the preformal and initial formal discussion of aspects and variations of dialectical systems with nondemonstrative reasons.}}
@ARTICLE{Peczenik_2000,title={Legal Knowledge about What},year={2000},author={Aleksander Peczenik and Aleksander Peczenik and Jaap Hage and Jaap Hage},doi={10.1111/1467-9337.00159},pmid={null},pmcid={null},mag_id={2003323143},journal={Ratio Juris},abstract={We assume, in contrast to many legal realists, that law is a part of reality. Law exists because people believe in law, but law is not identical with beliefs. Law supervenes on human beliefs, preferences, actions, dispositions and artefacts. Moreover, the morally binding personal interpretation of the law supervenes on two things together: on the individual?s knowledge of legal institutions and on moral obligation. The first supervenes in its turn on mutual beliefs; the second supervenes on motivations and dispositions of the individual, provided that she is morally sensitive and rational. Personal interpretation of law converts into social law, if other persons on the basis of overriding reasons do not contest it. Morally binding social law supervenes on moral motivation that is triggered by institutions that supervene on mutual beliefs.}}
@ARTICLE{Antoniou_2004,title={Defeasible logic with dynamic priorities},year={2004},author={Grigoris Antoniou},doi={10.1002/int.v19:5},pmid={null},pmcid={null},mag_id={2003399686},journal={Journal of intelligent systems},abstract={Defeasible logic is a nonmonotonic reasoning approach based on rules and priorities. Its design supports efficient implementation, and it shows promise to be deployed successfully in applications. So far, only static priorities have been used, provided by an external superiority relation. In this article we show how dynamic priorities can be integrated, where priority information is obtained from the deductive process itself. Dynamic priorities have been studied for other related reasoning systems such as default logic and argumentation. We define a proof theory, study its formal properties, and provide an argumentation semantics. © 2004 Wiley Periodicals, Inc.}}
@ARTICLE{Buckland_1952,title={Roman Law and Common Law: A Comparison in Outline},year={1952},author={William Buckland and W. W. Buckland and Arnold D. McNair and Arnold D. McNair},doi={null},pmid={null},pmcid={null},mag_id={2004280548},journal={null},abstract={1. The sources 2. The Law of Persons 3. Law of Property 4. Limited Interests and Servitudes 5. Universal Succession 6. Obligations: General 7. Obligations: General (cont.) 8. Particular Contracts 9. Quasi-contract and Negotiorum Gestio 10. Delict and Tort 11. Perticular dialects and Torts 12. Procedure.}}
@ARTICLE{Armitage_1998,title={Implementing a compliance manager},year={1998},author={Stephen Armitage and Stephen Armitage and Richard Stevens and Richard Stevens and Anthony Finkelstein and Anthony Finkelstein},doi={10.1007/bf02919969},pmid={null},pmcid={null},mag_id={2005881298},journal={Requirements Engineering},abstract={Many companies claim to adhere to standards for software project development. This is often used as a marketing tool when eliciting business. But how does the customer or project manager know that these standards are being completely and consistently applied in their projects? In the paper `Managing standards compliance' [http://www.cs.ucl.ac.uk/staff/w.emmerich/publications/TSE/stdscompl.html] we identify this problem and describe a support environment to provide identification and correction of non-compliance to standards. This paper details the experiences gained while implementing such a tolerant support system.}}
@ARTICLE{Conrad_2001,title={A cognitive approach to judicial opinion structure: applying domain expertise to component analysis},year={2001},author={Jack G. Conrad and Jack G. Conrad and Daniel P. Da bney and Daniel P. Da bney},doi={10.1145/383535.383536},pmid={null},pmcid={null},mag_id={2006013414},journal={null},abstract={Empirical research on basic components of American judicial opinions has only scratched the surface. Lack of a coordinated pool of legal experts or adequate computational resources are but two reasons responsible for this deficiency. We have undertaken a study to uncover fundamental components of judicial opinions found in American case law. The study was aided by a team of twelve expert attorney-editors with a combined total of 135 years of legal editing experience. The scientific hypothesis underlying the experiment was that after years of working closely with thousands of judicial opinions, expert attorneys would develop a refined and internalized schema of the content and structure of legal cases. In this study participants were permitted to describe both concept-related and format-related components. The resultant components, representing a combination of these two broad categories, are reported on in this paper. Additional experiments are currently under way which further validated and refine this set of components and apply them to new search paradigms.}}
@ARTICLE{Chen_2010,title={An algorithm to compute similarity between danger objects based on ontology for danger-aware systems},year={2010},author={Yongping Chen and Yongping Chen and Junbo Wang and Junbo Wang and Zixue Cheng and Zixue Cheng and Jing Lei and Lei Jing and Yinghui Zhou and Yinghui Zhou},doi={10.1109/isac.2010.5670467},pmid={null},pmcid={null},mag_id={2006760771},journal={null},abstract={Context awareness is one of the most important aspects in the current ubiquitous computing research field. Danger-aware system is a kind of context awareness system, which detects the dangerous situations and provides services responsive to the dangerous situation. However, there is a shortcoming in the current danger/context awareness systems that the reasoning mechanisms are not flexible. Without enough predefined contexts, the system cannot work well, e.g., if a detected context is similar to a predefined context, but not exactly same as the predefined one, the system cannot recognize it.}}
@ARTICLE{Davis_2003,title={The art of requirements triage},year={2003},author={Alan M. Davis and Alan M. Davis},doi={10.1109/mc.2003.1185216},pmid={null},pmcid={null},mag_id={2007642483},journal={IEEE Computer},abstract={Driven by an increasingly competitive market, companies add features and compress schedules for the delivery of every product, often creating a complete mismatch of requirements and resources that results in products failing to satisfy customer needs. Triage is the process of determining which requirements a product should satisfy given the time and resources available. The author presents three product development case studies and 14 recommendations for practicing this neglected art.}}
@ARTICLE{Kagdi_2007,title={A survey and taxonomy of approaches for mining software repositories in the context of software evolution},year={2007},author={Huzefa Kagdi and Huzefa Kagdi and Michael L. Collard and Michael L. Collard and Jonathan I. Maletic and Jonathan I. Maletic},doi={10.1002/smr.344},pmid={null},pmcid={null},mag_id={2008164297},journal={Journal of Software Maintenance and Evolution: Research and Practice},abstract={A comprehensive literature survey on approaches for mining software repositories (MSR) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of MSR investigations) and effective (i.e., facilitates similarities and comparisons of MSR investigations). Lastly, a number of open research issues in MSR that require further investigation are identified.}}
@ARTICLE{Godau_1999,title={The changing face of infrastructure management},year={1999},author={Ralph I. Godau and Ralph I. Godau},doi={10.1002/(sici)1520-6858(1999)2:4<226::aid-sys5>3.0.co;2-8},pmid={null},pmcid={null},mag_id={2008739041},journal={Systems Engineering},abstract={Infrastructure management refers to the ongoing body of knowledge associated with the management of economic infrastructure such as power, water, sewerage, gas, communications and transportation systems, and social infrastructure such as hospitals, schools, etc. The purpose of this paper is to provide an insight into the way infrastructure management is changing as a result of the increasing complexity caused by technical, economic, managerial, environmental, political, and social factors. Due to this complexity, traditional engineering management approaches for infrastructure systems are no longer effective in meeting the needs of the stakeholders, including the public at large. This paper proposes that the new direction for infrastructure management should be driven by the body of knowledge associated with development of systems and the management of existing systems (i.e., Systems Engineering/Asset Management). It begins by examining the issues that are important when developing any new engineering management system. It introduces the concept of systems thinking and suggests how a holistic vision can provide an insight into the significant relationships that need to be considered when developing management approaches for infrastructure. It discusses evolving engineering management approaches and how they relate to the changing environment. The paper concludes by suggesting that the adoption of a holistic approach to infrastructure management is the key to managing the increasingly complex interrelationships and will overcome many of the deficiencies in the traditional approaches. © 1999 John Wiley & Sons, Inc. Syst Eng 2: 226–236, 1999}}
@ARTICLE{Bex_2010,title={A hybrid formal theory of arguments, stories and criminal evidence},year={2010},author={Floris Bex and Floris Bex and P.J. van Koppen and Peter J. van Koppen and Henry Prakken and Henry Prakken and Bart Verheij and Bart Verheij},doi={10.1007/s10506-010-9092-x},pmid={null},pmcid={null},mag_id={2010366788},journal={Artificial Intelligence and Law},abstract={This paper presents a theory of reasoning with evidence in order to determine the facts in a criminal case. The focus is on the process of proof, in which the facts of the case are determined, rather than on related legal issues, such as the admissibility of evidence. In the literature, two approaches to reasoning with evidence can be distinguished, one argument-based and one story-based. In an argument-based approach to reasoning with evidence, the reasons for and against the occurrence of an event, e.g., based on witness testimony, are central. In a story-based approach, evidence is evaluated and interpreted from the perspective of the factual stories as they may have occurred in a case, e.g., as they are defended by the prosecution. In this paper, we argue that both arguments and narratives are relevant and useful in the reasoning with and interpretation of evidence. Therefore, a hybrid-approach is proposed and formally developed, doing justice to both the argument-based and the narrative-based perspective. By the formalization of the theory and the associated graphical representations, our proposal is the basis for the design of software developed as a tool to make sense of the evidence in complex cases.}}
@ARTICLE{Rittel_1973,title={Dilemmas in a general theory of planning},year={1973},author={Horst W.J. Rittel and Horst W. J. Rittel and Melvin M. Webber and Melvin M. Webber},doi={10.1007/bf01405730},pmid={null},pmcid={null},mag_id={2010793828},journal={Policy Sciences},abstract={The search for scientific bases for confronting problems of social policy is bound to fail, becuase of the nature of these problems. They are “wicked” problems, whereas science has developed to deal with “tame” problems. Policy problems cannot be definitively described. Moreover, in a pluralistic society there is nothing like the undisputable public good; there is no objective definition of equity; policies that respond to social problems cannot be meaningfully correct or false; and it makes no sense to talk about “optimal solutions” to social problems unless severe qualifications are imposed first. Even worse, there are no “solutions” in the sense of definitive and objective answers.}}
@ARTICLE{Garey_1979,title={Computers and Intractability: A Guide to the Theory of NP-Completeness},year={1979},author={M. R. Garey and Michael Randolph Garey and David S. Johnson and David S. Johnson and David S. Johnson and David S. Johnson},doi={null},pmid={null},pmcid={null},mag_id={2011039300},journal={null},abstract={null}}
@ARTICLE{Weber_2005,title={Textual case-based reasoning},year={2005},author={Rosina O. Weber and Rosina O. Weber and Kevin D. Ashley and Kevin D. Ashley and Stefanie Brüninghaus and Stefanie Brüninghaus},doi={10.1017/s0269888906000713},pmid={null},pmcid={null},mag_id={2012239333},journal={Knowledge Engineering Review},abstract={This commentary provides a definition of textual case-based reasoning (TCBR) and surveys research contributions according to four research questions. We also describe how TCBR can be distinguished from text mining and information retrieval. We conclude with potential directions for TCBR research.}}
@ARTICLE{Amaya_2007,title={Formal Models of Coherence and Legal Epistemology},year={2007},author={Amalia Amaya and Amalia Amaya},doi={10.1007/s10506-007-9050-4},pmid={null},pmcid={null},mag_id={2013019369},journal={Artificial Intelligence and Law},abstract={This paper argues that formal models of coherence are useful for constructing a legal epistemology. Two main formal approaches to coherence are examined: coherence-based models of belief revision and the theory of coherence as constraint satisfaction. It is shown that these approaches shed light on central aspects of a coherentist legal epistemology, such as the concept of coherence, the dynamics of coherentist justification in law, and the mechanisms whereby coherence may be built in the course of legal decision-making.}}
@ARTICLE{Berenbach_2004,title={Comparison of UML and text based requirements engineering},year={2004},author={Brian Berenbach and Brian Berenbach},doi={10.1145/1028664.1028766},pmid={null},pmcid={null},mag_id={2014713859},journal={null},abstract={There appears to be a real dichotomy in the use of the UML vs. text based Use Case development for requirements elicitation and documentation, that is, on those projects where use cases are work products. Not only are there different processes in place for text and graphical use case modeling, but also there are a variety of approaches and philosophies in each camp. This paper will discuss the prose vs. graphical approaches to requirements elicitation, observed advantages and disadvantages of each, and suggest best practices for improving and/or creating effective processes for requirements elicitation.}}
@ARTICLE{Prakken_2005,title={AI & Law, Logic and Argument Schemes},year={2005},author={Henry Prakken and Henry Prakken},doi={10.1007/s10503-005-4418-7},pmid={null},pmcid={null},mag_id={2014901261},journal={Argumentation},abstract={This paper reviews the history of AI & Law research from the perspective of argument schemes. It starts with the observation that logic, although very well applicable to legal reasoning when there is uncertainty, vagueness and disagreement, is too abstract to give a fully satisfactory classification of legal argument types. It therefore needs to be supplemented with an argument-scheme approach, which classifies arguments not according to their logical form but according to their content, in particular, according to the roles that the various elements of an argument can play. This approach is then applied to legal reasoning, to identify some of the main legal argument schemes. It is also argued that much AI & Law research in fact employs the argument-scheme approach, although it usually is not presented as such. Finally, it is argued that the argument-scheme approach and the way it has been employed in AI & Law respects some of the main lessons to be learnt from Toulmin’s The Uses of Argument.}}
@ARTICLE{McGraw_2006,title={Software Security: Building Security In},year={2006},author={Gary McGraw and Gary McGraw},doi={null},pmid={null},pmcid={null},mag_id={2015004885},journal={null},abstract={Summary form only given. Software security has come a long way in the last few years, but we've really only just begun. I will present a detailed approach to getting past theory and putting software security into practice. The three pillars of software security are applied risk management, software security best practices (which I call touchpoints), and knowledge. By describing a manageably small set of touchpoints based around the software artifacts that you already produce, I avoid religious warfare over process and get on with the business of software security. That means you can adopt the touchpoints without radically changing the way you work. The touchpoints I will describe include: code review using static analysis tools; architectural risk analysis; penetration testing; security testing; abuse case development; and security requirements. Like the yin and the yang, software security requires a careful balance-attack and defense, exploiting and designing, breaking and building-bound into a coherent package. Create your own Security Development Lifecycle by enhancing your existing software development lifecycle with the touchpoints}}
@ARTICLE{Walker_2007,title={Visualizing the dynamics around the rule-evidence interface in legal reasoning},year={2007},author={Vern R. Walker and Vern R. Walker},doi={10.1093/lpr/mgm015},pmid={null},pmcid={null},mag_id={2015753041},journal={Law, Probability and Risk},abstract={This paper presents a visual framework for modelling complex legal reasoning-reasoning that integrates legal rules and policies with expert and non-expert evidence. The framework is based on a many-valued, predicate, default logic. The paper first visualizes the two sides of the rule-evidence interface: rule-based deductions and evidence evaluation. It then explores ways to visualize several dynamics around that interface, including dynamics concerning evidentiary relevance, findings of fact, process decision making about motions, policy-based reasoning about rules and relevant-factor reasoning. The paper then concludes with visualizing dynamics across multiple cases and briefly discusses one pathway by which new legal rules might emerge from the factfinding process. The paper therefore presents a visual working environment for people who litigate or decide actual cases, who study judicial or administrative reasoning or who teach law.}}
@ARTICLE{Mens_2001,title={Future trends in software evolution metrics},year={2001},author={Tom Mens and Tom Mens and Serge Demeyer and Serge Demeyer},doi={10.1145/602461.602476},pmid={null},pmcid={null},mag_id={2017258744},journal={null},abstract={Since the famous statement "What is not measurable make measurable" of Galileo Galilei (1564 - 1642) it has been a major goal in science to quantify observations as a way to understand and control the underlying causes. With the growing awareness that evolution is a key aspect of software, an increasing number of computer scientists is investigating how metrics can be applied to evolving software artifacts. This paper provides a classification of the various approaches that use metrics to analyse, understand, control and improve the software evolution process, and identifies topics that require further research. As such, we expect that this paper will stimulate this emerging research area.}}
@ARTICLE{Schleicher_2010,title={Compliant business process design using refinement layers},year={2010},author={Daniel Schleicher and Daniel Schleicher and Tobias Anstett and Tobias Anstett and Frank Leymann and Frank Leymann and David Schumm and David Schumm},doi={10.1007/978-3-642-16934-2_11},pmid={null},pmcid={null},mag_id={2017481973},journal={null},abstract={In recent years compliance has emerged as one of the big IT challenges enterprises are faced with. The management of a multitude of regulations and the complexity of current business processes are problems that need to be addressed.

In this paper we present an approach based on so-called compliance templates to develop and manage compliant business processes involving different stakeholders.

We introduce the concept of a refinement process. In the refinement process each compliance template is refined in a layered way to get an executable business process. The refinement steps are executed on refinement layers by different stakeholders. Compliance constraints are used to restrict the way a compliance template can be refined. Introduced in a certain refinement layer of the refinement process, compliance constraints are propagated to higher refinement layers.}}
@ARTICLE{Poulin_1997,title={The other formalization of law: SGML modelling and tagging},year={1997},author={Daniel Poulin and Daniel Poulin and Guy Huard and Guy Huard and Alain Lavoie and Alain Lavoie},doi={10.1145/261618.261637},pmid={null},pmcid={null},mag_id={2017796265},journal={null},abstract={Article Free Access Share on The other formalization of law: SGML modelling and tagging Authors: Daniel Poulin Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7 Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7View Profile , Guy Huard Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7 Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7View Profile , Alain Lavoie Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7 Centre de recherche en droit public, University of Montréal, C.P. 6128, succ. Centre-ville, Montréal(Québec), Canada H3C 3J7View Profile Authors Info & Claims ICAIL '97: Proceedings of the 6th international conference on Artificial intelligence and lawJune 1997 Pages 82–88https://doi.org/10.1145/261618.261637Online:30 June 1997Publication History 5citation318DownloadsMetricsTotal Citations5Total Downloads318Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Grasso_2000,title={Dialectical argumentation to solve conflicts in advice giving},year={2000},author={Floriana Grasso and Floriana Grasso and Alison Cawsey and Alison Cawsey and Ray Jones and Ray Jones},doi={10.1006/ijhc.2000.0429},pmid={null},pmcid={null},mag_id={2017840457},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={Conflict situations do not only arise from misunderstandings, erroneous perceptions, partial knowledge, false beliefs, etc., but also from differences in “opinions” and in the different agents' value systems. It is not always possible, and maybe not even desirable, to “solve” this kind of conflict, as the sources are subjective. The communicating agents can, however, use knowledge of the opponent's preferences, to try and convince the partner of a point of view which they wish to promote. To deal with these situations requires an argumentative capacity, able to handle not only “demonstrative” arguments but also “dialectic” ones, which may not necessarily be based on rationality and valid premises. This paper presents a formalization of a theory of informal argumentation, focused on techniques to change attitudes of the interlocutor, in the domain of health promotion.}}
@ARTICLE{Jones_1986,title={Ought' and 'must},year={1986},author={Andrew Jones and Andrew J. I. Jones and Ingmar Pörn and Ingmar Pörn},doi={10.1007/bf00413581},pmid={null},pmcid={null},mag_id={2018509459},journal={Synthese},abstract={null}}
@ARTICLE{Nuseibeh_2011,title={Editorial: State of the Journal},year={2011},author={Bashar Nuseibeh and Bashar Nuseibeh},doi={10.1109/tse.2011.17},pmid={null},pmcid={null},mag_id={2018719839},journal={IEEE Transactions on Software Engineering},abstract={HAPPY New Year. It has been exactly one year since I started as Editor-in-Chief of the IEEE Transactions on Software Engineering (TSE), so in this editorial I would like to refl ect on to refl ect on this last year and to share with you some facts and fi gures and my plans for the next year. In my fi rst editorial last January, I suggested three medium term goals for the journals that I reiterate below and which I believe still hold: 1. To reemphasize the broad scope of the journal, encouraging the publication of multidisciplinary research as well as specialization in software engineering. 2. To engage with the software engineering community at large, including other journals, about the software engineering discipline, using not only the journal itself as a forum but through other professional outlets such as conferences and the Web. 3. To support a debate on the nature of scholarly discourse in software engineering, such as the kind and length of papers published in the journal, or the different forms of peer review and discussion around what is published. My fi rst goal above is rather subtle. The scope of the journal is already quite precisely defi ned and has not changed for some time. However, I still believe that as software—and software engineering—continues to permeate society and the development of systems of all kinds, software engineering research also has opportunities to grow. To grow in terms of its interaction with other disciplines outside its traditional boundaries, but also to grow it terms of specialization within, say, problem domain boundaries. To this end, I have encouraged the TSE editorial board to help solicit a broader range of submissions, but also to handle submissions outside TSE’s traditional boundaries with tolerance, even enthusiasm. This sometimes means making an additional effort to guide authors to formulate or reformulate submissions to make the work more accessible to a software engineering audience. The impact of this approach will take some time to have a visible effect, as most of the papers submitted to TSE in 2010 have yet to be published. With respect to my second goal above, community engagement is an ongoing effort. In 2010 I met with the editorsin-chief of many of the software engineering research journals, and we discussed ways in which we can help present research that is novel and exciting, that is rigorous and well-presented, and that is relevant and useful to readers of our publications. I have encouraged, but only been partially successful, in engaging TSE readers to contribute to the debate about the discipline of software engineering. I say “partially successful” because there has been almost no discussion on the TSE electronic forum, but I have received many individual e-mails from readers addressing very specifi c issues, such as those that I raise in my regular editorials. It does seem that an open ended invitation to discuss the discipline is too vague to engage most readers, and so I will continue to discuss specifi c topics in each issue of the journal. The third goal above is one that has been receiving attention from other journal and magazine editors as well. I do sense that the value of journal publications in software engineering is back on the rise, and I feel that my appeal to authors to submit different kinds (and lengths) of manuscripts is being addressed. I have said previously that there is nothing sacred about the kind and length of a software engineering research contribution, and in the last year TSE received more short and a few very long manuscripts. In all cases, I have asked associate editors to consider if content justifi es the length, rather than be distracted by any notion of suggested manuscript length. Papers published in 2011 will be partly the result of such an approach. What else have I learned from year 1? Well, personally, I have been saddened to observe that much of my time is spent handling exceptions of the rather unpleasant variety—plagiarism, duplicate or overlapping submissions, and, in a few cases, appeals. I have absolutely no problem at all with appeals. Authors should feel free to question review decisions on their papers—the review process is not infallible—and queries or complaints help test the robustness of the process and the resultant decisions. Similarly, I encourage readers to write and question or discuss published papers’ content. Some readers have chosen to do this anonymously and that’s fi ne, although I would welcome more open technical commentaries, which TSE has the mechanism to publish in hardcopy or in the digital library. What does bother me are the incidents of what can only be termed unethical behavior—such as plagiarism. The penalties for this are severe, and a “prohibited authors” list is updated and circulated among all IEEE editors each month, with each additional named author fully investigated before being “blacklisted” and barred from future submission to the journal. Incidence of self-plagiarism or duplicate submissions are equally troubling, and a real time drain for editors and reviewers. In a future editorial, I will also discuss the undesirable phenomenon of an LPU—a “least publishable unit”—which unfortunately seems be a way for some authors to increase their publication counts by publishing very small incremental contributions over previously published work. Duplicate or heavily overlapping submissions are often detected, due to the limited and specialized community of reviewers and the open and interactive relationships between journal editors.}}
@ARTICLE{Walton_1987,title={The ad Hominem argument as an informal fallacy},year={1987},author={Douglas Walton and Douglas Walton},doi={10.1007/bf00136781},pmid={null},pmcid={null},mag_id={2018745586},journal={Argumentation},abstract={This article outlines criteria for the evaluation of the argumentum ad hominem (argument against the person, or personal attack in argument) that is traditionally a part of the curriculum in informal logic. The argument is shown to be a kind of criticism which works by shifting the burden of proof in dialogue through citing a pragmatic inconsistency in an arguer's position. Several specific cases of ad hominem argumentation which pose interesting problems in analyzing this type of criticism are studied.}}
@ARTICLE{Castellanos_2011,title={An Ontology-Matching Based Proposal to Detect Potential Redundancies on Enterprise Architectures},year={2011},author={Camilo Castellanos and Camilo Castellanos and Darío Correal and Dario Correal and Francisco Murcia and Francisco Murcia},doi={10.1109/sccc.2011.16},pmid={null},pmcid={null},mag_id={2020429291},journal={null},abstract={Our work presents an approach to automatic detection of potentially redundant elements within an Enterprise Architecture (EA). Unintended or unidentified redundancies can affect the data quality, duplicating efforts which may lead to inconsistencies. Evaluating and identifying manually redundant elements in large information systems (IS) is a tedious, error-prone and time-consuming task. Untimely, incomplete or inaccurate analysis, could affect the dynamics and organizational flexibility that promotes an EA. Our proposal is a MDA (Model-Driven Architecture) and Ontology-Alignment based. The main idea is to perform a transformation from an EA model towards ontology in OWL format (Ontology Web Language) and exploit ontology matching tools to infer correspondences between concepts. Our objective is to support analysis and making decisions of architect with an integral view which describes EAs data and processes, and its inner relationships.}}
@ARTICLE{Fricke_2005,title={Design for changeability (DfC): Principles to enable changes in systems throughout their entire lifecycle},year={2005},author={Ernst Fricke and Ernst Fricke and Armin P. Schulz and Armin P. Schulz},doi={10.1002/sys.v8:4},pmid={null},pmcid={null},mag_id={2021154257},journal={Systems Engineering},abstract={In the past decades the world has been changing in almost every aspect. Systems development is facing rapidly changing and increasingly global environments in markets, competition, technology, regulatory, and societal systems. Systems to be delivered must be designed not only to meet customer or market needs, but also increasingly to meet requirements and constraints of systems sharing its operational context and throughout their entire lifecycle. The design of a system must provide for a continuous evolution of its architecture either by upgrading a system already in service or releasing a new version or derivative. Based on these key challenges imposed on development systems, this paper will evolve the idea of incorporating changeability into a system architecture. Flexibility, agility, robustness, and adaptability as four key aspects of changeability will be defined and described. Design principles to enable flexibility, agility, robustness, and adaptability within systems are proposed and described. A basic approach outlining and guiding an application of the framework described concludes this paper. Examples from varying industries will illustrate the applicability and implementation of selected principles. Thus this paper spans a view from why, when, and how changeability has to be incorporated into a system's architecture. © 2005 Wiley Periodicals, Inc. Syst Eng 8: 342–359, 2005}}
@ARTICLE{Boehm_1988,title={Understanding and controlling software costs},year={1988},author={Barry Boehm and Barry Boehm and Phillip N. Papaccio and P.N. Papaccio},doi={10.1109/32.6191},pmid={null},pmcid={null},mag_id={2021503621},journal={IEEE Transactions on Software Engineering},abstract={A discussion is presented of the two primary ways of understanding software costs. The black-box or influence-function approach provides useful experimental and observational insights on the relative software productivity and quality leverage of various management, technical, environmental, and personnel options. The glass-box or cost distribution approach helps identify strategies for integrated software productivity and quality improvement programs using such structures as the value chain and the software productivity opportunity tree. The individual strategies for improving software productivity are identified. Issues related to software costs and controlling them are examined and discussed. It is pointed out that a good framework of techniques exists for controlling software budgets, schedules, and work completed, but that a great deal of further progress is needed to provide an overall set of planning and control techniques covering software product qualities and end-user system objectives. >}}
@ARTICLE{Karlsson_1997,title={A cost-value approach for prioritizing requirements},year={1997},author={Jenny Karlsson and Joachim Karlsson and Kevin Ryan and Kevin Ryan},doi={10.1109/52.605933},pmid={null},pmcid={null},mag_id={2021767672},journal={IEEE Software},abstract={Developing software systems that meet stakeholders' needs and expectations is the ultimate goal of any software provider seeking a competitive edge. To achieve this, you must effectively and accurately manage your stakeholders' system requirements: the features, functions, and attributes they need in their software system. Once you agree on these requirements, you can use them as a focal point for the development process and produce a software system that meets the expectations of both customers and users. However, in real world software development, there are usually more requirements than you can implement given stakeholders' time and resource constraints. Thus, project managers face a dilemma: how do you select a subset of the customers' requirements and still produce a system that meets their needs? The authors developed a cost-value approach for prioritizing requirements and applied it to two commercial projects.}}
@ARTICLE{Grau_2008,title={PRiM: An i * -based process reengineering method for information systems specification},year={2008},author={G. Grau and Gemma Grau and Xavier Franch and Xavier Franch and Neil Maiden and Neil Maiden},doi={10.1016/j.infsof.2007.10.006},pmid={null},pmcid={null},mag_id={2022101110},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Gordon_2013,title={Introducing the Carneades web application},year={2013},author={Thomas F. Gordon and Thomas F. Gordon},doi={10.1145/2514601.2514637},pmid={null},pmcid={null},mag_id={2023077832},journal={null},abstract={This ICAIL system demonstration introduces the Web version of the Carneades argumentation system, which provides software tools based on a common computational model of argument graphs useful for policy deliberations and claims processing.}}
@ARTICLE{Loniewski_2010,title={A systematic review of the use of requirements engineering techniques in model-driven development},year={2010},author={Grzegorz Loniewski and Grzegorz Loniewski and Emilio Insfrán and Emilio Insfran and Emilio Insfran and Emilio Insfran and Silvia Abrahão and Silvia Abrahão},doi={10.1007/978-3-642-16129-2_16},pmid={null},pmcid={null},mag_id={2024275443},journal={null},abstract={Model-Driven Development (MDD) emphasizes the use of models at a higher abstraction level in the software development process and argues in favor of automation via model execution, transformation, and code generation. However, one current challenge is how to manage requirements during this process whilst simultaneously stressing the benefits of automation. This paper presents a systematic review of the current use of requirements engineering techniques in MDD processes and their actual automation level. 72 papers from the last decade have been reviewed from an initial set of 884 papers. The results show that although MDD techniques are used to a great extent in platform-independent models, platform-specific models, and at code level, at the requirements level most MDD approaches use only partially defined requirements models or even natural language. We additionally identify several research gaps such as a need for more efforts to explicitly deal with requirements traceability and the provision of better tool support.}}
@ARTICLE{Colen_2009,title={How much logical structure is helpful in content-based argumentation software for legal case solving?},year={2009},author={S. Colen and S. Colen and Fokie Cnossen and Fokie Cnossen and Bart Verheij and Bart Verheij},doi={10.1145/1568234.1568265},pmid={null},pmcid={null},mag_id={2024368005},journal={null},abstract={Current argumentation support software often employs graphical representations of logical relationships. Little is known about the extent to which logical structuring helps to increase a user's task performance. In this research, various levels of graphical representation of the logical structure of legal subject matter are experimentally compared in terms of performance. It is shown that logical structuring significantly increases task performance, but we have found no evidence that the extensive representation of logical structure as employed by several contemporary software applications is more effective or usable than a simplified graphical representation that was previously implemented in an application called ArguGuide.}}
@ARTICLE{Żurek_2013,title={Modeling teleological interpretation},year={2013},author={Tomasz Żurek and Tomasz Zurek and Michał Araszkiewicz and Michał Araszkiewicz},doi={10.1145/2514601.2514619},pmid={null},pmcid={null},mag_id={2026095380},journal={null},abstract={The paper presents a model of teleological interpretation of statutory legal rules as well as an example of the genuine law case, which has been modeled with use of established methodology.}}
@ARTICLE{Liu_2007,title={A static compliance-checking framework for business process models},year={2007},author={Y. Liu and Y. Liu and S. Müller and S. Müller and Kui Xu and K. Xu},doi={10.1147/sj.462.0335},pmid={null},pmcid={null},mag_id={2027158876},journal={Ibm Systems Journal},abstract={Regulatory compliance of business operations is a critical problem for enterprises. As enterprises increasingly use business process management systems to automate their business processes, technologies to automatically check the compliance of process models against compliance rules are becoming important. In this paper, we present a method to improve the reliability and minimize the risk of failure of business process management systems from a compliance perspective. The proposed method allows separate modeling of both process models and compliance concerns. Business process models expressed in the Business Process Execution Language are transformed into pi-calculus and then into finite state machines. Compliance rules captured in the graphical Business Property Specification Language are translated into linear temporal logic. Thus, process models can be verified against these compliance rules by means of model-checking technology. The benefit of our method is threefold: Through the automated verification of a large set of business process models, our approach increases deployment efficiency and lowers the risk of installing noncompliant processes; it reduces the cost associated with inspecting business process models for compliance; and compliance checking may ensure compliance of new process models before their execution and thereby increase the reliability of business operations in general.}}
@ARTICLE{Weidenhaupt_1998,title={Scenarios in system development: current practice},year={1998},author={K. Weidenhaupt and Klaus Weidenhaupt and K. Weidenhaupt and Klaus Pohl and Klaus Pohl and Matthias Jarke and Matthias Jarke and Peter Haumer and Peter Haumer},doi={10.1109/52.663783},pmid={null},pmcid={null},mag_id={2027515180},journal={IEEE Software},abstract={Scenario based approaches are becoming ubiquitous in systems analysis and design but remain vague in definition and scope. A survey of current practices indicates we must offer better means for structuring, managing, and developing their use in diverse contexts. The European Esprit project Crews (Cooperative Requirements Engineering with Scenarios) are seeking a deeper understanding of scenario diversity, necessary to improve methodological and tool support for scenario based requirements engineering. They follow a two pronged strategy to gain this understanding. First, following the "3 dimensions" requirements engineering framework developed in the precursor Nature project (K. Pohl, 1994), they developed a scenario classification framework based on a comprehensive survey of scenario literature in requirements engineering, human computer interaction, and other fields. They used the framework to classify 11 prominent scenario based approaches. Secondly, to complement this research framework, they investigated scenario applications in industrial projects through site visits with scenario user projects. The article focuses on these site visits. It was found that while many companies express interest in Jacobson's use case approach, actual scenario usage often falls outside what is described in textbooks and standard methodologies. Users therefore face significant scenario management problems not yet addressed adequately in theory or practice, and are demanding solutions to these problems.}}
@ARTICLE{White_1979,title={The Authority of Law},year={1979},author={Alan R. White and Alan R. White and Joseph Raz and Joseph Raz},doi={null},pmid={null},pmcid={null},mag_id={2027539254},journal={null},abstract={null}}
@ARTICLE{Philipps_1994,title={Artificial Morality and Artificial Law},year={1994},author={Lothar Philipps and Lothar Philipps},doi={10.1007/bf00871747},pmid={null},pmcid={null},mag_id={2027957496},journal={Artificial Intelligence and Law},abstract={The article investigates the interplay of moral rules in computer simulation. The investigation is based on two situations which are well-known to game theory: the prisoner's dilemma and the game of Chicken. The prisoner's dilemma can be taken to represent contractual situations, the game of Chicken represents a competitive situation on the one hand and the provision for a common good on the other. Unlike the rules usually used in game theory, each player knows the other's strategy. In that way, ever higher levels of reflection are reached reciprocally. Such strategies can be interpreted as ‘moral’ rules.}}
@ARTICLE{Niknafs_2012,title={The impact of domain knowledge on the effectiveness of requirements idea generation during requirements elicitation},year={2012},author={Ali Niknafs and Ali Niknafs and Daniel M. Berry and Daniel M. Berry},doi={10.1109/re.2012.6345802},pmid={null},pmcid={null},mag_id={2028681620},journal={null},abstract={It is believed that the effectiveness of requirements engineering activities depends at least partially on the individuals involved. One of the factors that seems to influence an individual's effectiveness in requirements engineering activities is knowledge of the problem being solved, i.e., domain knowledge. While a requirements engineer's having in-depth domain knowledge helps him or her to understand the problem easier, he or she can fall for tacit assumptions of the domain and might overlook issues that are obvious to domain experts. This paper describes a controlled experiment to test the hypothesis that adding to a requirements elicitation team for a computer-based system in a particular domain, requirements analysts that are ignorant of the domain improves the effectiveness of the requirements elicitation team. The results, although not conclusive, show some support for accepting the hypothesis. The results were analyzed also to determine the effect of creativity, industrial experience, and requirements engineering experience. The results suggest other hypotheses to be studied in the future.}}
@ARTICLE{Franch_2007,title={SYSTEMATIC CONSTRUCTION OF i STRATEGIC DEPENDENCY MODELS FOR SOCIO-TECHNICAL SYSTEMS},year={2007},author={Xavier Franch and Xavier Franch and G. Grau and Gemma Grau and Enric Mayol and Enric Mayol and Carme Quer and Carme Quer and Claudia Ayala and Claudia P. Ayala and Carlos Cares and Carlos Cares and Fredy Navarrete and Fredy Navarrete and Mariela Haya and Mariela Haya and Pere Botella and Pere Botella},doi={10.1142/s0218194007003148},pmid={null},pmcid={null},mag_id={2029827937},journal={International Journal of Software Engineering and Knowledge Engineering},abstract={Goal- and agent-oriented models have become a consolidated type of artifact in various software and knowledge engineering activities. Several languages exist for representing such type of models but there is a lack of associated methodologies for guiding their construction up to the necessary level of detail. In this paper we present RiSD, a method for building Strategic Dependency (SD) models in the i* notation. RiSD is defined in a prescriptive way to reduce uncertainness when constructing the model. RiSD tackles three fundamental issues: (1) it tends to reduce the average size of the resulting models; (2) it defines some traceability relationships among model elements; (3) it provides some lexical and syntactical conventions. As a result, we may say that RiSD supports the construction process of goal- and agent-oriented models whilst increasing their understanding.}}
@ARTICLE{Leenes_2001,title={Burden of proof in dialogue games and Dutch civil procedure},year={2001},author={Ronald Leenes and Ronald Leenes},doi={10.1145/383535.383549},pmid={null},pmcid={null},mag_id={2031827268},journal={null},abstract={In recent years dialectics has become popular in Artificial Intelligence and law. A sub-branch of this field concentrates on the development of dialogue games. Many of these legal dialogue systems are fairly simple, in some respects even too simple. Among the topics dialogue games can improve on, is the division of the burden of proof. In this paper I discuss some legal dialogue games in the light of an actual legal procedure to show the shortcomings of these models. The paper shows what computational dialectics can learn form legal practice.}}
@ARTICLE{Woods_1978,title={The Fallacy of ‘Ad Ignorantiam’},year={1978},author={John A. Woods and John Woods and Douglas Walton and Douglas Walton},doi={10.1111/j.1746-8361.1978.tb01304.x},pmid={null},pmcid={null},mag_id={2032374825},journal={Dialectica},abstract={Summary

This paper outlines a three-part analysis of the traditional informal fallacy of ad ignorantiam. As initially characterized, the fallacy consists in arguing that failure to prove falsity (truth) implies the truth (falsity) of a proposition.



First, the fallacy is located within confirmation theory as a confusion between the categories of “lack of confirming evidence” and “presence of disconfirming evidence”. Second, the structure of the fallacy can be seen as an illicit negation shift in Hintikka-style epistemic logic. Third, the fallacy can be studied as an attempt to unfairly shift the burden of proof in a dialectical game. We suggest that research on ad ignorantiam needs a broadening of the scope of philosophical logic to encompass concepts of correct argument in these three contexts.

Resume

Cet article propose une analyse en trois parties du sophisme informel traditionnel ad ignorantiam. Sous sa forme originale, celui-ci consiste a argumenter que l'absence de preuve de la faussete (verite) implique la verite (faussete) d'une proposition.



1
Dans le cadre de la theorie de la confirmation, le sophisme apparait comme une confusion entre les categories absence de confirmation experimentale et presence d'infirmation experimentale.

2
Dans le cadre d'une logique epistemique style Hintikka, la structure du sophisme peut etre consideree comme un passage illicite a la negation.

3
Dans un jeu dialectique, le sophisme peut etre presente comme une tentative deloyale pour rejeter sur l'autre la charge de la preuve.





Nous pensons que cette recherche sur le sophisme ad ignorantiam exige un elargissement du domaine de la logique philosophique de maniere a ce qu'elle puisse assurer une argumentation correcte dans ces trois contextes.

Zusammenfassung

Es wird eine dreiteilige Analyse des traditionellen Fehlschlusses ad ignorantiam vorgelegt. Wie eingangs gezeigt wird, besteht der Fehlschluss darin, dass vom Misslingen, die Falschheit eines Satzes zu beweisen, auf seine Wahrheit geschlossen wird.



Erstens wird der Fehlschluss im Rahmen der Bewahrungstehorie als Folge einer Verwechslung zwischen den beiden Kategorien des Mangels an Bewahrungsinstanzenund des Vorhandenseins von Bewahrungsinstanzendargestellt. Zweitens kann die Struktur des Fehlschlusses als eine unzulassige Negationsverschiebung im Sinne von Hintikkas epistemischer Logik betrachtet werden. Drittens kann der Fehlschluss als Versuch gedeutet werden, die Beweislast in einem dialektischen Spiel in unfairer Weise auf den Gegner abzuschieben.



Es wird nahegelegt, dass die Erforschung dieses Problems eine Erweiterung des Bereichs der philosophischen Logik erfordert, so dass diese auch die Begriffe des korrekten Argumentierens in den drei genannten Kontexten enthalt.}}
@ARTICLE{Branting_1991,title={Building explanations from rules and structured cases},year={1991},author={L. Karl Branting and L. Karl Branting},doi={10.1016/0020-7373(91)90012-v},pmid={null},pmcid={null},mag_id={2032974234},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={null}}
@ARTICLE{Alexander_2005,title={A Taxonomy of Stakeholders: Human Roles in System Development},year={2005},author={Ian Alexander and Ian F. Alexander and Ian F. Alexander and I. Alexander},doi={10.4018/jthi.2005010102},pmid={null},pmcid={null},mag_id={2033170101},journal={International Journal of Technology and Human Interaction},abstract={Systems engineers have often paid too little attention to the nature of the so-called â€œusersâ€ of products under development. These are better called stakeholders, as many roles are involved, and few of those are in direct contact with the developed products. A simple and robust conceptual framework for classifying development stakeholders â€” a taxonomy â€” is proposed. The taxonomy is product centric, with concentric â€œcirclesâ€ denoting broad categories of stakeholders. Within these, generic â€œslotsâ€ describe typical classes of stakeholders; these are subdivided into â€œroles,â€ which are expected to vary at least in name with the domain. Examples are given, and a popular template is reanalysed using the framework. The taxonomy has immediate value in identifying and validating stakeholder roles in requirements elicitation, helping to ensure that key viewpoints are not missed, and hence reducing the risk of instability and failure during development.}}
@ARTICLE{Macintosh_2009,title={Providing Argument Support for E-Participation},year={2009},author={Ann Macintosh and Ann Macintosh and Thomas F. Gordon and Thomas F. Gordon and Alastair Renton and Alastair Renton},doi={10.1080/19331680802662113},pmid={null},pmcid={null},mag_id={2034775650},journal={Journal of Information Technology & Politics},abstract={ABSTRACT As governments seek to consult their citizens over matters of policy, it becomes increasingly important for citizens to receive relevant information in a medium that they can use, and will want to use, in forming their opinion upon consultative issues. In e-participation, there is a clear requirement to understand how technology can support informed debate on issues, but there are two main obstacles in achieving this. The first is that the deliberation is often on complex issues, and therefore typically there are many arguments and counter arguments to consider, which, when presented in linear text, can be confusing for the public at large. Second, it is not obvious that many people actually have the necessary critical thinking skills to deliberate on issues. Argumentation systems have been used successfully in the domains of law and education, where they have been developed in response to a need for innovative and effective ways of teaching critical thinking, presenting and defending a point of ...}}
@ARTICLE{Bench‐Capon_2003,title={Try to see it my way: modelling persuasion in legal discourse},year={2003},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1023/b:arti.0000045997.45038.8f},pmid={null},pmcid={null},mag_id={2034868989},journal={Artificial Intelligence and Law},abstract={In this paper I argue that to explain and resolve some kinds of disagreement we need to go beyond what logic alone can provide. In particular, following Perelman, I argue that we need to consider how arguments are ascribed different strengths by different audiences, according to how accepting these arguments promotes values favoured by the audience to which they are addressed. I show how we can extend the standard framework for modelling argumentation systems to allow different audiences to be represented. I also show how this formalism can explain how some disputes can be resolved while in others the parties can only agree to differ. I illustrate this by consideration of a legal example. Finally, I make some suggestions as to where these values come from, and how they can be used to explain differences across jurisdictions, and changes in views over time.}}
@ARTICLE{Prakken_2003,title={Argumentation schemes and generalisations in reasoning about evidence},year={2003},author={Henry Prakken and Henry Prakken and Chris Reed and Chris Reed and Douglas Walton and Douglas Walton},doi={10.1145/1047788.1047794},pmid={null},pmcid={null},mag_id={2034927516},journal={null},abstract={This paper studies the modelling of legal reasoning about evidence within general theories of defeasible reasoning and argumentation. In particular, it is studied how Wigmore's method for charting evidence and its use by modern legal evidence scholars can be exploited by modern visualisation software for argumentation, and how a formal account of the method can be given in terms of logics for defeasible argumentation. Two notions turn out to be crucial, viz. argumentation schemes and empirical generalisations.}}
@ARTICLE{Francesconi_2009,title={Integrated access to legal literature through automated semantic classification},year={2009},author={Enrico Francesconi and Enrico Francesconi and Ginevra Peruginelli and G. Peruginelli and G. Peruginelli and Ginevra Peruginelli},doi={10.1007/s10506-008-9072-6},pmid={null},pmcid={null},mag_id={2035928539},journal={Artificial Intelligence and Law},abstract={Access to legal information and, in particular, to legal literature is examined for the creation of a search and retrieval system for Italian legal literature. The design and implementation of services such as integrated access to a wide range of resources are described, with a particular focus on the importance of exploiting metadata assigned to disparate legal material. The integration of structured repositories and Web documents is the main purpose of the system: it is constructed on the basis of a federation system with service provider functions, aiming at creating a centralized index of legal resources. The index is based on a uniform metadata view created for structured data by means of the OAI approach and for Web documents by a machine learning approach, which, in this paper, has been assessed as regards document classification. Semantic searching is a major requirement for legal literature users and a solution based on the exploitation of Dublin Core metadata, as well as the use of legal ontologies and related terms prepared for accessing indexed articles have been implemented.}}
@ARTICLE{Poulin_1993,title={Legal interpretation in expert systems},year={1993},author={Daniel Poulin and Daniel Poulin and Paul Bratley and Paul Bratley and Jacques Frémont and Jacques Frémont and Ejan Mackaay and Ejan Mackaay},doi={10.1145/158976.158988},pmid={null},pmcid={null},mag_id={2036876493},journal={null},abstract={A legal expert system expresses as a set of formal rules the norms found in the provisions of a statute or regulation, in case law or other legal texts. The process of constructing the system involves interpreting these legal texts and recasting each of them into one or more formal legal rules. The primary sources of law or law-formulations, to use Susskind’s term [Susskind 87, 36-37, 124], must be transformed into law-statements—statements about what the content of the law is—and these in turn must be translated into a formal language as rules of inference which Susskind terms legal productions. These transformations take place outside the context of specific cases, that is without reference to concrete legal problems those rules are designed to solve. The construction of a legal expert system involves two conceptual steps: to identify the legal norms that a statute conveys and to express these norms as formal rules. Several strategies have been proposed for this transformation process. Some consist of a rather straightforward transposition of law texts into formal language at the expense of a substantial loss of meaning of the legal concepts involved; others call for a subtler and more complex legal analysis. In all cases, however, what is at stake is the interpretation of legal documents. The problem of interpreting legal documents is well known to lawyers. In legal usage, the term interpretation is employed when the meaning of a legal text, typically a statutory provision, has to be assessed in a concrete situation. In constructing expert systems, one must interpret legal documents ahead of such concrete applications. This interpretation can only be provisional. The best one can do in constructing the knowledge base is to foreclose as few as possible of the meanings for particular provisions one may ultimately want to consider in concrete situations. Conversely, when using the system in a particular situation, one may want to consider different interpretations and their}}
@ARTICLE{Barry_2003,title={On the uniformity of software evolution patterns},year={2003},author={Evelyn Barry and Evelyn J. Barry and Chris F. Kemerer and Chris F. Kemerer and Sandra A. Slaughter and Sandra A. Slaughter},doi={10.1109/icse.2003.1201192},pmid={null},pmcid={null},mag_id={2037191245},journal={null},abstract={Preparations for Y2K reminded the software engineering community of the extent to which long-lived software systems are embedded in our daily environments. As systems are maintained and enhanced throughout their lifecycles they appear to follow generalized behaviors described by the laws of software evolution. Within this context, however, there is some question of how and why systems may evolve differently. The objective of this work is to answer the question: do systems follow a set of identifiable evolutionary patterns? In this paper we use software volatility to describe the lifecycle evolution of a portfolio of 23 software systems. We show by example that a vector of software volatility levels can represent lifecycle behavior of a software system. We further demonstrate that the portfolio ' s 23 software volatility vectors can be grouped into four distinguishable patterns. Thus, we show by example that there are different patterns of system lifecycle behavior, i.e. software evolution.}}
@ARTICLE{Conrad_2009,title={Query-based opinion summarization for legal blog entries},year={2009},author={Jack G. Conrad and Jack G. Conrad and Jochen L. Leidner and Jochen L. Leidner and Frank Schilder and Frank Schilder and Ravi Kondadadi and Ravi Kondadadi},doi={10.1145/1568234.1568253},pmid={null},pmcid={null},mag_id={2037940981},journal={null},abstract={We present the first report of automatic sentiment summarization in the legal domain. This work is based on processing a set of legal questions with a system consisting of a semi-automatic Web blog search module and FastSum, a fully automatic extractive multi-document sentiment summarization system. We provide quantitative evaluation results of the summaries using legal expert reviewers. We report baseline evaluation results for query-based sentiment summarization for legal blogs: on a five-point scale, average responsiveness and linguistic quality are slightly higher than 2 (with human inter-rater agreement at k = 0.75). To the best of our knowledge, this is the first evaluation of sentiment summarization in the legal blogosphere.}}
@ARTICLE{Moens_1997,title={Abstracting of legal cases: the SALOMON experience},year={1997},author={Marie‐Francine Moens and Marie-Francine Moens and Caroline Uyttendaele and Caroline Uyttendaele and Jos Dumortier and Jos Dumortier},doi={10.1145/261618.261643},pmid={null},pmcid={null},mag_id={2038086538},journal={null},abstract={ing of Legal Cases: The SALOMON Experience Marie-Francine Moens, Caroline Uyttendaele, Jos Dumortier Interdisciplinary Centre for Law and IT (ICRI) K.U. Leuven Tiensestraat 41 B-3000 Leuven Belgium {marie-france.moens, caroline.uyttendaele, jos.dumortier}@law.kuleuven.ac.be}}
@ARTICLE{Herrestad_1991,title={Norms and formalization},year={1991},author={Henning Herrestad and Henning Herrestad},doi={10.1145/112646.112667},pmid={null},pmcid={null},mag_id={2038180015},journal={null},abstract={null}}
@ARTICLE{Chandrasekaran_1986,title={Generic Tasks in Knowledge-Based Reasoning: High-Level Building Blocks for Expert System Design},year={1986},author={B. Chandrasekaran and B. Chandrasekaran},doi={10.1109/mex.1986.4306977},pmid={null},pmcid={null},mag_id={2038386889},journal={IEEE Intelligent Systems},abstract={null}}
@ARTICLE{Rissland_2005,title={Case-based reasoning and law},year={2005},author={Edwina L. Rissland and Edwina L. Rissland and Kevin D. Ashley and Kevin D. Ashley and L. Karl Branting and L. Karl Branting},doi={10.1017/s0269888906000701},pmid={null},pmcid={null},mag_id={2039074475},journal={Knowledge Engineering Review},abstract={A primary research stream that contributed to the birth of case-based reasoning (CBR) was Artificial Intelligence and Law. Since law is largely about cases, it is a particularly interesting domain for CBR researchers. This article surveys some of the historically significant systems and developments in this field.}}
@ARTICLE{Purdy_1987,title={Knowledge representation in “Default”: An attempt to classify general types of knowledge used by legal experts},year={1987},author={R. D. Purdy and R. D. Purdy},doi={10.1145/41735.41758},pmid={null},pmcid={null},mag_id={2039185386},journal={null},abstract={Article Free Access Share on Knowledge representation in “Default”: An attempt to classify general types of knowledge used by legal experts Author: R. D. Purdy The Univ. of Akron The Univ. of AkronView Profile Authors Info & Claims ICAIL '87: Proceedings of the 1st international conference on Artificial intelligence and lawDecember 1987 Pages 199–208https://doi.org/10.1145/41735.41758Online:01 December 1987Publication History 0citation222DownloadsMetricsTotal Citations0Total Downloads222Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Gupta_1993,title={Validation and Verification of Knowledge-Based Systems: A Survey},year={1993},author={Uma G. Gupta and Uma G. Gupta},doi={10.1007/bf00872136},pmid={null},pmcid={null},mag_id={2039794841},journal={Applied Intelligence},abstract={Validation and verification of expert systems or knowledge-based systems is a critical issue in the development and deployment of robust systems. This article is a comprehensive survey of the developments and trends in this field. More than 300 references are included in the References and Additional Readings at the end of article.}}
@ARTICLE{Boella_2014,title={A critical analysis of legal requirements engineering from the perspective of legal practice},year={2014},author={Guido Boella and Guido Boella and Llio Humphreys and Llio Humphreys and Robert Muthuri and Robert Muthuri and Piercarlo Rossi and Piercarlo Rossi and Piercarlo Rossi and Leendert van der Torre and Leendert van der Torre},doi={10.1109/relaw.2014.6893476},pmid={null},pmcid={null},mag_id={2039851805},journal={null},abstract={This paper reviews existing approaches to representing legal knowledge for legal requirements engineering. Legal requirement methodologies are rarely developed together with legal practitioners, with the result that often approaches are based on a simplified view of law which prevents their acceptance by legal practitioners. In this paper, we analyse how legal practitioners build legal knowledge and possibilities for existing approaches in RELaw to mirror legal practice.}}
@ARTICLE{Bylander_1987,title={Generic tasks for knowledge-based reasoning: the “right” level of abstraction for knowledge acquisition},year={1987},author={Tom Bylander and Tom Bylander and B. Chandrasekaran and B. Chandrasekaran},doi={10.1016/s0020-7373(87)80093-7},pmid={null},pmcid={null},mag_id={2040672389},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={null}}
@ARTICLE{Bano_2014,title={Systematic reviews in requirements engineering: A tertiary study},year={2014},author={Muneera Bano and Muneera Bano and Didar Zowghi and Didar Zowghi and Naveed Ikram and Naveed Ikram},doi={10.1109/empire.2014.6890110},pmid={null},pmcid={null},mag_id={2040892627},journal={null},abstract={There has been an increasing interest in conducting Systematic Literature Reviews (SLR) among Requirements Engineering (RE) researchers in recent years. However, so far there have been no tertiary studies conducted to provide a comprehensive overview of these published SLR in RE. In this paper we present a tertiary study of SLR that focus solely on RE related topics by following the guidelines of Evidence Based Software Engineering. We have conducted both automated search of major online sources and manual search of the RE and SLR related conferences and journals. Our tertiary study has identified 53 distinct systematic reviews published from 2006 to 2014 and reported in 64 publications. We have assessed the resulting SLR for their quality, and coverage of specific RE related topics thus identifying some gaps. We have observed that the quality of SLR in RE has been decreasing over the recent years. There is a strong need to replicate some of these SLR to increase the reliability of their results for future RE research.}}
@ARTICLE{Law_2014,title={Attitudes towards User Experience (UX) Measurement},year={2014},author={Effie Lai‐Chong Law and Effie Lai-Chong Law and Paul van Schaik and Paul van Schaik and Virpi Roto and Virpi Roto},doi={10.1016/j.ijhcs.2013.09.006},pmid={null},pmcid={null},mag_id={2041074845},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={Abstract   User experience (UX), as a recently established research area, is still haunted by the challenges of defining the scope of UX in general and operationalising experiential qualities in particular. To explore the basic question whether UX constructs are measurable, we conducted semi-structured interviews with 10 UX researchers from academia and one UX practitioner from the industry where a set of questions in relation to UX measurement were explored (Study 1). The interviewees expressed scepticism as well as ambivalence towards UX measures and shared anecdotes related to such measures in different contexts. Interestingly, the results suggested that design-oriented UX professionals tended to be sceptical about UX measurement. To examine whether such an attitude prevailed in the HCI community, we conducted a survey –  UX Measurement Attitudes Survey  (UXMAS) – with essentially the same set of 13 questions used in the interviews (Study 2). Specifically, participants were asked to rate a set of five statements to assess their attitude towards UX measurement, to identify (non)measurable experiential qualities with justifications, and to discuss the topic from the theoretical, methodological and practical perspectives. The survey was implemented in a paper-based and an online format. Altogether, 367 responses were received; 170 of them were valid and analysed. The survey provided empirical evidence on this issue as a baseline for progress in UX measurement. Overall, the survey results indicated that the attitude towards UX measurement was more positive than that identified in the interviews, and there were nuanced views on details of UX measurement. Implications for enhancing the acceptance of UX measures and the interplay between UX evaluation and system development are drawn: UX modelling grounded in theories to link experiential qualities with outcomes; the development of UX measurement tools with good measurement properties, and education within the HCI community to disseminate validated models, and measurement tools as well as their successful applications. Mutual recognition of the value of objective measures and subjective accounts of user experience can enhance the maturity of this area.}}
@ARTICLE{Hawalah_2011,title={A graph-based approach to measuring semantic relatedness in ontologies},year={2011},author={Ahmad Hawalah and Ahmad Hawalah and Maria Fasli and Maria Fasli},doi={10.1145/1988688.1988722},pmid={null},pmcid={null},mag_id={2041122867},journal={null},abstract={The need of determining the degree of semantic similarity, relatedness or distance between two concepts within the same ontology or two different ontologies is becoming an increasingly important task in the field of Information Retrieval. Although a great attention has been paid to design semantic similarity/relatedness methods based on taxonomies, there has been little discussion about the design of semantic similarity/relatedness methods based on ontologies. In this paper we introduce a novel graph-based semantic relatedness approach to calculate semantic relatedness considering both hierarchical and non-hierarchical concepts in an ontology. In addition, our approach considers some important properties such as different relation types, concepts' depth and distance that play an essential role in measuring semantic relatedness. Three experimental studies are provided to first illustrate that our approach give a different results than other measures in the literature. Then, we compare our approach against existed methods using a benchmark dataset, and finally, evaluate our approach by using a real ontology and compare the predictions of our semantic relatedness approach against the human-subject judgment. The results in all the experiments show a considerable improvement against traditional taxonomy-based measures.}}
@ARTICLE{Quaresma_1999,title={A collaborative legal information retrieval system using dynamic logic programming},year={1999},author={Paulo Quaresma and Paulo Quaresma and Paulo Quaresma and Irene Pimenta Rodrigues and Irene Pimenta Rodrigues},doi={10.1145/323706.323795},pmid={null},pmcid={null},mag_id={2041132990},journal={null},abstract={We propose a framework for a collaborative legal information retrieval system based on dynamic logic programming. In order to be collaborative our system keeps the context of the interaction and tries to infer the user intentions. Each event is represented by logic programming facts which are used to dynamically update the previous user model. User intentions are inferred from this new model and are the basis of the interaction between the system and the legal texts knowledge base. As legal texts knowledge base we are using the documents from the Portuguese Attorney General (Procuradoria Geral da Republica). In this paper we will show some examples of the obtained collaborative behaviour.}}
@ARTICLE{Mezghanni_2014,title={Learning of Legal Ontology Supporting the User Queries Satisfaction},year={2014},author={Imen Bouaziz Mezghanni and Imen Bouaziz Mezghanni and Faı̈ez Gargouri and Faiez Gargouri and Faiez Gargouri},doi={10.1109/wi-iat.2014.64},pmid={null},pmcid={null},mag_id={2041349382},journal={null},abstract={In recent years, the development of legal ontologies has increased significantly with the diversity of their applications known as complicated due to the complexity of their domain. In the preliminary part of this paper, we introduce the major steps in the learning process, then we present some works interested in Arabic ontology learning. The rest of the paper serves to propose our approach for ontology learning from Tunisian Legal Texts designed for legal information retrieval. The search process that we suggest exploits the user's profile and uses a query reformulation mechanism based on the learned ontology. The purpose of the system is to satisfy a user's specific retrieval requirement by finding the best response to his request.}}
@ARTICLE{Al-Kofahi_2001,title={A machine learning approach to prior case retrieval},year={2001},author={Khalid Al-Kofahi and Khalid Al-Kofahi and Alex Tyrrell and Alex Tyrrell and Arun Vachher and Arun Vachher and Paul Jackson and Peter Jackson},doi={10.1145/383535.383545},pmid={null},pmcid={null},mag_id={2041422034},journal={null},abstract={We describe a system that processes court opinions and retrieves related cases from a citator database, so that new cases can be linked to earlier ones that they impact. The design of the system combines information extraction, information retrieval and machine learning techniques in a novel way. The fully implemented program is capable of performing prior case retrieval at human levels of recall and acceptable levels of precision.}}
@ARTICLE{Meneely_2012,title={Validating software metrics: A spectrum of philosophies},year={2012},author={Andrew Meneely and Andrew Meneely and Ben Smith and Ben Smith and Laurie Williams and Laurie Williams},doi={10.1145/2377656.2377661},pmid={null},pmcid={null},mag_id={2041628322},journal={ACM Transactions on Software Engineering and Methodology},abstract={Context. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a “valid” metric.   Aim. The debate over what constitutes a valid metric centers on software metrics validation criteria. The objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature.   Method. We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria.   Results. Our 47 validation criteria represent a diverse view of what constitutes a valid metric. We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria.   Conclusions. Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric.}}
@ARTICLE{Bench‐Capon_2009,title={Isomorphism and argumentation},year={2009},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Thomas F. Gordon and Thomas F. Gordon},doi={10.1145/1568234.1568237},pmid={null},pmcid={null},mag_id={2042280927},journal={null},abstract={As knowledge representation tools become more sophisticated, and computer systems increase in power and ubiquity, the prospects of building practical applications based on the representation of large amounts of legislation draw closer. In this paper we reflect on our experience with developing a knowledge representation language for legal rules and an inference engine for this language in the Estrella project, in order to reconsider the principles which should guide the representation of legislation. One common demand, based largely on software engineering considerations relating to maintenance, verification and validation, is that representations should be isomorphic to their sources. We explore this notion by representing a fragment of German Family Law using our tools. We show that there are several different ways of representing even this small and simple fragment of law in an isomorphic fashion. Moreover these differences matter, in terms of where the burden of proof is allocated, in terms of the explanations produced, and in terms of the operational procedures that are reflected.}}
@ARTICLE{Breaux_2011,title={Towards a privacy management framework for distributed cybersecurity in the new data ecology},year={2011},author={Travis D. Breaux and Travis D. Breaux and Catherine Lotrionte and Catherine B. Lotrionte and Catherine B. Lotrionte},doi={10.1109/ths.2011.6107840},pmid={null},pmcid={null},mag_id={2042524378},journal={null},abstract={Cyber security increasingly depends on advance notice of emerging threats as individuals, groups or nations attempt to exfiltrate information or disrupt systems and services. Advance notice relies on having access to the right information at the right time. This information includes trace digital evidence, distributed across public and private networks that are governed by various privacy policies, inter-agency agreements, federal and state laws and international treaties. To enable rapid and assured information sharing that protects privacy, the US government needs a means to balance privacy with the need to share. In this paper, we review US laws and policies governing government surveillance and describe key elements for a privacy management framework that seeks to enable government investigations while protecting privacy in a systematic way. The framework aligns existing Federal investigative guidelines for attributing a cyberattack with concerns for automated decision making that arise from the Fourth Amendment “reasonable expectation of privacy” and several fair information practice principles. We discuss technical challenges for those seeking to implement this framework.}}
@ARTICLE{Keppens_2011,title={On extracting arguments from Bayesian network representations of evidential reasoning},year={2011},author={Jeroen Keppens and Jeroen Keppens},doi={10.1145/2018358.2018380},pmid={null},pmcid={null},mag_id={2042851346},journal={null},abstract={Bayesian networks are a predominant approach to analyse the findings of forensic scientists. In part, this is due to the way the Bayesian approach fits the scientific method employed in forensic practice. The design of Bayesian networks that accurately and comprehensively represent the relationships between investigative hypotheses and evidence remains difficult and sometimes contentious, however. Recent research has shown that argumentation can inform the construction of Bayesian networks. But argumentation is a distinct approach to evidential reasoning with its on representation formalisms. This issue could be alleviated if it were easy to represent Bayesian networks as argumentation diagrams. This position paper presents an investigation into the similarities, differences and synergies between Bayesian networks and argumentation diagrams and shows a first version of an algorithm to extract argumentation diagrams from Bayesian networks.}}
@ARTICLE{Sartor_2006,title={Fundamental legal concepts: a formal and teleological characterisation},year={2006},author={Giovanni Sartor and Giovanni Sartor},doi={10.1007/s10506-006-9009-x},pmid={null},pmcid={null},mag_id={2044223990},journal={Artificial Intelligence and Law},abstract={We shall introduce a set of fundamental legal concepts, providing a definition of each of them. This set will include, besides the usual deontic modalities (obligation, prohibition and permission), the following notions: obligative rights (rights related to other's obligations), permissive rights, erga-omnes rights, normative conditionals, liability rights, different kinds of legal powers, potestative rights (rights to produce legal results), result-declarations (acts intended to produce legal determinations), and sources of the law.}}
@ARTICLE{Grabmair_2015,title={Introducing LUIMA: an experiment in legal conceptual retrieval of vaccine injury decisions using a UIMA type system and tools},year={2015},author={Matthias Grabmair and Matthias Grabmair and Kevin D. Ashley and Kevin D. Ashley and Ran Chen and Ran Chen and Preethi Sureshkumar and Preethi Sureshkumar and Chen Wang and Chen Wang and Eric Nyberg and Eric Nyberg and Vern R. Walker and Vern R. Walker},doi={10.1145/2746090.2746096},pmid={null},pmcid={null},mag_id={2044405664},journal={null},abstract={This paper presents first results from a proof of feasibility experiment in conceptual legal document retrieval in a particular domain (involving vaccine injury compensation). The conceptual markup of documents is done automatically using LUIMA, a law-specific semantic extraction toolbox based on the UIMA framework. The system consists of modules for automatic sub-sentence level annotation, machine learning based sentence annotation, basic retrieval using Apache Lucene and a machine learning based reranking of retrieved documents. In a leave-one-out experiment on a limited corpus, the resulting rankings scored higher for most tested queries than baseline rankings created using a commercial full-text legal information system.}}
@ARTICLE{Browning_1999,title={Sources of Schedule Risk in Complex System Development},year={1999},author={Tyson R. Browning and Tyson R. Browning},doi={10.1002/(sici)1520-6858(1999)2:3<129::aid-sys1>3.0.co;2-h},pmid={null},pmcid={null},mag_id={2045000258},journal={Systems Engineering},abstract={Schedule risk is an important category of risk in complex system product development. This paper presents a framework that facilitates understanding schedule risk from a systems perspective. Research findings from literature and a Delphi-type survey of experienced product development managers and system engineers at a major aerospace company are synthesized into a framework characterizing sources of schedule uncertainty. The framework includes not only key uncertainty drivers but also the hypothesized or theorized relationships between them. Since risk is more than just uncertainty, consequences of schedule overruns and of schedule uncertainty itself are also discussed. This research contributes a more comprehensive, systems view to the studies of product development and risk management and to the practice of both in industry. The paper also examines potential paths for future research. © 1999, John Wiley & Sons, Inc. Syst Eng 3: 129–142, 1999}}
@ARTICLE{Walton_2013,title={Teleological Justification of Argumentation Schemes},year={2013},author={Douglas Walton and Douglas Walton and Giovanni Sartor and Giovanni Sartor},doi={10.1007/s10503-012-9262-y},pmid={null},pmcid={null},mag_id={2045422233},journal={Argumentation},abstract={Argumentation schemes are forms of reasoning that are fallible but correctable within a self-correcting framework. Their use provides a basis for taking rational action or for reasonably accepting a conclusion as a tentative hypothesis, but they are not deductively valid. We argue that teleological reasoning can provide the basis for justifying the use of argument schemes both in monological and dialogical reasoning. We consider how such a teleological justification, besides being inspired by the aim of directing a bounded cognizer to true belief and correct choices, needs to take into account the attitudes of dialogue partners as well as normative models of dialogue and communicative activity types, in particular social and cultural settings.}}
@ARTICLE{Waismann_1965,title={The Principles of Linguistic Philosophy},year={1965},author={Friedrich Waismann and Friedrich Waismann and Rom Harré},doi={null},pmid={null},pmcid={null},mag_id={2046040443},journal={null},abstract={Preface to the Second Edition - Preface to the First Edition - PART 1: THE TRANSITION FROM THE CLASSICAL TO THE LINGUISTIC VIEW OF PHILOSOPHY - The Nature of a Philosophical Problem - Examples of Philosophical Problems and Their Solutions - Is There a priori Knowledge? - Grammatical Models - Appendix to Part 1 - PART 2: ELEMENTS OF A PHILOSOPHICAL GRAMMAR - Introduction - The Causal Interpretation of Language - What is a Rule? - Meaning - Different Types of Explanation - Names - Names of Species - Problems of Communication - Structural Description - What is a Proposition? - The Theory of the Common Structure - Meaning and Verification - Some Remarks on the Concepts 'to be able', 'to know', 'to understand' - Combinations of Propositions - The Logical Calculus - Towards a Logic of Questions - Index}}
@ARTICLE{Branting_1993,title={An issue-oriented approach to judicial document assembly},year={1993},author={L. Karl Branting and L. Karl Branting},doi={10.1145/158976.159005},pmid={null},pmcid={null},mag_id={2046094729},journal={null},abstract={Article An issue-oriented approach to judicial document assembly Share on Author: L. Karl Branting View Profile Authors Info & Claims ICAIL '93: Proceedings of the 4th international conference on Artificial intelligence and lawAugust 1993 Pages 228–235https://doi.org/10.1145/158976.159005Online:01 August 1993Publication History 9citation190DownloadsMetricsTotal Citations9Total Downloads190Last 12 Months6Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access}}
@ARTICLE{Bench‐Capon_1989,title={Deep models, normative reasoning and legal expert systems},year={1989},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/74014.74020},pmid={null},pmcid={null},mag_id={2046828146},journal={null},abstract={This paper discusses the role of deep models and deontic logic in legal expert systems. Whilst much research work insists on the importance of both these features, legal expert systems are being built using shallow models and no more propositional logic, and are claimed to be successful in use. There is then a prima facie conflict between findings of research and commercial practice, which this paper explores, and attempts to explain.}}
@ARTICLE{Robbes_2007,title={A Change-based Approach to Software Evolution},year={2007},author={Romain Robbes and Romain Robbes and Michele Lanza and Michele Lanza},doi={10.1016/j.entcs.2006.06.015},pmid={null},pmcid={null},mag_id={2046882143},journal={Electronic Notes in Theoretical Computer Science},abstract={Software evolution research is limited by the amount of information available to researchers: Current version control tools do not store all the information generated by developers. They do not record every intermediate version of the system issued, but only snapshots taken when a developer commits source code into the repository. Additionally, most software evolution analysis tools are not a part of the day-to-day programming activities, because analysis tools are resource intensive and not integrated in development environments. We propose to model development information as change operations that we retrieve directly from the programming environment the developers are using, while they are effecting changes to the system. This accurate and incremental information opens new ways for both developers and researchers to explore and evolve complex systems.}}
@ARTICLE{Horkoff_2013,title={Comparison and evaluation of goal-oriented satisfaction analysis techniques},year={2013},author={Jennifer Horkoff and Jennifer Horkoff and Eric Yu and Eric Yu},doi={10.1007/s00766-011-0143-y},pmid={null},pmcid={null},mag_id={2047057637},journal={Requirements Engineering},abstract={Goal-oriented requirements engineering (GORE) has been introduced as a means of modeling and understanding the motivations for system requirements. Using models to make goals explicit helps to avoid system failures due to implementing the wrong requirements or ignoring certain stakeholder needs. These models are unique when compared to other models used in system analysis in that their structure naturally lends itself to an analysis of goal satisfaction. Existing work claims that analysis using goal models can facilitate decision making over functional or design alternatives, using criteria in the model. Many different approaches to the analysis of goal-oriented requirements models have been proposed, including several procedures that analyze the satisfaction or denial of goals. These procedures make different choices in their interpretation of the goal model syntax, the methods to resolve conflicting or partial evidence, and in the way they represent satisfaction. This work uses three available tools implementing seven similar goal satisfaction analysis procedures to analyze three sample goal models. Results are reported and compared. The purpose of this comparison is to understand the ways in which procedural design choices affect analysis results, and how differences in analysis results could lead to different recommendations over alternatives in the model. Our comparison shows that different satisfaction analysis techniques for goal models can produce variable results, depending on the structure of the model. Comparison findings lead us to recommend the use of satisfaction analysis techniques for goal models as only heuristics for decision making. Our results emphasize investigation into the benefits of satisfaction analysis beyond decision making, namely improving model quality, increasing domain knowledge, and facilitating communication.}}
@ARTICLE{Lynch_2009,title={Toward assessing law students' argument diagrams},year={2009},author={Collin F. Lynch and Collin Lynch and Kevin D. Ashley and Kevin D. Ashley and Niels Pinkwart and Niels Pinkwart and Vincent Aleven and Vincent Aleven},doi={10.1145/1568234.1568264},pmid={null},pmcid={null},mag_id={2048214958},journal={null},abstract={The development of graphical argument models is an active and growing area of research in Artificial Intelligence and Law. The aim is to develop models which may be readily used by legal professionals and novices to produce and parse arguments. If this goal is to be realized it is important to develop models that human reasoners can manipulate and assess consistently. We report on an ongoing study of graph agreement in the context of the LARGO system.}}
@ARTICLE{Hage_2000,title={Law, Morals and Defeasibility},year={2000},author={Jaap Hage and Jaap Hage and Aleksander Peczenik and Aleksander Peczenik},doi={10.1111/1467-9337.00158},pmid={null},pmcid={null},mag_id={2048379900},journal={Ratio Juris},abstract={This paper gives a logical characterization of the interrelation between law and morals. To this purpose it first outlines a logic for defeasible reasoning with rules and principles and illustrates the operation of this logic in the field of law. Then it offers a brief argument why law and morals are interrelated. This paper ends by showing how the logic for defeasible reasoning provides tools to logically characterize some aspects of the interrelation between law and morals.}}
@ARTICLE{Liebowìtz_1986,title={Expert systems in law: a survey and case study},year={1986},author={Jay Liebowìtz and Jay Liebowitz},doi={10.1016/s0736-5853(86)80063-6},pmid={null},pmcid={null},mag_id={2050723745},journal={Telematics and Informatics},abstract={null}}
@ARTICLE{Lehman_2001,title={An approach to a theory of software evolution},year={2001},author={M. M. Lehman and Meir M. Lehman and Juan F. Ramil and Juan F. Ramil},doi={10.1145/602461.602473},pmid={null},pmcid={null},mag_id={2051315303},journal={null},abstract={Summary form only given. The paper briefly refers to a number of the, by now well known, results of the author's studies of software evolution since they provide a basis and framework for the development of a theory of the phenomenon. The author then summarises his most recent results outlining a proof that every E-type program reflects an unbounded number of assumptions about the application implemented, supported or modelled by the program. He shows that the presence of such assumptions is inevitable and that some of these become invalid over time as a consequence of changes in the dynamic real world. Some of the finite set of known assumptions reflected in the program is also become invalid. Together the resultant ever extending invalidity causes the software to require continuing change or to become ever more unsatisfactory or even invalid. This phenomenon provides the underlying and unavoidable cause of the universal experience that E-type software must be continually evolved to remain satisfactory and suggests that its further study and the development of methods and tools to reduce its considerable impact, economic and social cost, must form an important part of future software engineering R & D.}}
@ARTICLE{Lau_2006,title={Locating related regulations using a comparative analysis approach},year={2006},author={Gloria T. Lau and Gloria T. Lau and Haoyi Wang and Haoyi Wang and Kincho H. Law and Kincho H. Law},doi={10.1145/1146598.1146662},pmid={null},pmcid={null},mag_id={2051983405},journal={null},abstract={The sheer volume and complexity of government regulations make any attempt to locate, understand and interpret the information a daunting task. Other factors, such as the scattered distribution of the regulations across many sources, different terminologies and cross referencing, further complicate the technical issues in developing a regulation information management system. This paper describes a comparative analysis approach and its potential application to assist locating relevant regulations from different sources. Examples from environmental regulations are employed to illustrate the proposed methodology and framework.}}
@ARTICLE{Nadah_2007,title={Licensing digital content with a generic ontology: escaping from the jungle of rights expression languages},year={2007},author={Nadia Nadah and Nadia Nadah and Mélanie Dulong de Rosnay and Melanie Dulong de Rosnay and Bruno Bachimont and Bruno Bachimont},doi={10.1145/1276318.1276330},pmid={null},pmcid={null},mag_id={2052161931},journal={null},abstract={Digital contents distributed over the internet are regulated by law and by technical management systems. The latter include a semantic component that describes licenses, i.e. rights of use which are granted to the user. These elements of Digital Rights Management (DRM) systems are called Rights Expression Languages (REL), they gather terms and relations needed to build licenses. Some are based on an ontology of online licenses, not necessarily related to applicable law and various legal systems, and cannot interoperate. As a consequence, there is a need for a more generic way to express licenses. Here, generic means that rightholders should only need to express the license they need once, and semi-automatic tools should then translate this license so it can be browsed by any specific system. Hence it implies the necessity to be able to model concept semantics in order to translate a license expressed in generic terms into more specific terms that are compliant with the specific standards used by distribution systems. This work comes as part of larger studies on legal ontologies, legal systems and RELs.}}
@ARTICLE{Rissland_1995,title={Detecting change in legal concepts},year={1995},author={Edwina L. Rissland and Edwina L. Rissland and Menahem Friedman and M. Timur Friedman},doi={10.1145/222092.222209},pmid={null},pmcid={null},mag_id={2052351877},journal={null},abstract={Article Free Access Share on Detecting change in legal concepts Authors: Edwina L. Rissland Department of Computer Science, University of Massachusetts, Amherst, MA Department of Computer Science, University of Massachusetts, Amherst, MAView Profile , M. Timur Friedman Department of Computer Science, University of Massachusetts, Amherst, MA Department of Computer Science, University of Massachusetts, Amherst, MAView Profile Authors Info & Claims ICAIL '95: Proceedings of the 5th international conference on Artificial intelligence and lawMay 1995 Pages 127–136https://doi.org/10.1145/222092.222209Online:24 May 1995Publication History 25citation356DownloadsMetricsTotal Citations25Total Downloads356Last 12 Months9Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF}}
@ARTICLE{Smith_2010,title={Challenges for protecting the privacy of health information: required certification can leave common vulnerabilities undetected},year={2010},author={Ben Smith and Ben Smith and Andrew C. M. Austin and Andrew Austin and M.J.C. Brown and Matt Brown and Jason King and Jason King and Jerrod Lankford and Jerrod Lankford and Jerrod L. Lankford and Andrew Meneely and Andrew Meneely and Laurie Williams and Laurie Williams},doi={10.1145/1866914.1866916},pmid={null},pmcid={null},mag_id={2052539391},journal={null},abstract={The use of electronic health record (EHR) systems by medical professionals enables the electronic exchange of patient data, yielding cost and quality of care benefits. The United States American Recovery and Reinvestment Act (ARRA) of 2009 provides up to $34 billion for meaningful use of certified EHR systems. But, will these certified EHR systems provide the infrastructure for secure patient data exchange? As a window into the ability of current and emerging certification criteria to expose security vulnerabilities, we performed exploratory security analysis on a proprietary and an open source EHR. We were able to exploit a range of common code-level and design-level vulnerabilities. These common vulnerabilities would have remained undetected by the 2011 security certification test scripts from the Certification Commission for Health Information Technology, the most widely used certification process for EHR systems. The consequences of these exploits included, but were not limited to: exposing all users' login information, the ability of any user to view or edit health records for any patient, and creating a denial of service for all users. Based upon our results, we suggest that an enhanced set of security test scripts be used as entry criteria to the EHR certification process. Before certification bodies spend the time to certify that an EHR application is functionally complete, they should have confidence that the software system meets a basic level of security competence.}}
@ARTICLE{Al-Kofahi_1999,title={Anaphora resolution in the extraction of treatment history language from court opinions by partial parsing},year={1999},author={Khalid Al-Kofahi and Khalid Al-Kofahi and Brian Grom and Brian Grom and Brian Grom and Brian Grom and Paul Jackson and Peter Jackson},doi={10.1145/323706.323788},pmid={null},pmcid={null},mag_id={2052604718},journal={null},abstract={This paper describes an information extraction system that identifies and analyzes statements in court opinions regarding the value of cited cases as precedents. The system employs partial parsing techniques in conjunction with a semantic grammar to identify the language associated with such rulings. The most novel aspect of the system lies in its anaphora resolution module that combines syntactic, semantic, and domain-specific inference rules with local discourse information to link such language to case references.}}
@ARTICLE{Lesmo_2009,title={Extracting semantic annotations from legal texts},year={2009},author={Leonardo Lesmo and Leonardo Lesmo and Alessandro Mazzei and Alessandro Mazzei and Daniele Paolo Radicioni and Daniele P. Radicioni},doi={10.1145/1557914.1557944},pmid={null},pmcid={null},mag_id={2053871624},journal={null},abstract={This paper illustrates a system designed to automatically extract semantic annotations of the normative modifications present in legal texts. The work relies on a deep parsing approach. The problem of semantically annotating legal texts is cast to the problem of mapping parse trees to semantic frames representing such modifications. We report a preliminary experimentation along with the dataset employed, and discuss the results to point out future improvements.}}
@ARTICLE{Wooldridge_2005,title={On obligations and normative ability: Towards a logical analysis of the social contract},year={2005},author={Michael Wooldridge and Michael Wooldridge and Wiebe van der Hoek and Wiebe van der Hoek},doi={10.1016/j.jal.2005.04.006},pmid={null},pmcid={null},mag_id={2054414141},journal={Journal of Applied Logic},abstract={null}}
@ARTICLE{Wardeh_2013,title={Argumentation based tools for policy-making},year={2013},author={Maya Wardeh and Maya Wardeh and Adam Wyner and Adam Wyner and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/2514601.2514640},pmid={null},pmcid={null},mag_id={2054468359},journal={null},abstract={Citizens have a variety of ways to consult with their representatives about policy proposals, seeking justifications, objecting to all or part of it, or making a counter-proposal. For the first, the representative needs only to state a justification. For the second, the representative would want to understand the objections, which may involve asking some questions. For the third, the citizen would have to provide a well formulated proposal that can then be critiqued from the standpoint of the government's own policy proposal. At the end of such a consultation, users will have aired their proposals, understood the implications, and received feedback on how their proposals contrast to that of the government.}}
@ARTICLE{Belew_1987,title={A connectionist approach to conceptual information retrieval},year={1987},author={Richard K. Belew and Richard K. Belew},doi={10.1145/41735.41749},pmid={null},pmcid={null},mag_id={2054525468},journal={null},abstract={This report proposes that recent advances using low-level connectionist representations offer new possibilities to those interested in free text information retrieval (IR). The AIR system demonstrates that this representation suits the IR domain well, particularly the special problems attending the more sophisticated forms of conceptual retrieval required in legal applications. Also, the natural way in which connectionist representations allow  learning  means that AIR can avoid the high costs associated with manual indexing while providing comparable results. The paper begins by motivating the importance of legal information retrieval, from the perspectives of both the Law and artificial intelligence (AI). Our approach is then compared to traditional methods for IR, and to more recent work using higher-level  symbolic  representations from AL After a brief introduction to connectionist representations in general, the AIR system is presented. The paper closes with evidence that this system does, in fact, begin to support the use of those “open textured” concepts that make the Law both a very difficult and a very illuminating domain for AI research.}}
@ARTICLE{Susskind_1987,title={Expert systems in law: out of the research laboratory and into the marketplace},year={1987},author={Richard Susskind and R. E. Susskind},doi={10.1145/41735.41736},pmid={null},pmcid={null},mag_id={2055425697},journal={null},abstract={The major goal of workers in the field of expert systems in law can be summarized as follows: through the use of computer technology, to make scarcs, human legal knowledge and expertise more widely available and easily accessible. This paper is directly concerned with that goal. It is divided into two parts. The first discusses a three year interdisciplinary research project in expert systems in law that involved the Law Faculty and the Programming Research Group of the University of Oxford. Some of the jurisprudential aspects of the project are discussed and the prototype system that was developed is described. The second part of the paper addresses various considerations that are crucial for bringing expert systems in law out of the research laboratory and into the marketplace. And in this latter connection, the author's latest work - with Ernst & Whinney - is introduced.}}
@ARTICLE{Wyner_2012,title={A legal case OWL ontology with an instantiation of Popov v. Hayashi},year={2012},author={Adam Wyner and Adam Wyner and Rinke Hoekstra and Rinke Hoekstra},doi={10.1007/s10506-012-9119-6},pmid={null},pmcid={null},mag_id={2055912896},journal={Artificial Intelligence and Law},abstract={The paper provides an OWL ontology for legal cases with an instantiation of the legal case Popov v. Hayashi. The ontology makes explicit the conceptual knowledge of the legal case domain, supports reasoning about the domain, and can be used to annotate the text of cases, which in turn can be used to populate the ontology. A populated ontology is a case base which can be used for information retrieval, information extraction, and case based reasoning. The ontology contains not only elements for indexing the case (e.g. the parties, jurisdiction, and date), but as well elements used to reason to a decision such as argument schemes and the components input to the schemes. We use the Protege ontology editor and knowledge acquisition system, current guidelines for ontology development, and tools for visual and linguistic presentation of the ontology.}}
@ARTICLE{Keppens_2011,title={Compositional Bayesian modelling for computation of evidence collection strategies},year={2011},author={Jeroen Keppens and Jeroen Keppens and Qiang Shen and Qiang Shen and Chris Price and Chris Price},doi={10.1007/s10489-009-0208-5},pmid={null},pmcid={null},mag_id={2055991537},journal={Applied Intelligence},abstract={As forensic science and forensic statistics become increasingly sophisticated, and judges and juries demand more timely delivery of more convincing scientific evidence, crime investigation is becoming progressively more challenging. In particular, this development requires more effective and efficient evidence collection strategies, which are likely to produce the most conclusive information with limited available resources. Evidence collection is a difficult task, however, because it necessitates consideration of: a wide range of plausible crime scenarios, the evidence that may be produced under these hypothetical scenarios, and the investigative techniques that can recover and interpret the plausible pieces of evidence. A knowledge based system (KBS) can help crime investigators by retrieving and reasoning with such knowledge, provided that the KBS is sufficiently versatile to infer and analyse a wide range of plausible scenarios. This paper presents such a KBS. It employs a novel compositional modelling technique that is integrated into a Bayesian model based diagnostic system. These theoretical developments are illustrated by a realistic example of serious crime investigation.}}
@ARTICLE{Hilts_2012,title={Design and evaluation of the goal-oriented design knowledge library framework},year={2012},author={Andrew Hilts and Andrew Hilts and Eric Yu and Eric Yu},doi={10.1145/2132176.2132226},pmid={null},pmcid={null},mag_id={2056653358},journal={null},abstract={In this paper, we describe the motivation, development, features, and evaluation of the Goal-Oriented Design Knowledge Library (GO-DKL) framework. This framework encompasses a model and method for extracting, codifying and storing in a database, relational excerpts of design knowledge from scholarly publications. We also outline a method for analyzing such a knowledge base, which is intended to support information system designers to retrieve, contextualize and evaluate knowledge base contents in relation to their own unique projects. Such a system may help designers build on each others' ideas and support an emerging community of practice. The framework was evaluated by six information system design practitioners during semi-structured interviews. For this project, we codified design knowledge published in academic studies of online deliberation systems.}}
@ARTICLE{Feigin_1986,title={Intergroup Diversity and Concordance for Ranking Data: An Approach via Metrics for Permutations},year={1986},author={Paul D. Feigin and Paul D. Feigin and Mayer Alvo and Mayer Alvo},doi={10.1214/aos/1176349947},pmid={null},pmcid={null},mag_id={2056813207},journal={Annals of Statistics},abstract={Motivated by the apportionment of diversity analysis due to C. R. Rao, a general approach to comparing populations of rankers is proposed. Each permutation metric corresponds to a particular population characteristic that forms the basis of the comparison. Tests of hypotheses concerning equality of characteristics are developed. Throughout, comparison is made with earlier work, most of which is based on the use of only the Spearman metric. Extension to tied rankings is discussed. Examples for two groups are presented which illustrate the computational feasibility as well as the value of the proposed procedures.}}
@ARTICLE{Horty_1999,title={Precedent, deontic logic, and inheritance},year={1999},author={John F. Horty and John F. Horty},doi={10.1145/323706.323716},pmid={null},pmcid={null},mag_id={2056870591},journal={null},abstract={The purpose of this paper is to establish some connections between precedent-based reasoning as it is studied in the field of Artificial Intelligence and Law, particularly in the work of Ashley, and two other fields: deontic logic and nonmonotonic logic. First, a deontic logic is described that allows for sensible reasoning in the presence of conflicting norms. Second, a simplified version of Ashley's account of precedent-based reasoning is reformulated within the framework of this deontic logic. Finally, some ideas from the theory of nonmonotonic inheritance are employed to show how Ashley's account might be elaborated to allow for a richer representation of the process of argumentation.}}
@ARTICLE{Baron_2007,title={The search problem posed by large heterogeneous data sets in litigation: possible future approaches to research},year={2007},author={Jason R. Baron and Jason R. Baron and Jason R. Baron and Paul Thompson and Paul Thompson},doi={10.1145/1276318.1276344},pmid={null},pmcid={null},mag_id={2057511617},journal={null},abstract={Lawyers and their large institutional clients increasingly face the enormous problem of how to efficiently and efficaciously conduct searches for relevant documents in large heterogeneous electronic data sets, for the purpose of responding to litigation demands. Past research indicates that lawyers greatly overestimate their true rate of recall in civil discovery. The unprecedented size, scale, and complexity of electronically stored data now potentially subject to routine capture in litigation, for purpose of preservation, access, and review, presents information retrieval researchers with a series of important challenges to overcome. This paper describes the current context of e-discovery and discusses the potential for IR and AI research to address the challenges of conducting e-discovery. The TREC Legal Track is presented as a forum for the evaluation of e-discovery research and one new evaluation measure, elusion, is described, which has potential for addressing problems of measuring recall.}}
@ARTICLE{Grabmair_2013,title={Using event progression to enhance purposive argumentation in the value judgment formalism},year={2013},author={Matthias Grabmair and Matthias Grabmair and Kevin D. Ashley and Kevin D. Ashley},doi={10.1145/2514601.2514610},pmid={null},pmcid={null},mag_id={2057761729},journal={null},abstract={This paper expands on the previously published value judgment formalism. The representation of situations is enhanced by introducing event progressions similar to actions in general AI planning. Using event progressions, situations can be assessed as to what facts they contain as well as what facts may ensue with some likelihood, thereby opening up a situation space. Purposive legal argumentation can be modeled using propositions and rules controlling the likelihoods of value-laden consequences. The paper expands the formalism to cover event progressions and illustrates the functionality using an example based on Young v. Hitchens.}}
@ARTICLE{Moulin_1990,title={Knowledge acquisition from prescriptive texts},year={1990},author={Bernard Moulin and Bernard Moulin and Daniel Rousseau and Daniel Rousseau},doi={10.1145/98894.99136},pmid={null},pmcid={null},mag_id={2058483539},journal={null},abstract={There is a growing interest for the application of artificial intelligence in law. Research activities have investigated different areas : formulating legislation with the aid of logical models, legal reasoning, case-based reasoning, developing expert systems applied to the juridical or administrative domains. In project A.C.A.T. (Acquisition des connaissances et analyse de textes), we explore the possibility of creating knowledge bases by exploiting information contained in texts which are used in organizations. Our research focuses on a particular category of prescriptive texts : regulations from the Government of Quebec.  In order to verify these hypothesis we are developing a knowledge-acquisition system which will enable human specialists to transform a prescriptive text into the form of a knowledge base which can be exploited by an inference engine.  We introduce a model which enables us to identify three layers in prescriptive texts : the macrostructure, the microstructure and the dominial component. We describe the general architecture of the knowledge acquisition system which enables us to create “deontic” knowledge bases. We present the main knowledge structures used by the knowledge acquisition sub-system : the text grammars of macrostructure and microstructure.}}
@ARTICLE{Gacitúa_2010,title={On the Effectiveness of Abstraction Identification in Requirements Engineering},year={2010},author={Ricardo Gacitúa and Ricardo Gacitua and Pete Sawyer and Pete Sawyer and Vincenzo Gervasi and Vincenzo Gervasi},doi={10.1109/re.2010.12},pmid={null},pmcid={null},mag_id={2060526416},journal={null},abstract={The identification of abstractions, i.e. terms that have a particular significance in a given domain, and such that they can indirectly characterize the most salient features of the document in which they appear, has often been recognized as a useful tool in the analysis of domain descriptions and requirements documents in software development. In this paper we propose a new technique for the identification of single- and multi-word abstractions named Relevance driven abstraction identification (RAI) and a corresponding tool implementation, present an experiment comparing the effectiveness of our technique with human judgement and with a different technique proposed in the literature, and discuss a number of ways in which the abstractions so identified can be used to good profit in requirements engineering.}}
@ARTICLE{Atkinson_2013,title={Distinctive features of persuasion and deliberation dialogues},year={2013},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Douglas Walton and Douglas Walton},doi={10.1080/19462166.2012.708670},pmid={null},pmcid={null},mag_id={2060774598},journal={Argument & Computation},abstract={The distinction between action persuasion dialogues and deliberation dialogues is not always obvious at first sight. In this paper, we provide a characterisation of both types of dialogues that draws out the distinctive features of each. It is important to recognise the distinctions since participants in both types of dialogues will have different aims, which in turn affects whether a successful outcome can be reached. Such dialogues are typically conducted by exchanging arguments for and against certain options. The moves of the dialogue are designed to facilitate such exchanges. In particular, we show how the pre- and post-conditions for the use of particular moves in the dialogues are very different depending upon whether they are used as part of a persuasion over action or a deliberation dialogue. We draw out the distinctions with reference to a running example that we also present as a logic program in order to give a clear characterisation of the two types of dialogues, which is intended to enable t...}}
@ARTICLE{Walton_2014,title={Baseballs and arguments from fairness},year={2014},author={Douglas Walton and Douglas Walton},doi={10.1007/s10506-013-9151-1},pmid={null},pmcid={null},mag_id={2061085701},journal={Artificial Intelligence and Law},abstract={This paper applies two argumentation schemes, argument from fairness and argument from lack of knowledge (along with other schemes of lesser prominence) to model the reasoning given by Judge McCarthy supporting his decision to divide the proceeds of a homerun baseball in the case of Popov v. Hayashi. Several versions of both schemes are explained and discussed, and then applied to the argumentation given by Judge McCarthy as the basis of the reasoning used to arrive at his decision. The scheme for argument from fairness is shown to be based on a special principle in Perelman's theory of justice.}}
@ARTICLE{Barzilay_2003,title={Sentence alignment for monolingual comparable corpora},year={2003},author={Regina Barzilay and Regina Barzilay and Noémie Elhadad and Noémie Elhadad},doi={10.3115/1119355.1119359},pmid={null},pmcid={null},mag_id={2061235289},journal={null},abstract={We address the problem of sentence alignment for monolingual corpora, a phenomenon distinct from alignment in parallel corpora. Aligning large comparable corpora automatically would provide a valuable resource for learning of text-to-text rewriting rules. We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs. Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task.}}
@ARTICLE{Chorley_2003,title={Developing legal knowledge based systems through theory construction},year={2003},author={Alison Chorley and Alison Chorley and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1145/1047788.1047805},pmid={null},pmcid={null},mag_id={2062066019},journal={null},abstract={In this note we report a project exploring the notion of reasoning with legal cases as a process of theory construction, evaluation and application, as described in work by Bench-Capon and Sartor.}}
@ARTICLE{Keppens_2006,title={Knowledge based crime scenario modelling},year={2006},author={Jeroen Keppens and Jeroen Keppens and Burkhard Schäfer and Burkhard Schafer},doi={10.1016/j.eswa.2005.07.011},pmid={null},pmcid={null},mag_id={2062723412},journal={Expert Systems With Applications},abstract={null}}
@ARTICLE{Grobe_2009,title={RDF, Jena, SparQL and the 'Semantic Web'},year={2009},author={Michael Grobe and Michael Grobe},doi={10.1145/1629501.1629525},pmid={null},pmcid={null},mag_id={2062759953},journal={null},abstract={The Resource Description Format (RDF) is used to represent information modeled as a "graph": a set of individual objects, along with a set of connections among those objects. In that role, RDF is one of the pillars of the so-called Semantic Web. This paper describes how RDF-XML is used to serialize information represented using graphs, how RDF graphs can be read and written by using the Jena software package, and how distributed graphs can be queried using the SparQL query language. It includes examples showing how SparQL can be used to query data (such as the Gene Ontology) that is structured in hierarchies, and how SparQL queries can be submitted through SparQL endpoints. It does not, however, delve into inference or the Web Ontology Language (OWL), but should provide a foundation for understanding those topics.}}
@ARTICLE{Bueno_2003,title={Analyzing the use of dynamic weights in legal case based system},year={2003},author={Tânia C. D’Agostini Bueno and Tania C. D. Bueno and Tania C. D. Bueno and André Bortolon and Andre Bortolon and Hugo César Hoeschl and Hugo Cesar Hoeschl and Eduardo S. Mattos and Eduardo da Silva Mattos and Marcelo Ribeiro and Marcelo S. Ribeiro},doi={10.1145/1047788.1047822},pmid={null},pmcid={null},mag_id={2065867532},journal={null},abstract={This paper presents a case-based system to retrieve legal statements of Brazilian High Courts that emphasizes the use of theoretical structures to represent cases. AlphaThemis system has the characteristics from other case-based systems. The major innovation is to allow the user modify the weights used to determine the importance of each feature in similarity measure. This innovation is called dynamic weights. Their objective is to improve the retrieval accuracy in systems that have a human expert supporting the case base creation. In this paper, we analyze the recall and usefulness of dynamic weights, suggesting a new calibration of attribute relevance to achieve the their total efficiency.}}
@ARTICLE{Masoudnia_2014,title={Mixture of experts: a literature survey},year={2014},author={Saeed Masoudnia and Saeed Masoudnia and Reza Ebrahimpour and Reza Ebrahimpour},doi={10.1007/s10462-012-9338-y},pmid={null},pmcid={null},mag_id={2066334462},journal={Artificial Intelligence Review},abstract={Mixture of experts (ME) is one of the most popular and interesting combining methods, which has great potential to improve performance in machine learning. ME is established based on the divide-and-conquer principle in which the problem space is divided between a few neural network experts, supervised by a gating network. In earlier works on ME, different strategies were developed to divide the problem space between the experts. To survey and analyse these methods more clearly, we present a categorisation of the ME literature based on this difference. Various ME implementations were classified into two groups, according to the partitioning strategies used and both how and when the gating network is involved in the partitioning and combining procedures. In the first group, The conventional ME and the extensions of this method stochastically partition the problem space into a number of subspaces using a special employed error function, and experts become specialised in each subspace. In the second group, the problem space is explicitly partitioned by the clustering method before the experts' training process starts, and each expert is then assigned to one of these sub-spaces. Based on the implicit problem space partitioning using a tacit competitive process between the experts, we call the first group the mixture of implicitly localised experts (MILE), and the second group is called mixture of explicitly localised experts (MELE), as it uses pre-specified clusters. The properties of both groups are investigated in comparison with each other. Investigation of MILE versus MELE, discussing the advantages and disadvantages of each group, showed that the two approaches have complementary features. Moreover, the features of the ME method are compared with other popular combining methods, including boosting and negative correlation learning methods. As the investigated methods have complementary strengths and limitations, previous researches that attempted to combine their features in integrated approaches are reviewed and, moreover, some suggestions are proposed for future research directions.}}
@ARTICLE{Walton_2013,title={Argument from analogy in legal rhetoric},year={2013},author={Douglas Walton and Douglas Walton},doi={10.1007/s10506-013-9139-x},pmid={null},pmcid={null},mag_id={2066660796},journal={Artificial Intelligence and Law},abstract={This paper applies recent work on scripts and stories developed as tools of evidential reasoning in artificial intelligence to model the use of argument from analogy as a rhetorical device of persuasion. The example studied is Gerry Spence's closing argument in the case of Silkwood v. Kerr-McGee Corporation, said to be the most persuasive closing argument ever used in an American trial. It is shown using this example how argument from analogy is based on a similarity premise where similarity between two cases is modeled using the device of a story scheme from the hybrid theory of legal evidential reasoning (Bex in Arguments, stories and criminal evidence: a formal hybrid theory. Springer, Dordrecht 2011). It is shown how the rhetorical strategy of Spence's argumentation in the closing argument interweaves argument from analogy with explanation through three levels.}}
@ARTICLE{Gordon_1987,title={Some problems with prolog as a knowledge representation language for legal expert systems},year={1987},author={Thomas F. Gordon and Thomas F. Gordon},doi={10.1080/13600869.1987.9966253},pmid={null},pmcid={null},mag_id={2066673954},journal={International Review of Law, Computers & Technology},abstract={(1987). Some problems with prolog as a knowledge representation language for legal expert systems. International Review of Law, Computers & Technology: Vol. 3, No. 1, pp. 52-67.}}
@ARTICLE{Andersson_2016,title={Do no harm: the balance of "beneficence" and "non-maleficence"},year={2016},author={Gunnar Andersson and Gunnar Andersson and Gunnar B. J. Andersson and Jens R. Chapman and Jens R. Chapman and Mark B. Dekutoski and Mark B. Dekutoski and Joseph R Dettori and Joseph R Dettori and Joseph R. Dettori and Michael G. Fehlings and Michael G. Fehlings and Daryl R. Fourney and Daryl R. Fourney and Dan Norvell and Dan Norvell and James Neil Weinstein and James Neil Weinstein},doi={null},pmid={20407347},pmcid={null},mag_id={2068400936},journal={null},abstract={“… Above all do no harm” launches a career in medicine and throughout their career compels the physician to act in the patient's best interest. It can be said that the potential for complication, its anticipation, and need for mutual trust makes the patient-physician relationship, so unique.Comp}}
@ARTICLE{Zhang_2007,title={Economic Consequences of the Sarbanes-Oxley Act of 2002},year={2007},author={Ivy Xiying Zhang and Ivy Xiying Zhang},doi={10.1016/j.jacceco.2007.02.002},pmid={null},pmcid={null},mag_id={2068600196},journal={Journal of Accounting and Economics},abstract={This paper investigates the economic consequences of the Sarbanes-Oxley Act (SOX) by examining market reactions to related legislative events. Using concurrent stock returns of non-U.S.-traded foreign firms to estimate normal U.S. returns, I find that U.S. firms experienced a statistically significant negative cumulative abnormal return around key SOX events. I then examine the cross-sectional variation of U.S. firms' returns around these events. Regression results are consistent with the nonaudit services and governance provisions imposing net costs. Additional tests show that deferring the compliance of Section 404, which mandates an internal control test, resulted in significant cost savings for nonaccelerated filers.}}
@ARTICLE{Hage_1995,title={Teleological reasoning in reason-based logic},year={1995},author={Jaap Hage and Jaap Hage},doi={10.1145/222092.222098},pmid={null},pmcid={null},mag_id={2069591255},journal={null},abstract={null}}
@ARTICLE{Kavakli_2002,title={Goal-Oriented Requirements Engineering: A Unifying Framework},year={2002},author={Evangelia Kavakli and Evangelia Kavakli},doi={10.1007/pl00010362},pmid={null},pmcid={null},mag_id={2069673757},journal={Requirements Engineering},abstract={The study of contemporary requirements engineering (RE) methodologies indicates that modelling of organisational goals constitutes a central activity of the RE process. In particular, goals provide the rationale and drivethe elaboration of the requirements that operationalise them. They also provide the criteria against which the completeness and correctness of the requirements specification is validated. In other words, requirements implement goals in the same way that programs implement design specifications. Despite the significance of goals in RE, research in the field is fragmented. No research has so far taken place in order to define the overall role that goals play in RE. This paper puts forward a unifying view of goal analysis in the context of RE. This allows the identification of similarities and differences between the different conceptions of goal used by different approaches and promotes the understanding of the overall role of goal analysis in RE. Based on this understanding the various approaches can be put together, thus leading to a stronger goal-driven RE framework that takes advantage of the contributions from the many streams of goal-oriented research.}}
@ARTICLE{Polinsky_2006,title={Optimal Fines and Auditing When Wealth is Costly to Observe},year={2006},author={Maria Polinsky and A. Mitchell Polinsky},doi={10.2139/ssrn.583923},pmid={null},pmcid={null},mag_id={2070396650},journal={International Review of Law and Economics},abstract={This article studies optimal fines when an offender's wealth is private information that can be obtained by the enforcement authority only after a costly audit. I derive the optimal fine for the underlying offense, the optimal fine for misrepresenting one's wealth level, and the optimal audit probability. I demonstrate that the optimal fine for misrepresenting wealth equals the fine for the offense divided by the audit probability, and therefore generally exceeds the fine for the offense. The optimal audit probability is positive, increases as the cost of an audit declines, and equals unity if the cost is sufficiently low. If the optimal audit probability is less than unity, there are some individuals who are capable of paying the fine for the offense who misrepresent their wealth levels. I also show that the optimal fine for the offense results in underdeterrence due to the cost of auditing wealth levels.}}
@ARTICLE{Modgil_2013,title={A general account of argumentation with preferences},year={2013},author={Sanjay Modgil and Sanjay Modgil and Henry Prakken and Henry Prakken},doi={10.1016/j.artint.2012.10.008},pmid={null},pmcid={null},mag_id={2071122890},journal={Artificial Intelligence},abstract={This paper builds on the recent ASPIC^+ formalism, to develop a general framework for argumentation with preferences. We motivate a revised definition of conflict free sets of arguments, adapt ASPIC^+ to accommodate a broader range of instantiating logics, and show that under some assumptions, the resulting framework satisfies key properties and rationality postulates. We then show that the generalised framework accommodates Tarskian logic instantiations extended with preferences, and then study instantiations of the framework by classical logic approaches to argumentation. We conclude by arguing that ASPIC^+@?s modelling of defeasible inference rules further testifies to the generality of the framework, and then examine and counter recent critiques of Dung@?s framework and its extensions to accommodate preferences.}}
@ARTICLE{Lévy_2010,title={An Environment for the Joint Management of Written Policies and Business Rules},year={2010},author={François Lévy and François Lévy and Abdooulaye Guissé and Abdoulaye Guissé and Adeline Nazarenko and Adeline Nazarenko and Nouha Omrane and Nouha Omrane and Sylvie Szulman and Sylvie Szulman},doi={10.1109/ictai.2010.95},pmid={null},pmcid={null},mag_id={2071418399},journal={null},abstract={The contemporary world produces huge bodies of policies and regulations, while the underlying procedures tend to be automated in decision systems, which are designed to define, deploy, execute, monitor and maintain the various rules to which an organization or enterprise has to comply. It is important that the written documentation is integrated into such decision systems in order to refer to the texts to explain decisions, to update the systems when the policy evolves or, conversely, to amend the source documents if some of the rules happen to be inconsistent. The problem is that the complexity of information to be searched for is not reachable by an automated processing, but that their volume prohibits a manual one. Arguing that the integration of policies in decision systems can be better achieved through a semantic annotation than by the full parsing of the source documentation, this paper presents a technical environment that enables the building and exploitation of such semantic annotations.}}
@ARTICLE{Berman_1991,title={Incorporating procedural context into a model of case-based legal reasoning},year={1991},author={Donald H. Berman and Donald H. Berman and Carole D. Hafner and Carole D. Hafner},doi={10.1145/112646.112648},pmid={null},pmcid={null},mag_id={2071985461},journal={null},abstract={In this paper we analyze the procedural considerations that affect the use of legal casesas precedents and propose a model of procedural knowledge that can be combined with substantive legal reasoning models to produce a more robust theory of case-based legal reasoning in common law jurisdictions. Our model addresses one component of procedural knowledge the distinction between questions of fact and questions of law. We categorize 32 different procedural scenarios into 10 basic types of legal results. We then propose rules for determining the precedential value of these result types. Finally we suggest a method for incorporating procedural distinctions into case-based reasoning systems.}}
@ARTICLE{Arthasartsri_2009,title={Validation and verification methodologies in A380 aircraft reliability program},year={2009},author={Supanee Arthasartsri and Supanee Arthasartsri and He Ren and He Ren},doi={10.1109/icrms.2009.5270030},pmid={null},pmcid={null},mag_id={2072029960},journal={null},abstract={Reliability Validation and Verification (V&V) has become a major issue in every industry nowadays. In the development of an embedded system, it is important to be able to determine if the system meets specifications and if its outputs are correct. This is the process of V&V. Airbus applied this V&V process in safety assessment on A380 from aircraft level to supplier level. Aircraft level safety process was implemented for the first time on a large-scale commercial aircraft. To ensure the achievement of goal reliability before entry into service, Airbus has put in more tests on A380 beyond the safety requirements from FAA. This paper presents the methodology of V&V process in A380 aircraft reliability program with the example case study of how Engine Alliance implemented this process in GP7200 design for safety and reliability.}}
@ARTICLE{Yoshino_1995,title={The systematization of legal meta-inference},year={1995},author={Hideaki Yoshino and Hajime Yoshino},doi={10.1145/222092.222257},pmid={null},pmcid={null},mag_id={2073277235},journal={null},abstract={null}}
@ARTICLE{Miseldine_2008,title={Supporting Evidence-Based Compliance Evaluation for Partial Business Process Outsourcing Scenarios},year={2008},author={Philip Miseldine and Philip Miseldine and Ulrich Flegel and U. Flegel and Andreas Schaad and Andreas Schaad},doi={10.1109/relaw.2008.9},pmid={null},pmcid={null},mag_id={2073736287},journal={null},abstract={We present the challenges facing businesses wishing to outsource processes to service providers who must maintain regulatory compliance via data access control procedures. We argue that it is not currently possible to capture the necessary agreements, and supporting evidence, pertaining to the usage of data a client may send to a service provider. As a result, the richness of evidence and controls a client has available to it reduces when they choose to use an outsourcer, therefore lessening the business value of considering service outsourcing. The paper introduces a model to clarify these issues, which is implemented against a health-care scenario, to show how data usage in an outsourcing scenario can be better captured and controlled.}}
@ARTICLE{Hage_1994,title={Reason-Based Logic: a Logic for Reasoning with Rules and Reasons},year={1994},author={Jaap Hage and Jaap Hage and Bart Verheij and Bart Verheij},doi={10.1080/13600834.1994.9965701},pmid={null},pmcid={null},mag_id={2074366137},journal={Information & Communications Technology Law},abstract={Abstract The main claim of this paper is that reasoning with rules, especially rules of law, is different from reasoning with statements that are true or false. This difference is, amongst others, reflected in the defeasibility of arguments in which rules play a role. Reason‐based logic is a logic that has special facilities for dealing with rules and with reasons based on rules. In particular it allows arguments in which conclusions are derived by ‘weighing’ the reasons that plead for and against them. In this article we illustrate some characteristics of reasoning with rules, and show how reason‐based logic deals with these characteristics. The article is concluded with some general considerations concerning reason‐based logic and a comparison with some other logics for defeasible reasoning.}}
@ARTICLE{Conrad_2015,title={The role of evaluation in AI and law: an examination of its different forms in the AI and law journal},year={2015},author={Jack G. Conrad and Jack G. Conrad and John Zeleznikow and John Zeleznikow},doi={10.1145/2746090.2746116},pmid={null},pmcid={null},mag_id={2074615744},journal={null},abstract={This paper explores the presence and forms of evaluation in articles published in the journal Artificial Intelligence and Law for the ten-year period from 2005 through 2014. It represents a meta-level study of some the most significant works produced by the AI and Law community, in this case nearly 140 research articles published in the AI and Law journal. It also compares its findings to previous work conducted on evaluation appearing in the Proceedings of the International Conference on Artificial Intelligence and Law (ICAIL). In addition, the paper highlights works harnessing performance evaluation as one of their chief scientific tools and the means by which they use it. It extends the argument for why evaluation is essential in formal Artificial Intelligence and Law reports such as those in the journal. As in the case of two earlier works on the topic, it pursues answers to the questions: how good is the system, algorithm or proposal?, how reliable is the approach or technique?, and, ultimately, does the method work? The paper investigates the role of performance evaluation in scientific research reports, underscoring the argument that a performance-based 'ethic' signifies a level of maturity and scientific rigor within a community. In addition, the work examines recent publications that address the same critical issue within the broader field of Artificial Intelligence.}}
@ARTICLE{Simmons_2004,title={Requirements triage: what can we learn from a "medical" approach?},year={2004},author={Erik Simmons and E. Simmons},doi={10.1109/ms.2004.25},pmid={null},pmcid={null},mag_id={2075204642},journal={IEEE Software},abstract={New-product development is commonly risky, judging by the number of high-profile failures that continue to occur-especially in software engineering. We can trace many of these failures back to requirements-related issues. Triage is a technique that the medical profession uses to prioritize treatment to patients on the basis of their symptoms' severity. Trauma triage provides some tantalizing insights into how we might measure risk of failure early, quickly, and accurately. For projects at significant risk, we could activate a "requirements trauma system" to include specialists, processes, and tools designed to correct the issues and improve the probability that the project ends successfully. We explain these techniques and suggest how we can adapt them to help identify and quantify requirements-related risks.}}
@ARTICLE{Carlon_2005,title={"If you cannot measure it, you cannot improve it".},year={2005},author={Graziano C. Carlon and Graziano C. Carlon and Arthur H. Combs and Arthur H. Combs},doi={10.1097/01.ccm.0000162564.77999.f8},pmid={15891353},pmcid={null},mag_id={2075514431},journal={Critical Care Medicine},abstract={Department of Clinical Anesthesiology, Cornell University Medical College, New York, NY (Carlon) Department of Clinical Medicine, Washington University School of Medicine, St. Louis, MO (Combs) Everest Biomedical Instruments Company, St. Louis, MO (Combs)}}
@ARTICLE{Moens_2005,title={Combining structured and unstructured information in a retrieval model for accessing legislation},year={2005},author={Marie‐Francine Moens and Marie-Francine Moens},doi={10.1145/1165485.1165507},pmid={null},pmcid={null},mag_id={2075682899},journal={null},abstract={Legislative sources are currently accessible via portal web sites. Users demand precise and exhaustive answers to their information queries. When legislation is drafted, it contains text-rich information that is increasingly marked with XML tags. The statute structure as signaled by XML markup can be exploited to more precisely answer free information queries. In this paper we report on several XML retrieval models that we explicitly designed for the retrieval of legislation. We show that the models provide more advanced access to the content of statutes.}}
@ARTICLE{Prakken_1993,title={A logical framework for modelling legal argument},year={1993},author={Henry Prakken and Henry Prakken},doi={10.1145/158976.158977},pmid={null},pmcid={null},mag_id={2076754485},journal={null},abstract={This paper investigates the relevance of the logical study of argumentation systems for AI-and-law research, in particular for modelling the adversarial aspect of legal reasoning. It does so in applying the argumentation framework of Prakken (1993a/b) to the legal domain. Three elements of the framework are particularly illustrated: firstly, its generality, in that it leaves room for any standard for comparing pairs of arguments; secondly, its ability to model the combined use of these standards; and finally, its relevance for modelling metalevel reasoning. These three features make the framework suitable as a logical framework for any theory of legal argument.}}
@ARTICLE{Prakken_1997,title={Reasoning with precedents in a dialogue game},year={1997},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1145/261618.261621},pmid={null},pmcid={null},mag_id={2080120721},journal={null},abstract={null}}
@ARTICLE{Araszkiewicz_2011,title={Analogy, similarity and factors},year={2011},author={Michał Araszkiewicz and Michał Araszkiewicz},doi={10.1145/2018358.2018372},pmid={null},pmcid={null},mag_id={2080180564},journal={null},abstract={Analogy has been considered in AI and law primarily in relation to reasoning from precedent cases rather than reasoning from statutes. Where a statutory provision does not apply to a case, the principle of e contrario, that if the case is not covered by the rule the negation of the conclusion can be taken as established, has typically been assumed to apply. There are, however, cases where analogy is an appropriate way to bring a case under a statutory rule. In this paper we discuss using analogy in reasoning with states, and where this should be avoided and e contrario followed. Our account will be based on the notion of factors as developed in AI and law case based reasoning.}}
@ARTICLE{Sillito_2006,title={Questions programmers ask during software evolution tasks},year={2006},author={Jonathan Sillito and Jonathan Sillito and Jonathan Sillito and Gail C. Murphy and Gail C. Murphy and Kris De Volder and Kris De Volder},doi={10.1145/1181775.1181779},pmid={null},pmcid={null},mag_id={2080534028},journal={null},abstract={Though many tools are available to help programmers working on change tasks, and several studies have been conducted to understand how programmers comprehend systems, little is known about the specific kinds of questions programmers ask when evolving a code base. To fill this gap we conducted two qualitative studies of programmers performing change tasks to medium to large sized programs. One study involved newcomers working on assigned change tasks to a medium-sized code base. The other study involved industrial programmers working on their own change tasks on code with which they had experience. The focus of our analysis has been on what information a programmer needs to know about a code base while performing a change task and also on howthey go about discovering that information. Based on this analysis we catalog and categorize 44 different kinds of questions asked by our participants. We also describe important context for how those questions were answered by our participants, including their use of tools.}}
@ARTICLE{Boella_2003,title={Permissions and obligations in hierarchical normative systems},year={2003},author={Guido Boella and Guido Boella and Leendert van der Torre and Leendert van der Torre},doi={10.1145/1047788.1047818},pmid={null},pmcid={null},mag_id={2080575678},journal={null},abstract={In this paper we discuss different types of permissions and their roles in deontic logic. We study the distinction between weak and strong permissions in the context of input/output logic, combining the logic with constraints, priorities and hierarchies of normative authorities. In this setting we observe that the notion of prohibition immunity no longer applies, and we introduce a new notion of permission as exception and a new distinction between static and dynamic norms. We show that strong permissions can dynamically change a normative system by adding exceptions to obligations, provide an explicit representation of what is permitted to the subjects of the normative system and allow higher level authorities to limit the changes that lower level authorities can do to the normative system.}}
@ARTICLE{Zave_1997,title={Classification of research efforts in requirements engineering},year={1997},author={Pamela Zave and Pamela Zave},doi={10.1145/267580.267581},pmid={null},pmcid={null},mag_id={2082204982},journal={ACM Computing Surveys},abstract={Requirements engineering is the branch of software engineering concerned with the real-world goals for, functions of, and constraints on software systems. It is also concerned with the relationship of these factors to precise specifications of software behavior, and to their evolution over time and across software families. Of all the areas in which computer scientists do research, requirements engineering is probably the most informal, interdisciplinary, and subjective. Although these qualities are inherent to the topic under investigation, they make scientists and mathematicians uncomfortable. Given these circumstances, a rigorous classification of research efforts in requirements engineering-if comprehensive and intelligible-might have several benefits, including: 1. It would delineate the area and would encourage research coverage of the whole area. 2. It would provide structure that might encourage the discovery and articulation of new principles. 3. It would assist in grouping similar things, such as competing solutions to the same problem. These groupings would be a great help in comparing, extending, and exploiting results. This article proposes and justifies a trial classification scheme. An earlier version was used to organize the papers submitted to this symposium, and the scheme has been refined somewhat in response to inadequacies discovered during the process of selecting the program. It is offered in hopes of stimulating discussion and eventual consensus. The fist issue to be tackled is the heterogeneity of the topics usually considered part of requirements engineering. They include Tasks that must be completed: elicitation, validation, specification. Problems that must be solved: barriers to communication, incompleteness, inconsistency. Solutions to problems: formal languages and analysis algorithms, prototyping, metrics, traceability. Ways of contributing to knowledge: descriptions of practice, case studies, controlled experiments. Types of system:: embedded systems, safety-critical systems, distributed systems. A list with all these topics is intended to be comprehensive, but its heterogeneity undermines all chance of bringing order to the field. There seems to be a need for several orthogonal dimensions of classification. While multiple dimensions will certainly help us cope with the heterogeneity of concerns, there is a danger of making the classification scheme too complex to use. I have compromised by settling on two dimensions, which are presented separately in the next two sections.}}
@ARTICLE{Lame_2005,title={Updating ontologies in the legal domain},year={2005},author={Guiraude Lame and Guiraude Lame and Sylvie Desprès and Sylvie Després and Sylvie Desprès},doi={10.1145/1165485.1165509},pmid={null},pmcid={null},mag_id={2082536945},journal={null},abstract={This paper describes experimental research investigating automated techniques for creating legal ontologies from legislation and for merging existing ontologies with new ones when new laws are adopted. The first part of the paper describes how the ontologies are built, the second part how they are merged for updating the first one.Given a legal ontology, elaborated using certain methods and tools and dedicated to information retrieval, we explore the way to update it. We present important elements concerning the initial ontology: purposes, methods and tools. We describe the experimentation of the updating of a legal ontology.}}
@ARTICLE{Lyons_1979,title={The Authority of Law: Essays on Law and Morality},year={1979},author={David Lyons and Joseph Raz and Joseph Raz},doi={10.1093/acprof:oso/9780198253457.001.0001},pmid={null},pmcid={null},mag_id={2083201648},journal={The Philosophical Review},abstract={This book is concerned with the nature of law and its relation to morality, concentrating on the proper moral attitude of a citizen towards the law of his country. The author begins by presenting a new analysis of the concept of legitimate authority and then gives a detailed explanation of the legal positivist's approach to law. Within this framework the author examines several areas where legal analysis is often thought to be impregnated with moral values, namely the social functions of law, the ideals of the rule of law, and the role of the courts. The last part of the book is devoted to some key substantive problems. The author argues that there is no obligation to obey the law. He provides a new analysis of respect for law, emphasizing its moral importance. The author maintains there is no right to civil disobedience in a liberal state (though actos of civil disobedience may occasionally be justified even in such a state) and he argues for a right of conscientious objection in certain areas.}}
@ARTICLE{Jiang_2015,title={Regulatory compliance of business processes},year={2015},author={Jie Jiang and Jie Jiang and Huib Aldewereld and Huib Aldewereld and Virginia Dignum and Virginia Dignum and Shuzheng Wang and Shuzheng Wang and Ziv Baida and Z.S. Baida},doi={10.1007/s00146-014-0536-9},pmid={null},pmcid={null},mag_id={2083507094},journal={Ai & Society},abstract={Organizations, be it public or private, have to ensure that their operations are complying with various governmental regulations, otherwise they may suffer from law suits and financial losses, or they may even not be allowed to operate (e.g., in case of repeated violations). Therefore, organizations need to have a clear understanding of all the relevant regulations and verify that their business processes are designed and performed in a desired way. However, regulations can be fairly complex in terms of the conditions, targets, and scopes they refer to. Moreover, when considering a set of regulations, the possibility of interrelationships between them brings added complexity to compliance checking. Thus, ensuring regulatory compliance is not only labor and time consuming but also complex. In this paper, we propose a consistency and compliance checker framework (CCCF) that considers sets of interrelated regulations and aims at providing automated supports for organizations to analyze and verify their regulatory compliance. More specifically, CCCF takes legal regulations and business processes as inputs and provides the results of whether the regulations are consistent, whether the business processes are compliant with the regulations, and which business operations need to be adjusted in case of non-compliance. To validate our approach, we use a case study of customs declaration in international trade .}}
@ARTICLE{Bex_2007,title={Formalising argumentative story-based analysis of evidence},year={2007},author={Floris Bex and Floris Bex and Henry Prakken and Henry Prakken and Bart Verheij and Bart Verheij},doi={10.1145/1276318.1276320},pmid={null},pmcid={null},mag_id={2083584323},journal={null},abstract={In the present paper, we provide a formalised version of a merged argumentative and story-based approach towards the analysis of evidence. As an application, we are able to show how our approach sheds new light on inference to the best explanation with case evidence. More specifically, it will be clarified how the events in a case story that are considered to be proven abductively explain the otherwise unproven events of the case story. We compare our approach with existing AI work on modelling legal reasoning with evidence.}}
@ARTICLE{Blair_1985,title={An evaluation of retrieval effectiveness for a full-text document-retrieval system},year={1985},author={David Blair and David Blair and David C. Blair and M. E. Maron and M. E. Maron},doi={10.1145/3166.3197},pmid={null},pmcid={null},mag_id={2083605078},journal={Communications of The ACM},abstract={An evaluation of a large, operational full-text document-retrieval system (containing roughly 350,000 pages of text) shows the system to be retrieving less than 20 percent of the documents relevant to a particular search. The findings are discussed in terms of the theory and practice of full-text document retrieval.}}
@ARTICLE{Greenleaf_1987,title={Expert Systems in Law: The DataLex Project},year={1987},author={Graham Greenleaf and Graham Greenleaf and Andrew Mowbray and Andrew Mowbray and Alan L. Tyree and Alan L. Tyree},doi={10.1145/41735.41737},pmid={null},pmcid={null},mag_id={2083950595},journal={null},abstract={This paper presents the development by the DataLex Project, a joint research project by the authors, of a software ‘shell’ specifically suited to the development of legal expert systems, and its use to develop a number of experimental applications programs used for teaching and demonstrations in several Australian universities. The paper considers a number of characteristics which make the development of legal expert systems different in character from the development of similar systems in engineering, medicine and the natural sciences. These differences, together with some practical constraints, has been responsible for the particular form which the software tools have taken.We argue there are a number of systems which have been built or are being built by people whose primary interest and training is in computer science and who fundamentally misunderstand the nature of law and legal reasoning, primarily widespread misconception that law is a simple system of rules and that legal inference consists of a simple deductive application of these rules. We challenge the view that there is need to construct "deep conceptual" models before effective legal expert systems may be built, concluding that it is an empirical problem which may only be resolved by building systems and evaluating their performance. The design criteria for the software and applications developed by the DataLex Project are then set out, and the shell and a number of resulting applications described. The shell includes both a decision network module (LES:DN) and a module for precedent analysis by nearest neighbour discriminant analysis (PAANDA). A full text retrieval system (AIRS) is intended to be integrated into the shell, and the reasons why this is necessary are explained. Note: This was the first published paper concerning the DataLex project, and the first paper presented at the 1st ICAIL Conference.}}
@ARTICLE{Daniels_1997,title={Finding legally relevant passages in case opinions},year={1997},author={Jody J. Daniels and Jody J. Daniels and Edwina L. Rissland and Edwina L. Rissland},doi={10.1145/261618.261627},pmid={null},pmcid={null},mag_id={2084268194},journal={null},abstract={null}}
@ARTICLE{Boer_2011,title={An agent-based legal knowledge acquisition methodology for agile public administration},year={2011},author={Alexander Boer and Alexander Boer and Tom van Engers and Tom M. van Engers},doi={10.1145/2018358.2018383},pmid={null},pmcid={null},mag_id={2084582605},journal={null},abstract={This paper proposes a knowledge elicitation method based on serious gaming for theory construction about the effects of the law on the behaviours of agents. These games provide input to simulations of business process and product design alternatives. For knowledge representation, we have combined agent role descriptions with a generic task framework. An important thesis of this paper is that, in the interest of quick and simple domain analysis, agent roles, not intelligent agents, should be the focal object of simulation of complex social organizations. At least if getting a grip on social complexity is the purpose of modeling.}}
@ARTICLE{Svensson_2010,title={Managing Quality Requirements: A Systematic Review},year={2010},author={Richard Berntsson Svensson and Richard Berntsson Svensson and Martin Höst and Martin Höst and Björn Regnell and Björn Regnell},doi={10.1109/seaa.2010.55},pmid={null},pmcid={null},mag_id={2084610557},journal={null},abstract={It is commonly acknowledged that the management of quality requirements is an important and difficult part of the requirements engineering process, which plays a critical role in software product development. In order to identify current research about quality requirements, a systematic literature review was performed. This paper identifies available empirical studies of quality requirements. A database and manual search identified 1,560 studies, of which 18 were found to be empirical research studies of high quality, and relevant to the research questions. The review investigates what is currently known about the benefits and limitations of methods of quality requirements. In addition, the state of research is presented for five identified areas: elicitation, dependencies, quality requirements metrics, cost estimations, and prioritization.}}
@ARTICLE{Barboza_2014,title={Towards a legal compliance verification approach on the procurement process of IT solutions for the Brazilian Federal Public Administration},year={2014},author={Lamartine da Silva Barboza and Lamartine da Silva Barboza and Gilberto Amado de Azevedo Cysneiros Filho and Gilberto Cysneiros Filho and Ricardo Souza and Ricardo André Cavalcante de Souza},doi={10.1109/relaw.2014.6893481},pmid={null},pmcid={null},mag_id={2084822030},journal={null},abstract={null}}
@ARTICLE{Capuano_2014,title={A Methodology based on Commonsense Knowledge and Ontologies for the Automatic Classification of Legal Cases},year={2014},author={Nicola Capuano and Nicola Capuano and Carmen De Maio and Carmen De Maio and Saverio Salerno and Saverio Salerno and Daniele Toti and Daniele Toti},doi={10.1145/2611040.2611048},pmid={null},pmcid={null},mag_id={2087351226},journal={null},abstract={We describe a methodology for the automatic classification of legal cases expressed in natural language, which relies on existing legal ontologies and a commonsense knowledge base. This methodology is founded on a process consisting of three phases: an enrichment of a given legal ontology by associating its terms with topics retrieved from the Wikipedia knowledge base; an extraction of relevant concepts from a given textual legal case; and a matching between the enriched ontological terms and the extracted concepts. Such a process has been successfully implemented in a corresponding tool that is part of a larger framework for self-litigation and legal support for the Italian law.}}
@ARTICLE{Lauritsen_2005,title={Intelligent tools for managing factual arguments},year={2005},author={Marc Lauritsen and Marc Lauritsen},doi={10.1145/1165485.1165501},pmid={null},pmcid={null},mag_id={2088510451},journal={null},abstract={By exploring some practical questions in the context of a supremely impractical debate, this article seeks to highlight the challenges and opportunities faced by those trying to promote better use of intelligent tools in the legal workplace. It lays out design features for an imagined online argument manager, describes knowledge engineering challenges such a system presents, and links these to recent research and scholarship. In addition to reviewing theoretical characteristics of factual argumentation, this article considers what kinds of tools are or could be available for everyday use by nonspecialists.}}
@ARTICLE{Plant_2003,title={Methodologies for the development of knowledge-based systems, 1982–2002},year={2003},author={Robert Plant and Robert Plant and Rose F. Gamble and Rose F. Gamble},doi={10.1017/s026988890300064x},pmid={null},pmcid={null},mag_id={2088571338},journal={Knowledge Engineering Review},abstract={Knowledge-based systems have often been criticised for the limited theoretical base upon which they are constructed. This view asserts that systems are often developed in an ad hoc, individual way that leads to unmaintainable, unreliable and non-rigorous systems. The last decade, however, has seen an increased effort to produce methodologies to counter this view as well as continued research into validation and verification techniques. This paper presents a brief discussion of some of the important research in knowledge-based system life cycles and development methods. Methodologies are considered and are discussed in light of two sets of quality assurance criteria.}}
@ARTICLE{Lane_2009,title={Best Practices for Standards Communities},year={2009},author={Gia. Lane and G.T. Lane},doi={10.1109/mc.2009.210},pmid={null},pmcid={null},mag_id={2089519702},journal={IEEE Computer},abstract={The technical standards world is in danger of being broken, hijacked, and exploited by partisan legal and political maneuvering. Today we confront a pivotal moment in the world of technology standards, one that also involves the global economy and ecology. The need to rebuild our financial infrastructure and implement ways to preserve and conserve the environment has given many in the technical community a moment of clarity with regard to the role standards must play in these times. There is a growing realization that while the technical standards world has functioned well enough for decades, the system is in danger of being broken, hijacked, and exploited by partisan legal and political maneuvering. Many of us now recognize an opportunity to restore trust and implement consistent and pervasive safeguards in the standards community.}}
@ARTICLE{Ruiz_2014,title={An intelligent simulation environment for manufacturing systems},year={2014},author={Nancy Ruiz and Nancy Ruiz and Adriana Giret and Adriana Giret and Vicente Botti and Vicente Botti and Víctor Feria and Victor Feria},doi={10.1016/j.cie.2014.06.013},pmid={null},pmcid={null},mag_id={2089864615},journal={Computers & Industrial Engineering},abstract={SimIShop is a simulation tool for intelligent manufacturing systems.The tool uses an agent-supported architecture to simulate manufacturing systems.SimIShop is an easy to use tool.It supports: model creation, simulation, animation and evaluation. The manufacturing field is an area where the application of simulation is an essential tool for validating methods and architectures before applying them on the factory floor. Despite the fact that there are a great number of simulation tools, most of them do not take into account the specific requirements of the "new manufacturing era" such as distributed organization, interoperability, cooperation, scalability, fault tolerance and agility. On the other hand, Multiagent System technology has demonstrated its utility in manufacturing system modeling and implementation. Agenthood features such as proactivity, reactivity, and sociability may also be useful for associating them with the specific simulation needs of the new changing requirements for manufacturing systems. In this paper, an Agent-supported Simulation Environment for intelligent manufacturing systems is presented. The different roles that are played by the agents of the simulation environment are defined taking into account the specific dynamic features in manufacturing simulation and the requirements of the new manufacturing era. Moreover, the interaction and cooperation scenarios among these agents are specified to facilitate manufacturing simulation in an appropriate and flexible way. A detailed evaluation study, with regards to the new manufacturing era requirements, demonstrates the advantages of the proposed approach over current state-of-the-art proposals.}}
@ARTICLE{Hossain_2011,title={Towards an understanding of tailoring scrum in global software development: a multi-case study},year={2011},author={Emam Hossain and Emam Hossain and Paul L. Bannerman and Paul L. Bannerman and Ross Jeffery and Ross Jeffery},doi={10.1145/1987875.1987894},pmid={null},pmcid={null},mag_id={2089973558},journal={null},abstract={There is growing interest in applying Scrum practices in Global Software Development to leverage the advantages of both. However, the effective use of Scrum practices largely depends on close interactions between project stakeholders. The distribution of project stakeholders in GSD provides significant challenges related to project collaboration processes that may limit the use of Scrum. However, project managers increasingly seek to use the Scrum model in their distributed projects. While there is an emerging body of industrial experience, there are limited empirical studies that discuss Scrum tailoring in GSD. The paper reports a multi-case study that investigates the impact of key project contextual factors on the use of Scrum practices in GSD. This study is relevant to researchers and practitioners who are seeking ways to use Scrum in GSD and improve project effectiveness.}}
@ARTICLE{Schweighofer_1995,title={Information filtering: the computation of similarities in large corpora of legal texts},year={1995},author={Erich Schweighofer and Erich Schweighofer and Werner Winiwarter and Werner Winiwarter and Dieter Merkl and Dieter Merkl},doi={10.1145/222092.222205},pmid={null},pmcid={null},mag_id={2090301936},journal={null},abstract={TraditiottaJ information retrieval systems do not satis& the lawyers’ demands because they provide only syntactic representation of legal data, The bottleneck for the creation of the more promising conceptual information retrieval systems is the time-consuming knowledge acquisition. The best solution is the representation of legal knowledge by simple linguistic tools, statistics and neural networks. In our prototype KONTERM we represent legal knowledge about concepts and documents by a knowledge base which is structured by statistical and connectionist methods. In fitture, this knowledge base will be used to filter legal knowledge from documents.}}
@ARTICLE{Herrestad_1995,title={Obligations directed from bearers to counterparts},year={1995},author={Henning Herrestad and Henning Herrestad and Christen Krogh and Christen Krogh},doi={10.1145/222092.222243},pmid={null},pmcid={null},mag_id={2092834548},journal={null},abstract={This articles provides a logical analysis of the concepts of directed obligation, prohibition and permission. These concepts are used to express normative relations between a bearer and a counterpart. They play a predominant role in Hohfeld’s analysis of rights. On our analysis, a directed obligation is defined as a conj unction of a statement expressing that a bearer ought to do a certain act and a statement expressing that it ought to be the case for the counterpart that the bearer does this act. A similar definition is offered of directed prohibition and permission. The proposed analysis is discussed in view of two competing theories of rights: the benefit theory and the claimant theory. The present proposal is found to be supported by the benefit theory. 1 The problem of how to represent directed obligations This paper contributes to the formal analysis of the concept of rights in the tradition from Hohfeld, Kanger and Lindahl. In a number of papers Allen and Saxon have defended the value of a formal representation of rights when making programs to aid the process of drafting or interpreting law (cf. [AS86], [AS93a], [AS93b]). (Concerning the relevance to automated tools for legal drafting, see also [H G93].) This supports the relevance of our paper to AI and Law. Furthermore, Jones and Sergot argues that certain complex systems are best analysed from a ‘normative perspective’, and suggests Lindahl’s analysis as a useful tool (cf. [JS93] ). In [Kro95] it is argued that a representation of normative relations discussed here may be useful to the design of multi–agent systems, as these systems have reached a level where full regimentation i= impossible and reliance will be based on the ability to make contracts between agents. This suggests the relevance of the present paper to mainstream AI as well. Permission to copy without fee all or part of this material is granted provided that the copies we not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee andlor specific permission.}}
@ARTICLE{Freeman_1996,title={A model of argumentation and its application to legal reasoning},year={1996},author={Kathleen P. Freeman and Kathleen Freeman and Kathleen Freeman and Arthur M. Farley and Arthur M. Farley},doi={10.1007/bf00118492},pmid={null},pmcid={null},mag_id={2093412334},journal={Artificial Intelligence and Law},abstract={We present a computational model of dialectical argumentation that could serve as a basis for legal reasoning. The legal domain is an instance of a domain in which knowledge is incomplete, uncertain, and inconsistent. Argumentation is well suited for reasoning in such weak theory domains. We model argument both as information structure, i.e., argument units connecting claims with supporting data, and as dialectical process, i.e., an alternating series of moves by opposing sides. Our model includes burden of proof as a key element, indicating what level of support must be achieved by one side to win the argument. Burden of proof acts as move filter, turntaking mechanism, and termination criterion, eventually determining the winner of an argument. Our model has been implemented in a computer program. We demonstrate the model by considering program output for two examples previously discussed in the artificial intelligence and legal reasoning literature.}}
@ARTICLE{Amyot_2011,title={User Requirements Notation: The First Ten Years, The Next Ten Years (Invited Paper)},year={2011},author={Daniel Amyot and Daniel Amyot and Gunter Mussbacher and Gunter Mussbacher},doi={10.4304/jsw.6.5.747-768},pmid={null},pmcid={null},mag_id={2093562400},journal={Journal of Software},abstract={The User Requirements Notation (URN), standardized by the International Telecommunication Union in 2008, is used to model and analyze requirements with goals and scenarios. This paper describes the first ten years of development of URN, and discusses ongoing efforts targeting the next ten years. We did a study inspired by the systematic literature review approach, querying five major search engines and using the existing URN Virtual Library. Based on the 281 scientific publications related to URN we collected and analyzed, we observe a shift from a more conventional use of URN for telecommunications and reactive systems to business process management and aspect-oriented modeling, with relevant extensions to the language being proposed. URN also benefits from a global and active research community, although industrial contributions are still sparse. URN is now a leading language for goal-driven and scenario-oriented modeling with a promising future for many application domains.}}
@ARTICLE{Governatori_2005,title={REPRESENTING BUSINESS CONTRACTS IN RuleML},year={2005},author={Guido Governatori and Guido Governatori},doi={10.1142/s0218843005001092},pmid={null},pmcid={null},mag_id={2094758254},journal={International Journal of Cooperative Information Systems},abstract={This paper presents an approach for the specification and implementation of translating contracts from a human-oriented form into an executable representation for monitoring. This will be done in the setting of RuleML. The task of monitoring contract execution and performance requires a logical account of deontic and defeasible aspects of legal language; currently such aspects are not covered by RuleML; accordingly we show how to extend it to cover such notions. From its logical form, the contract will thus be transformed into a machine readable rule notation and eventually implemented as executable semantics via any mark-up languages depending on the client's preference, for contract monitoring purposes.}}
@ARTICLE{Dag_2006,title={An experiment on linguistic tool support for consolidation of requirements from multiple sources in market-driven product development},year={2006},author={Johan Natt och Dag and Johan Natt och Dag and Thomas Thelin and Thomas Thelin and Björn Regnell and Björn Regnell},doi={10.1007/s10664-006-6405-5},pmid={null},pmcid={null},mag_id={2095459395},journal={Empirical Software Engineering},abstract={This paper presents an experiment with a linguistic support tool for consolidation of requirements sets. The experiment is designed based on the requirements management process at a large market-driven software development company that develops generic solutions to satisfy many different customers. New requirements and requests for information are continuously issued, which must be analyzed and responded to. The new requirements should first be consolidated with the old to avoid reanalysis of previously elicited requirements and to complement existing requirements with new information. In the presented experiment, a new open-source tool is evaluated in a laboratory setting. The tool uses linguistic engineering techniques to calculate similarities between requirements and presents a ranked list of suggested similar requirements, between which links may be assigned. It is hypothesized that the proposed technique for finding and linking similar requirements makes the consolidation more efficient. The results show that subjects that are given the support provided by the tool are significantly more efficient and more correct in consolidating two requirements sets, than are subjects that do not get the support. The results suggest that the proposed techniques may give valuable support and save time in an industrial requirements consolidation process.}}
@ARTICLE{Staples_2007,title={Experiences using systematic review guidelines},year={2007},author={Mark Staples and Mark Staples and Mahmood Niazi and Mahmood Niazi},doi={10.1016/j.jss.2006.09.046},pmid={null},pmcid={null},mag_id={2096598237},journal={Journal of Systems and Software},abstract={null}}
@ARTICLE{Bieman_2001,title={Multi-view software evolution: a UML-based framework for evolving object-oriented software},year={2001},author={James M. Bieman and James M. Bieman},doi={10.1109/icsm.2001.972751},pmid={null},pmcid={null},mag_id={2096840655},journal={null},abstract={It is well-known that uncontrolled change to software can lead to increasing evolution costs caused by deteriorating structure and compromised system qualities. For complex systems, the need to carefully manage system evolution is critical. In this paper we outline an approach to managing evolution of object-oriented (OO) software. The approach is based on a goal-directed, cyclic process, in which OO models are transformed and quantitatively evaluated in each cycle. Evaluation criteria guide developers in choosing between alternative transformations in each cycle. The process, transformations, and evaluation techniques can be used to develop systems from a set of baseline models.}}
@ARTICLE{Żurek_2009,title={Supporting of legal reasoning for cases which are not strictly regulated by law},year={2009},author={Tomasz Żurek and Tomasz Zurek and Emil Kruk and Emil Kruk},doi={10.1145/1568234.1568263},pmid={null},pmcid={null},mag_id={2097028559},journal={null},abstract={Statute law legislators are usually not in a position to foresee each and every situation or event which may actually occur in real life. That is why lawyers in the course of their everyday practice very often struggle with interpreting the cases which are not expressly regulated in the law. The legal theory and practice has given rise to a wide array of methods to deal with this type of problems. The solutions described in this article have been implemented in the advisory system developed by the authors whose main goal is to provide automatic legal advice on the Agricultural Tax}}
@ARTICLE{Choi_2006,title={Challenges Associated with Privacy in Health Care Industry: Implementation of HIPAA and the Security Rules},year={2006},author={Young B. Choi and Young B. Choi and K.E. Capitan and Kathleen E. Capitan and Joshua S. Krause and Joshua S. Krause and Meredith M. Streeper and Meredith M. Streeper},doi={10.1007/s10916-006-7405-0},pmid={16548416},pmcid={null},mag_id={2097141591},journal={Journal of Medical Systems},abstract={This paper discusses the challenges associated with privacy in health care in the electronic information age based on the Health Insurance Portability and Accountability Act (HIPAA) and the Security Rules. We examine the storing and transmission of sensitive patient data in the modern health care system and discuss current security practices that health care providers institute to comply with HIPAA Security Rule regulations. Based on our research results, we address current outstanding issues that act as impediments to the successful implementation of security measures and conclude the discussion and offer possible avenues of future research.}}
@ARTICLE{Berman_1988,title={Obstacles to the development of logic-based models of legal reasoning},year={1988},author={Donald H. Berman and Donald H. Berman and Carole D. Hafner and Carole D. Hafner},doi={null},pmid={null},pmcid={null},mag_id={2097608136},journal={null},abstract={null}}
@ARTICLE{Governatori_2010,title={Superiority based revision of defeasible theories},year={2010},author={Guido Governatori and Guido Governatori and Francesco Olivieri and Francesco Olivieri and Simone Scannapieco and Simone Scannapieco and Matteo Cristani and Matteo Cristani},doi={10.1007/978-3-642-16289-3_10},pmid={null},pmcid={null},mag_id={2097677609},journal={null},abstract={We propose a systematic investigation on how to modify a preference relation in a defeasible logic theory to change the conclusions of the theory itself. We argue that the approach we adopt is applicable to legal reasoning, where users, in general, cannot change facts and rules, but can propose their preferences about the relative strength of the rules.

We provide a comprehensive study of the possible combinatorial cases and we identify and analyse the cases where the revision process is successful.}}
@ARTICLE{Paulk_2001,title={Capability Maturity Model for Software},year={2001},author={Mark C. Paulk and Mark C. Paulk and Bruce Curtis and Bill Curtis and Mary Beth Chrissis and Mary Beth Chrissis},doi={10.1109/9781118156667.ch2},pmid={null},pmcid={null},mag_id={2097700520},journal={null},abstract={This article provides an overview of the Capability Maturity Model® for Software (Software CMM®) and the concepts of software process maturity. (Capability Maturity Model and CMM are registered in the U.S. Patent and Trademark Office.) It contains a background discussion of why process is crucial to organizational and project success, a description of the development of the CMM, a detailed summary of the model, and a description of the model's use for process improvement and the evaluation of software suppliers; describes the use of the CMM in the context of the SEI's IDEALSM (IDEAL is a service mark of Carnegie Mellon University) approach to process improvement; summarizes some of the strengths and weaknesses of the current model and its use; characterizes the state of the practice and the return on investment for software process improvement; and concludes with a discussion of likely future directions for CMM-like models.


Keywords:

capability maturity model;
trademark;
Carnegie Mellon University;
principles;
total quality management;
software;
maturity model;
uses;
key process improvement;
case studies}}
@ARTICLE{Pang_2008,title={Opinion Mining and Sentiment Analysis},year={2008},author={Bo Pang and Bo Pang and Lillian Lee and Lillian Lee},doi={10.1561/1500000011},pmid={null},pmcid={null},mag_id={2097726431},journal={Foundations and Trends in Information Retrieval},abstract={An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.

This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.}}
@ARTICLE{Wyner_2010,title={Approaches to text mining arguments from legal cases},year={2010},author={Adam Wyner and Adam Wyner and Raquel Mochales-Palau and Raquel Mochales-Palau and Marie‐Francine Moens and Marie-Francine Moens and David Milward and David Milward},doi={10.1007/978-3-642-12837-0_4},pmid={null},pmcid={null},mag_id={2097794246},journal={null},abstract={This paper describes recent approaches using text-mining to automatically profile and extract arguments from legal cases. We outline some of the background context and motivations. We then turn to consider issues related to the construction and composition of corpora of legal cases. We show how a Context-Free Grammar can be used to extract arguments, and how ontologies and Natural Language Processing can identify complex information such as case factors and participant roles. Together the results bring us closer to automatic identification of legal arguments.}}
@ARTICLE{Kharbili_2008,title={Business process compliance checking : current state and future challenges},year={2008},author={Marwane El Kharbili and Marwane El Kharbili and Ana Karla Alves de Medeiros and Ana Karla Alves de Medeiros and Sebastian Stein and Sebastian Stein and Wil M. P. van der Aalst and Wil M. P. van der Aalst},doi={null},pmid={null},pmcid={null},mag_id={2098419680},journal={null},abstract={Regulatory compliance sets new requirements for business process management (BPM). Companies seek to enhance their corporate governance processes and are required to put in place measures for ensuring compliance to regulations. In this sense, this position paper (i) reviews the current work in the context of BPM systems and (ii) suggests future directions to improve the current status. During the literature review, techniques are classified as supporting forward or backward compliance. The latter is a post-execution compliance (i.e. based on execution histories of systems) and the former takes place at design- or run-time. In a nutshell, this position paper claims that four main aspects need to be incorporated by current compliance checking techniques: (i) an integrated approach able to cover the full BPM life-cycle, (ii) the support for compliance checks beyond control-flow-related aspects, (iii) intuitive graphical notations for business analysts, and (iv) embedding semantic technologies during the definition, deployment and executions of compliance checks.}}
@ARTICLE{Boella_2009,title={Lex minus dixit quam voluit, lex magis dixit quam voluit: a formal study on legal compliance and interpretation},year={2009},author={Guido Boella and Guido Boella and Guido Boella and Guido Governatori and Guido Governatori and Antonino Rotolo and Antonino Rotolo and Leendert van der Torre and Leendert van der Torre},doi={10.1007/978-3-642-16524-5_11},pmid={null},pmcid={null},mag_id={2098730435},journal={null},abstract={This paper argues in favour of the necessity of dynamically restricting and expanding the applicability of norms regulating computer systems like multiagent systems, in situations where the compliance to the norm does not achieve the purpose of the norm. We propose a logical framework which distinguishes between constitutive and regulative norms and captures the norm change power and at the same time the limitations of the judicial system in dynamically revising the set of constitutive rules defining the concepts on which the applicability of norms is based. In particular, the framework is used to reconstruct some interpretive arguments described in legal theory such as those corresponding to the Roman maxims lex minus dixit quam voluit and lex magis dixit quam voluit. The logical framework is based on an extension of defeasible logic.}}
@ARTICLE{Vreeswijk_1997,title={Abstract argumentation systems},year={1997},author={Gerard A. W. Vreeswijk and Gerard A. W. Vreeswijk},doi={10.1016/s0004-3702(96)00041-0},pmid={null},pmcid={null},mag_id={2098746567},journal={Artificial Intelligence},abstract={In this paper, we develop a theory of abstract argumentation systems. An abstract argumentation system is a collection of “defeasible proofs”, called arguments, that is partially ordered by a relation expressing the difference in conclusive force. The prefix “abstract” indicates that the theory is concerned neither with a specification of the underlying language, nor with the development of a subtheory that explains the partial order. An unstructured language, without logical connectives such as negation, makes arguments not (pairwise) inconsistent, but (groupwise) incompatible. Incompatibility and difference in conclusive force cause defeat among arguments. The aim of the theory is to find out which arguments eventually emerge undefeated. These arguments are considered to be in force. Several results are established. The main result is that arguments that are in force are precisely those that are in the limit of a so-called complete argumentation sequence.}}
@ARTICLE{Cleland‐Huang_2005,title={Utilizing supporting evidence to improve dynamic requirements traceability},year={2005},author={Jane Cleland‐Huang and Jane Cleland-Huang and Raffaella Settimi and Raffaella Settimi and Chuan Duan and Chuan Duan and Xuchang Zou and Xuchang Zou},doi={10.1109/re.2005.78},pmid={null},pmcid={null},mag_id={2099175080},journal={null},abstract={Requirements traceability provides critical support throughout all phases of a software development project. However practice has repeatedly shown the difficulties involved in long term maintenance of traditional traceability matrices. Dynamic retrieval methods minimize the need for creating and maintaining explicit links and can significantly reduce the effort required to perform a manual trace. Unfortunately they suffer from recall and precision problems. This paper introduces three strategies for incorporating supporting information into a probabilistic retrieval algorithm in order to improve the performance of dynamic requirements traceability. The strategies include hierarchical modeling, logical clustering of artifacts, and semi-automated pruning of the probabilistic network. Experimental results indicate that enhancement strategies can be used effectively to improve trace retrieval results thereby increasing the practicality of utilizing dynamic trace retrieval methods.}}
@ARTICLE{Fowler_2007,title={Network Analysis and the Law: Measuring the Legal Importance of Precedents at the U.S. Supreme Court},year={2007},author={James H. Fowler and James H. Fowler and Timothy R. Johnson and Timothy R. Johnson and James F. Spriggs and James F. Spriggs and Sangick Jeon and Sangick Jeon and Paul J. Wahlbeck and Paul J. Wahlbeck},doi={10.1093/pan/mpm011},pmid={null},pmcid={null},mag_id={2099183456},journal={Political Analysis},abstract={We construct the complete network of 26,681 majority opinions written by the U.S. Supreme Court and the cases that cite them from 1791 to 2005. We describe a method for using the patterns in citations within and across cases to create importance scores that identify the most legally relevant precedents in the network of Supreme Court law at any given point in time. Our measures are superior to existing network-based alternatives and, for example, offer information regarding case importance not evident in simple citation counts. We also demonstrate the validity of our measures by showing that they are strongly correlated with the future citation behavior of state courts, the U.S. Courts of Appeals, and the U.S. Supreme Court. In so doing, we show that network analysis is a viable way of measuring how central a case is to law at the Court and suggest that it can be used to measure other legal concepts.}}
@ARTICLE{Hage_2001,title={Formalizing legal coherence},year={2001},author={Jaap Hage and Jaap Hage},doi={10.1145/383535.383538},pmid={null},pmcid={null},mag_id={2099215247},journal={null},abstract={This paper briefly argues for a (particular variant of) a coherence theory of legal justification and theory construction. It does so by placing coherentism in a tradition of general epistemology and practical reasoning. One part of the theory, namely the part that deals with the relation between abstract goals and concrete regulations, is described in detail, and formalized.}}
@ARTICLE{Kerrigan_2005,title={Regulation-centric, logic-based compliance assistance framework},year={2005},author={Shawn Kerrigan and Shawn Kerrigan and Kincho H. Law and Kincho H. Law},doi={10.1061/(asce)0887-3801(2005)19:1(1)},pmid={null},pmcid={null},mag_id={2099334919},journal={Journal of Computing in Civil Engineering},abstract={This paper describes the development of a logic based regulation compliance assistance system that builds upon an extensible markup language (XML) framework. First, a document repository containing federal regulations and supplemental documents, and an XML framework for representing regulations and associated metadata are briefly discussed. The prototype effort for the regulation assistance system focuses on federal environmental regulations and related documents. The compliance assistance system is illustrated in the domain of used oil management. The overall objective is to develop a formal infrastructure for regulatory information management and compliance assistance.}}
@ARTICLE{Sabetzadeh_2007,title={Consistency Checking of Conceptual Models via Model Merging},year={2007},author={Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Shiva Nejati and Shiva Nejati and Sotirios Liaskos and Sotirios Liaskos and Steve Easterbrook and Steve Easterbrook and Marsha Chećhik and Marsha Chechik},doi={10.1109/re.2007.18},pmid={null},pmcid={null},mag_id={2099698375},journal={null},abstract={Requirements elicitation involves the construction of large sets of conceptual models. An important step in the analysis of these models is checking their consistency. Existing research largely focuses on checking consistency of individual models and of relationships between pairs of models. However, such strategy does not guarantee global consistency. In this paper, we propose a consistency checking approach that addresses this problem for homogeneous models. Given a set of models and a set of relationships between them, our approach works by first constructing a merged model and then verifying this model against the consistency constraints of interest. By keeping proper traceability information, consistency diagnostics obtained over the merge are projected back to the original models and their relationships. The paper also presents a set of reusable expressions for defining consistency constraints in conceptual modelling. We demonstrate the use of the developed expressions in the specification of consistency rules for class and ER diagrams, and i* goal models.}}
@ARTICLE{Karlsson_1998,title={An evaluation of methods for prioritizing software requirements},year={1998},author={Jenny Karlsson and Joachim Karlsson and Joachim Karlsson and Joachim Karlsson and Claes Wohlin and Claes Wohlin and Björn Regnell and Björn Regnell},doi={10.1016/s0950-5849(97)00053-0},pmid={null},pmcid={null},mag_id={2099911908},journal={Information & Software Technology},abstract={This article describes an evaluation of six different methods for prioritizing software requirements. Based on the quality requirements for a telephony system, the authors individually used all six methods on separate occasions to prioritize the requirements. The methods were then characterized according to a number of criteria from a user's perspective. We found the analytic hierarchy process to be the most promising method, although it may be problematic to scale-up. In an industrial follow-up study we used the analytic hierarchy process to further investigate its applicability. We found that the process is demanding but worth the effort because of its ability to provide reliable results, promote knowledge transfer and create consensus among project members.}}
@ARTICLE{Young_2009,title={Identifying Commitment-Based Software Requirements to Thwart Unfair and Deceptive Practices},year={2009},author={Jessica D. Young and Jessica D. Young and Annie I. Antón and Annie I. Antón},doi={10.1109/relaw.2009.6},pmid={null},pmcid={null},mag_id={2100531500},journal={null},abstract={Companies publish privacy notices to notify consumers about their information practices. These privacy notices express company-specific commitments to the consumer about how the company will collect, use, and securely store data. Requirements engineers need to understand these commitments so they may be operationalized into specific security and privacy requirements. In the U.S., corporate commitments must meet the Federal Trade Commission's expectations for fair business practices. Companies must thus ensure that they maintain these commitments in accordance with corporate policies, laws, and regulations. We present an approach for identifying commitment-based requirements to support this objective.}}
@ARTICLE{Zimmermann_2004,title={Mining version histories to guide software changes},year={2004},author={Thomas Zimmermann and Thomas Zimmermann and P. Weibgerber and Peter Weisgerber and P. Weibgerber and P. Weibgerber and Stephan Diehl and Stephan Diehl and Andreas Zeller and Andreas Zeller},doi={10.1109/icse.2004.1317478},pmid={null},pmcid={null},mag_id={2100849134},journal={null},abstract={We apply data mining to version histories in order to guide programmers along related changes: "Programmers who changed these functions also changed. . . ". Given a set of existing changes, such rules (a) suggest and predict likely further changes, (b) show up item coupling that is indetectable by program analysis, and (c) prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict 26% of further files to be changed - and 15% of the precise functions or variables. The topmost three suggestions contain a correct location with a likelihood of 64%.}}
@ARTICLE{Offermann_2009,title={Outline of a design science research process},year={2009},author={Philipp Offermann and Philipp Offermann and Olga Levina and Olga Levina and Marten Schönherr and Marten Schönherr and Udo Bub and Udo Bub},doi={10.1145/1555619.1555629},pmid={null},pmcid={null},mag_id={2100987490},journal={null},abstract={Discussions about the body of knowledge of information systems, including the research domain, relevant perspectives and methods have been going on for a long time. Many researchers vote for a combination of research perspectives and their respective research methodologies; rigour and relevance as requirements in design science are generally accepted. What has been lacking is a formalisation of a detailed research process for design science that takes into account all requirements. We have developed such a research process, building on top of existing processes and findings from design research. The process combines qualitative and quantitative research and references well-known research methods. Publication possibilities and self-contained work packages are recommended. Case studies using the process are presented and discussed.}}
@ARTICLE{Massey_2008,title={A Requirements-based Comparison of Privacy Taxonomies},year={2008},author={Aaron K. Massey and Aaron K. Massey and Annie I. Antón and Annie I. Antón},doi={10.1109/relaw.2008.1},pmid={null},pmcid={null},mag_id={2101452663},journal={null},abstract={Understanding the nature of privacy regulation is a challenge that requirements engineers face when building software systems in financial, healthcare, government, or other sensitive industries. Requirements engineers have begun to model privacy requirements based on taxonomic classifications of privacy. Independently, legal research has modeled privacy harms in a taxonomic fashion. In this paper, we compare a requirements engineering taxonomy of privacy protections and vulnerabilities to a legal taxonomy of privacy harms. We seek to determine the extent to which the concepts and terminology are consistent between the two taxonomies. A consistent, standard vocabulary for privacy concepts for both requirements engineers and lawyers will improve the common understanding of privacy concepts, legal traceability and compliance auditing. We conclude that the taxonomies we analyzed are reasonably compatible. We believe this compatibility indicates that a taxonomic understanding of privacy is a promising area of research for requirements engineers.}}
@ARTICLE{Whittle_2000,title={Generating statechart designs from scenarios},year={2000},author={Jon Whittle and Jon Whittle and Johann Schumann and Johann Schumann and Johann Schumann},doi={10.1145/337180.337217},pmid={null},pmcid={null},mag_id={2102071894},journal={null},abstract={This paper presents an algorithm for automatically generating UML statecharts from a collection of UML sequence diagrams. Computer support for this transition between requirements and design is important for a successful application of UML's highly iterative, distributed software development process. There are three main issues which must be addressed when generating statecharts from sequence diagrams. Firstly, conflicts arising from the merging of independently developed sequence diagrams must be detected and resolved. Secondly, different sequence diagrams often contain identical or similar behaviors. For a true interleaving of the sequence diagrams, these behaviors must be recognized and merged. Finally, generated statecharts usually are only an approximation of the system and thus must be hand-modified and refined by designers. As such, the generated artifact should be highly structured and readable. In terms of statecharts, this corresponds to the introduction of hierarchy. Our algorithm successfully tackles all three of these aspects and will be illustrated in this paper with a well-known ATM example.}}
@ARTICLE{Perini_2007,title={An Empirical Study to Compare the Accuracy of AHP and CBRanking Techniques for Requirements Prioritization},year={2007},author={Anna Perini and Anna Perini and Angelo Susi and Angelo Susi and Filippo Ricca and Filippo Ricca and Cinzia Bazzanella and Cinzia Bazzanella},doi={10.1109/cere.2007.1},pmid={null},pmcid={null},mag_id={2102949421},journal={null},abstract={Requirements prioritization aims at identifying the most important requirements for a system (or a release). A large number of approaches have been proposed so far, to help decision makers in performing this activity. Some of them provide supporting tools. Questions on when a prioritization technique should be preferred to another one as well as on how to characterize and measure their properties arise. Several empirical studies have been conducted to analyze characteristics of the available approaches, but their results are often difficult to compare. In this paper we discuss an empirical study aiming at evaluating two state-of-the art, tool-supported requirements prioritization techniques, AHP and CBRanking. The experiment has been conducted with 18 experienced subjects on a set of 20 requirements from a real project. We focus on a crucial variable, namely the ranking accuracy. We discuss different ways to measure it and analyze the data collected in the experimental study with reference to this variable. Results indicate that AHP gives more accurate rankings than CBRanking, but the ranks produced by the two methods are similar for all the involved subjects.}}
@ARTICLE{Grau_2006,title={J-PRiM: A Java Tool for a Process Reengineering i* Methodology},year={2006},author={G. Grau and Gemma Grau and Xavier Franch and Xavier Franch and Silvana De Gyves Avila and S. Avila},doi={10.1109/re.2006.36},pmid={null},pmcid={null},mag_id={2103123591},journal={null},abstract={The i* approach is a consolidated modelling technique that has proven to be useful in the requirements engineering phases of software development. Using i* requires the adoption of a methodology for defining the models and tool support for manipulating them. For addressing those aspects, we propose J-PRiM, a tool that allows to define i* models by applying PRiM, our Process Reengineering i* Methodology.}}
@ARTICLE{Beach_2015,title={A rule-based semantic approach for automated regulatory compliance in the construction sector},year={2015},author={Thomas Beach and Thomas Beach and Yacine Rezgui and Yacine Rezgui and Haijiang Li and Haijiang Li and Tala Kasim and Tala Kasim},doi={10.1016/j.eswa.2015.02.029},pmid={null},pmcid={null},mag_id={2103438167},journal={Expert Systems With Applications},abstract={A rule-based semantic approach for compliance checking.An ontological framework for regulatory compliance checking.Extracting regulations from semantic analysis of textual documents.Semantic rules to deliver regulatory compliance checking based on instances of the proposed ontology.Semantic mapping of regulations to data file formats. A key concern for professionals in any industry is ensuring regulatory compliance. Regulations are often complex and require in depth technical knowledge of the domain in which they operate. The level of technical detail and complexity in regulations is a barrier to their automation due to extensive software development time and costs that are involved. In this paper we present a rule-based semantic approach formulated as a methodology to overcome these issues by allowing domain experts to specify their own regulatory compliance systems without the need for extensive software development. Our methodology is based on the key idea that three semantic contexts are needed to fully understand the regulations being automated: the semantics of the target domain, the specific semantics of regulations being considered, and the semantics of the data format that is to be checked for compliance. This approach allows domain experts to create and maintain their own regulatory compliance systems, within a semantic domain that is familiar to them. At the same time, our approach allows for the often diverse nature of semantics within a particular domain by decoupling the specific semantics of regulations from the semantics of the domain itself. This paper demonstrates how our methodology has been validated using a series of regulations automated by professionals within the construction domain. The regulations that have been developed are then in turn validated on real building data stored in an industry specific format (the IFCs). The adoption of this methodology has greatly advanced the process of automating these complex sets of construction regulations, allowing the full automation of the regulation scheme within 18months. We believe that these positive results show that, by adopting our methodology, the barriers to the building of regulatory compliance systems will be greatly lowered and the adoption of three semantic domains proposed by our methodology provides tangible benefits.}}
@ARTICLE{Torre_1999,title={Diagnosis and decision making in normative reasoning},year={1999},author={Leendert van der Torre and Leendert W. Torre and Yao‐Hua Tan and Yao-Hua Tan},doi={10.1023/a:1008359312576},pmid={null},pmcid={null},mag_id={2103987643},journal={Artificial Intelligence and Law},abstract={Diagnosis theory reasons about incomplete knowledge and only considers the past. It distinguishes between violations and non-violations. Qualitative decision theory reasons about decision variables and considers the future. It distinguishes between fulfilled goals and unfulfilled goals. In this paper we formalize normative diagnoses and decisions in the special purpose formalism DIO(DE)2 as well as in extensions of the preference-based deontic logic PDL. The DIagnostic and DEcision-theoretic framework for DEontic reasoning DIO(DE)2 formalizes reasoning about violations and fulfillments, and is used to characterize the distinction between normative diagnosis theory and (qualitative) decision theory. The extension of the preference-based deontic logic PDL shows how normative diagnostic and decision-theoretic reasoning -- i.e. reasoning about violations and fulfillments -- can be formalized as an extension of deontic reasoning.}}
@ARTICLE{Možina_2005,title={Argument based machine learning applied to law},year={2005},author={Martin Možina and Martin Možina and Jure Žabkar and Jure Žabkar and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Ivan Bratko and Ivan Bratko},doi={10.1007/s10506-006-9002-4},pmid={null},pmcid={null},mag_id={2104540263},journal={Artificial Intelligence and Law},abstract={In this paper we discuss the application of a new machine learning approach – Argument Based Machine Learning – to the legal domain. An experiment using a dataset which has also been used in previous experiments with other learning techniques is described, and comparison with previous experiments made. We also tested this method for its robustness to noise in learning data. Argumentation based machine learning is particularly suited to the legal domain as it makes use of the justifications of decisions which are available. Importantly, where a large number of decided cases are available, it provides a way of identifying which need to be considered. Using this technique, only decisions which will have an influence on the rules being learned are examined.}}
@ARTICLE{Kitchenham_2007,title={Cross versus Within-Company Cost Estimation Studies: A Systematic Review},year={2007},author={Barbara Kitchenham and Barbara Kitchenham and Emilia Mendes and Emilia Mendes and Guilherme Horta Travassos and Guilherme Horta Travassos},doi={10.1109/tse.2007.1001},pmid={null},pmcid={null},mag_id={2104789242},journal={IEEE Transactions on Software Engineering},abstract={The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., $20 projects) that used leave-one-out cross validation all found that the within-company model was significantly different (better) from the cross-company model. The results of this review are inconclusive. It is clear that some organizations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e., based on different data bases or at least different single company data sets) and should address specific hypotheses concerning the conditions that would favor cross-company or within-company models. In addition, experimenters need to standardize their experimental procedures to enable formal meta-analysis, and recommendations are made in Section 3.}}
@ARTICLE{Conmy_2007,title={Challenges when using Model Driven Architecture in the development of Safety Critical Software},year={2007},author={Philippa Conmy and Philippa Conmy and Richard F. Paige and Richard F. Paige},doi={10.1109/mompes.2007.4},pmid={null},pmcid={null},mag_id={2104930391},journal={null},abstract={The model driven architecture (MDA) is an approach to software engineering in which models are systematically developed and transformed into code. This paper discusses some of the issues which would need to be overcome when attempting to certify a safety critical design or software developed with the MDA approach, partially based on our experience with an avionics software case study. We particularly focus on the need to certify MDA artefacts and produce a compelling system safety case}}
@ARTICLE{Chung_2009,title={On Non-Functional Requirements in Software Engineering},year={2009},author={Lawrence Chung and Lawrence Chung and Julio César Prado Leite and Julio Cesar Sampaio do Prado Leite},doi={10.1007/978-3-642-02463-4_19},pmid={null},pmcid={null},mag_id={2105539612},journal={null},abstract={Essentially a software system's utility is determined by both its functionality and its non-functional characteristics, such as usability, flexibility, performance, interoperability and security. Nonetheless, there has been a lop-sided emphasis in the functionality of the software, even though the functionality is not useful or usable without the necessary non-functional characteristics. In this chapter, we review the state of the art on the treatment of non-functional requirements (hereafter, NFRs), while providing some prospects for future directions.}}
@ARTICLE{Brewka_1994,title={Reasoning about priorities in default logic},year={1994},author={Gerhard Brewka and Gerhard Brewka},doi={null},pmid={null},pmcid={null},mag_id={2105597099},journal={null},abstract={In this paper we argue that for realistic applications involving default reasoning it is necessary to reason about the priorities of defaults. Existing approaches require the knowledge engineer to explicitly state all relevant priorities which are then handled in an extra-logical manner, or they are restricted to priorities based on specificity, neglecting other relevant criteria. We present an approach where priority information can be represented within the logical language.

Our approach is based on PDL, a prioritized extension of Reiter's Default Logic recently proposed by the same author. In PDL the generation of extensions is controlled by an ordering of the defaults. This property is used here in the following way: we first build Reiter extensions of a given default theory. These extensions contain explicit information about the priorities of defaults. We then eliminate every extension E that cannot be reconstructed as a PDL extension based on a default ordering that is compatible with the priority information in E. An example from legal reasoning illustrates the power of our approach.}}
@ARTICLE{Ajani_2007,title={Terminological and ontological analysis of European directives: multilinguism in law},year={2007},author={Gianmaria Ajani and Gianmaria Ajani and Leonardo Lesmo and Leonardo Lesmo and Guido Boella and Guido Boella and Alessandro Mazzei and Alessandro Mazzei and Piercarlo Rossi and Piercarlo Rossi and Piercarlo Rossi},doi={10.1145/1276318.1276327},pmid={null},pmcid={null},mag_id={2105898483},journal={null},abstract={This paper describes the philosophy behind our tool called "Legal Taxonomy Syllabus", the analytical instruments it provides and some case studies. The Legal Taxonomy Syllabus is an ontology based tool designed to annotate and recover multi-lingua legal information and build conceptual dictionaries. The Legal Taxonomy Syllabus allows to build legal dictionaries in a bottom up fashion starting from the annotation of legal terms by legal terminological experts and to let legal ontology engineers refine the resulting taxonomies of concepts. The Legal Taxonomy Syllabus and its analytical tools provide help to lawyers to study the peculiarities of European Union Directives concerning the polysemy of legal terms, and the terminological and conceptual misalignment. By means of two case studies we show how the Legal Taxonomy Syllabus can help the processes of drafting and translating of the Directives.}}
@ARTICLE{Berry_2003,title={From Contract Drafting to Software Specification: Linguistic Sources of Ambiguity},year={2003},author={Daniel M. Berry and Daniel M. Berry and Erik Kamsties and Erik Kamsties and Michael Krieger and Michael M. Krieger},doi={null},pmid={null},pmcid={null},mag_id={2106364303},journal={null},abstract={This handbook is about writing software requirements specifications and legal contracts, two kinds of documents with similar needs for completeness, consistency, and precision. Particularly when these are written, as they usually are, in natural language, ambiguity—by any definition—is a major cause of their not specifying what they should. Simple misuse of the language in which the document is written is one source of these ambiguities. This handbook describes the ambiguity phenomenon from several points of view, including linguistics, software engineering, and the law. Several strategies for avoiding and detecting ambiguities are presented. Strong emphasis is given on the problems arising from the use of heavily used and seemingly unambiguous words and phrases such as “all”, “each”, and “every” in defining or referencing sets; positioning of “only”, “also”, and “even”; precedences of “and” and “or”; “a”, “all”, “any”, “each”, “one”, “some”, and “the” used as quantifiers; “or” and “and/or”; “that” vs. “which”; parallelism; pronouns referring to an idea; multiple adjectives; etc. Many examples from requirements documents and legal documents are examined. While no guide can overcome the careless or indifferent writer, this handbook is offered as a guide both for writing better requirements or contracts and for inspecting them for potential ambiguities.}}
@ARTICLE{Austin_1962,title={Sense and Sensibilia},year={1962},author={John Austin and J. L. Austin and G. J. Warnock},doi={null},pmid={null},pmcid={null},mag_id={2106808087},journal={null},abstract={A very nice book! And rejoice according to have him, 'there's no wonder on the child. And edward this book while, paradoxically endorsing the novel. As a very believable personality traits, of all the extended. Because marianne had planned to ourselves or sensibility if I am. You are by showing sense and marianne was born. And marianne rashly writes them when, I wouldn't have. I love however was liked.}}
@ARTICLE{Kitchenham_2009,title={Systematic literature reviews in software engineering - A systematic literature review},year={2009},author={Barbara Kitchenham and Barbara Kitchenham and O. P. Brereton and O. Pearl Brereton and David Budgen and David Budgen and Mark Turner and Mark Turner and John Bailey and John Bailey and John W. Bailey and Stephen Linkman and Stephen Linkman},doi={10.1016/j.infsof.2008.09.009},pmid={null},pmcid={null},mag_id={2106956101},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Nikolić_2009,title={Co-Evolutionary Method For Modelling Large Scale Socio-Technical Systems Evolution},year={2009},author={Igor Nikolić and Igor Nikolic},doi={null},pmid={null},pmcid={null},mag_id={2107834435},journal={null},abstract={Exactly predicting the future of an evolving large scale socio-technical system is impossible. Yet, if we are to sustainably manage the industrial and infrastructure systems our society depends on, we must understand how the actions we take today will affect the evolution of these systems. Simulating how the social and technical networks co-evolve over time allows us to explore possible system futures. This knowledge can help today’s decision makers to steer the system away from undesirable evolutionary pathways. Creating models that capture the complexity of socio-technical systems co-evolution requires multiple formalisms to be encoded in a modeling framework that itself evolves. This thesis presents a method for creating Agent Based Models that suitably represent complex evolving systems. The method involves a co-evolution between the technical aspects of model development, the social process involving the stakeholders, the collection of relevant domain knowledge and the encoding of facts. Through seven case studies the method is demonstrated to yield subsequent generations of richer and ever more useful simulation models.}}
@ARTICLE{Ågotnes_2009,title={Power in normative systems},year={2009},author={Thomas Ågotnes and Thomas Ågotnes and Wiebe van der Hoek and Wiebe van der Hoek and Moshe Tennenholtz and Moshe Tennenholtz and Michael Wooldridge and Michael Wooldridge},doi={null},pmid={null},pmcid={null},mag_id={2108053427},journal={null},abstract={Power indices such as the Banzhaf index were originally developed within voting theory in an attempt to rigorously characterise the influence that a voter is able to wield in a particular voting game. In this paper, we show how such power indices can be applied to understanding the relative importance of agents when we attempt to devise a coordination mechanism using the paradigm of social laws, or normative systems. Understanding how pivotal an agent is with respect to the success of a particular social law is of benefit when designing such social laws: we might typically aim to ensure that power is distributed evenly amongst the agents in a system, to avoid bottlenecks or single points of failure. After formally defining the framework and illustrating the role of power indices in it, we investigate the complexity of computing these indices, showing that the characteristic complexity result is #P-completeness. We then investigate cases where computing indices is computationally easy.}}
@ARTICLE{Cleland‐Huang_2007,title={Best Practices for Automated Traceability},year={2007},author={Jane Cleland‐Huang and Jane Cleland-Huang and Raffaella Settimi and Raffaella Settimi and Е.В. Романова and E. Romanova and Brian Berenbach and B. Berenbach and B. Berenbach and B. Berenbach and B. Berenbach and Brian Berenbach and Steven M. Clark and S. Clark},doi={10.1109/mc.2007.195},pmid={null},pmcid={null},mag_id={2108155297},journal={IEEE Computer},abstract={Automated traceability applies information-retrieval techniques to generate candidate links, sharply reducing the effort of manual approaches to build and maintain a requirements trace matrix as well as providing after-the-fact traceability in legacy documents.The authors describe nine best practices for implementing effective automated traceability.}}
@ARTICLE{Davis_2006,title={Effectiveness of Requirements Elicitation Techniques: Empirical Results Derived from a Systematic Review},year={2006},author={A.M. Davis and Alan M. Davis and Al Davis and Óscar Dieste and Oscar Dieste and Ann M. Hickey and Ann M. Hickey and Natália Juristo and Natalia Juristo and Ana M. Moreno and Ana Moreno},doi={10.1109/re.2006.17},pmid={null},pmcid={null},mag_id={2108410964},journal={null},abstract={This paper reports a systematic review of empirical studies concerning the effectiveness of elicitation techniques, and the subsequent aggregation of empirical evidence gathered from those studies. The most significant results of the aggregation process are as follows: (1) Interviews, preferentially structured, appear to be one of the most effective elicitation techniques; (2) Many techniques often cited in the literature, like card sorting, ranking or thinking aloud, tend to be less effective than interviews; (3) Analyst experience does not appear to be a relevant factor; and (4) The studies conducted have not found the use of intermediate representations during elicitation to have significant positive effects. It should be noted that, as a general rule, the studies from which these results were aggregated have not been replicated, and therefore the above claims cannot be said to be absolutely certain. However, they can be used by researchers as pieces of knowledge to be further investigated and by practitioners in development projects, always taking into account that they are preliminary findings.}}
@ARTICLE{Farzindar_2004,title={LetSum, an automatic Legal Text Summarizing system},year={2004},author={Atefeh Farzindar and Atefeh Farzindar and Guy Lapalme and Guy Lapalme},doi={null},pmid={null},pmcid={null},mag_id={2108664951},journal={null},abstract={This paper presents our work on the development of a new methodology for automatic summarization of justice decision. We describe LetSum (Legal text Sum- marizer), a prototype system, which determines the thematic structure of a judgment in four themes INTRODUCTION, CONTEXT, JURIDICAL ANALYSIS and CONCLUSION. Then it identifies the relevant sentences for each theme. We discuss the evaluation of produced summaries with statistical method and also human evaluation based on jurist judgment. The results so far indicate good performance of the system when compared with other summarization technologies.}}
@ARTICLE{Perry_1994,title={Dimensions of software evolution},year={1994},author={Perry and Perry},doi={10.1109/icsm.1994.336765},pmid={null},pmcid={null},mag_id={2108944883},journal={null},abstract={Software evolution is usually considered in terms of corrections, improvements and enhancements. While helpful, this approach does not take into account the fundamental dimensions of well-engineered software systems (the domains, experience, and process) and how they themselves evolve and affect the evolution of systems for which they are the context. I discuss each dimension, provide examples to illustrate its various aspects and summarize how evolution in that dimension affects system evolution. Only by taking this holistic approach to evolution can we understand evolution and effectively manage it. >}}
@ARTICLE{Verheij_1998,title={An Integrated View on Rules and Principles},year={1998},author={Bart Verheij and Bart Verheij and Jaap Hage and Jaap Hage and H.J. van den Herik and H. Jaap van den Herik},doi={10.1023/a:1008247812801},pmid={null},pmcid={null},mag_id={2109132676},journal={Artificial Intelligence and Law},abstract={In the law, it is generally acknowledged that there are intuitive differences between reasoning with rules and reasoning with principles. For instance, a rule seems to lead directly to its conclusion if its condition is satisfied, while a principle seems to lead merely to a reason for its conclusion. However, the implications of these intuitive differences for the logical status of rules and principles remain controversial.

A radical opinion has been put forward by Dworkin (1978). The intuitive differences led him to argue for a strict logical distinction between rules and principles. Ever since, there has been a controversy whether the intuitive differences between rules and principles require a strict logical distinction between the two. For instance, Soeteman (1991) disagrees with Dworkin's opinion, and argues that rules and principles cannot be strictly distinguished, and do not have a different logical structure.

In this paper, we claim that the differences between rules and principles are merely a matter of degree. We give an integrated view on rules and principles in which rules and principles have the same logical structure, but different behavior in reasoning. In this view, both rules and principles are considered to consist of a condition and a conclusion. The observed differences between rules and principles are, in our view, the result of different types of relations that they have with other rules and principles. In the integrated view, typical rules and typical principles are the extremes of a spectrum.

We support our claim by giving an explicit formalization of our integrated view using the recently developed formal tools provided by Reason-Based Logic. As an application of our view on rules and principles, we give three ways of reconstructing reasoning by analogy.}}
@ARTICLE{Bex_2015,title={An integrated theory of causal stories and evidential arguments},year={2015},author={Floris Bex and Floris Bex},doi={10.1145/2746090.2746094},pmid={null},pmcid={null},mag_id={2109873999},journal={null},abstract={In the process of proof alternative stories that explain 'what happened' in a case are tested using arguments based on evidence. Building on the author's earlier hybrid theory, this paper presents a formal theory that combines causal stories and evidential arguments, further integrating the different types of reasoning in a framework for structured argumentation. This then allows for correct reasoning with causal and evidential rules, and further integrates arguments and stories by grounding them both in well-known dialectical argumentation semantics.}}
@ARTICLE{Shortliffe_1976,title={Computer-based medical consultations, MYCIN},year={1976},author={Edward H. Shortliffe},doi={null},pmid={null},pmcid={null},mag_id={2110293626},journal={null},abstract={null}}
@ARTICLE{Rysselberghe_2003,title={Reconstruction of successful software evolution using clone detection},year={2003},author={F. Van Rysselberghe and F. Van Rysselberghe and Serge Demeyer and Serge Demeyer},doi={10.1109/iwpse.2003.1231219},pmid={null},pmcid={null},mag_id={2110440639},journal={null},abstract={In modern software engineering, researchers regard a software system as an organic life form that must continue to evolve to remain successful. Unfortunately, little is known about how successful software systems have evolved, and consequently little has been learned from previous experience. We demonstrate a heuristic to reconstruct evolution processes of existing software systems by exploiting techniques to detect duplication in large amounts of data. A case study, evaluating various versions of Tomcat using this heuristic, revealed that the removal of duplicated code is a much smaller concern than grouping functionality in classes with one clear responsibility.}}
@ARTICLE{Ghose_1999,title={A formal basis for consistency, evolution and rationale management in requirements engineering},year={1999},author={Aditya Ghose and Aditya Ghose},doi={10.1109/tai.1999.809769},pmid={null},pmcid={null},mag_id={2110522568},journal={null},abstract={This paper presents a formal framework that addresses the twin problems of inconsistencies in requirements specifications and requirements evolution. It presents techniques (building on results from the areas of default reasoning and belief revision) for identifying maximal consistent subsets of a specification rendered inconsistent by a change step, with provision for retaining requirements that would be otherwise discarded, in anticipation of their future reuse. The paper identifies the need for consistent application of requirements rationale and provides support for this in the framework. While the problem of requirements evolution is intractable in the general case, tractable special cases exist within the framework. The paper also provides pointers to designing tools based on this framework.}}
@ARTICLE{Zowghi_1997,title={A logical framework for modeling and reasoning about the evolution of requirements},year={1997},author={Didar Zowghi and Didar Zowghi and Ray Offen and R. Offen},doi={10.1109/isre.1997.566875},pmid={null},pmcid={null},mag_id={2110537096},journal={Requirements Engineering},abstract={We present a logical framework for modeling and reasoning about the evolution of requirements. We demonstrate how a sufficiently rich meta level logic can formally capture intuitive aspects of managing changes to requirements models, while maintaining completeness and consistency. We consider a theory as the deductive closure of a given set of axioms and conclude that software engineering is concerned, in essence, with, building and managing large theories. This theory construction commences with the development of the requirements model which we view as a theory of some nonmonotonic logic. Requirements evolution then involves the mapping of one such theory to another. Exploiting the deductive power of the theory of belief revision and nonmonotonic reasoning we develop a formal description of this mapping, as well as the requirements engineering process itself. This work thus offers a rigorous approach to reasoning about requirements evolution and a important focus for defining semantically well founded methods and tools for the effective management of changing requirements.}}
@ARTICLE{Carter_2001,title={Evolving beyond requirements creep: a risk-based evolutionary prototyping model},year={2001},author={R.A. Carter and Ryan A. Carter and Annie I. Antón and Annie I. Antón and Aldo Dagnino and Aldo Dagnino and Laurie Williams and Laurie Williams},doi={10.1109/isre.2001.948548},pmid={null},pmcid={null},mag_id={2112086934},journal={Requirements Engineering},abstract={Evolutionary prototyping focuses on gathering a correct and consistent set of requirements. The process lends particular strength to building quality software by means of the ongoing clarification of existing requirements and the discovery of previously missing or unknown requirements. Traditionally, the iterative reexamination of a systems requirements has not been the panacea that practitioners sought, due to the predisposition for requirements creep and the difficulty in managing it. The paper proposes the combination of evolutionary prototyping and an aggressive risk mitigation strategy. Together, these techniques support successful requirements discovery and clarification, and they guard against the negative effects of requirements creep. We embody these techniques in a comprehensive software development model, which we call the EPRAM (Evolutionary Prototyping with Risk Analysis and Mitigation) model. The model was intentionally designed to comply with the Level 2 Key Process Area of the Software Engineering Institute's Capability Maturity Model. Validation is currently underway on several software development efforts that employ the model to support the rapid development of electronic commerce applications.}}
@ARTICLE{Bench‐Capon_2016,title={Dilemmas and paradoxes: cycles in argumentation frameworks},year={2016},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1093/logcom/exu011},pmid={null},pmcid={null},mag_id={2112102618},journal={Journal of Logic and Computation},abstract={In this paper I offer an interpretation of cycles in Dung-style argumentation frameworks, in which even length cycles are treated as dilemmas and odd length cycles as paradoxes. The different properties of cycles with different parities arising from the use of preferred semantics are argued to be coherent with this interpretation.}}
@ARTICLE{McCarty_1989,title={A language for legal Discourse I. basic features},year={1989},author={L. Thorne McCarty and L. T. McCarty},doi={10.1145/74014.74037},pmid={null},pmcid={null},mag_id={2112877948},journal={null},abstract={In two previous papers on the prospects for intelligent legal information systems 122, 271, I advocated the development of “deep conceptual models” of particular legal domains. My motivation was both practical and theoretical. On the practical side, I argued, our long-term goal should be an integrated analysis/planning/retrieval system that matches as closely as possible the way a lawyer actually thinks about a legal problem. On the theoretical side, my work with Sridharan on the TAXMAN project [31, 32, 441 had clarified the importance of an adequate domain theory in any attempt to model the arguments of lawyers in hard cases, For both purposes, I claimed, deep conceptual models are essential.}}
@ARTICLE{Bex_2013,title={Legal stories and the process of proof},year={2013},author={Floris Bex and Floris Bex and Bart Verheij and Bart Verheij},doi={10.1007/s10506-012-9137-4},pmid={null},pmcid={null},mag_id={2113691608},journal={Artificial Intelligence and Law},abstract={In this paper, we continue our research on a hybrid narrative-argumentative approach to evidential reasoning in the law by showing the interaction between factual reasoning (providing a proof for `what happened' in a case) and legal reasoning (making a decision based on the proof). First we extend the hybrid theory by making the connection with reasoning towards legal consequences. We then emphasise the role of legal stories (as opposed to the factual stories of the hybrid theory). Legal stories provide a coherent, holistic legal perspective on a case. They steer what needs to be proven but are also selected on the basis of what can be proven. We show how these legal stories can be used to model a shift of the legal perspective on a case, and we discuss how gaps in a legal story can be filled using a factual story (i.e. the process of reasoning with circumstantial evidence). Our model is illustrated by a discussion of the Dutch Wamel murder case.}}
@ARTICLE{He_2006,title={Ensuring compliance between policies, requirements and software design: a case study},year={2006},author={Qingfeng He and Qingfeng He and Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón and Laurie A Jones and Laurie A. Jones},doi={10.1109/iwia.2006.7},pmid={null},pmcid={null},mag_id={2113870525},journal={null},abstract={Specifying correct and complete access control policies is essential to secure data and ensure privacy in information systems. Traditionally, policy specification has not been an explicit part of the software development process. This isolation of policy specification from software development often results in policies that are not in compliance with system requirements and/or organizational security and privacy policies, leaving the system vulnerable to data breaches. This paper presents the results and lessons learned from a case study that employs the Requirements-based Access Control Analysis and Policy Specification (ReCAPS) method to specify access control policies for a Web-based event registration system. The ReCAPS method aids software and security engineers in specifying access control policies derived from requirements specifications and other available sources. Our case study revealed that the ReCAPS method helps identify inconsistencies across various software artifacts, such as requirements specification, database design, and organizational security and privacy policies. Had these problems not been identified and resolved, they would have crippled later phases of software development, resulted in missing or incomplete system functionality, and compromised the system's security and privacy. This case study reinforces, validates, and extends our previous recommendations that access control policy specification should be an integral part of the software development process for information systems to achieve information assurance and improve the quality of the information system.}}
@ARTICLE{Bennett_2001,title={An Architectural model for service-based software with ultra rapid evolution},year={2001},author={Keith H. Bennett and Keith H. Bennett and Malcolm Munro and Malcolm Munro and Nicolas Gold and Nicolas Gold and P. Layzell and Paul J. Layzell and David Budgen and David Budgen and Pearl Brereton and Pearl Brereton},doi={10.1109/icsm.2001.972742},pmid={null},pmcid={null},mag_id={2113950443},journal={null},abstract={There is an urgent industrial need for new approaches to software evolution that will lead to far faster implementation of software changes. For the past 40 years, the techniques, processes and methods of software development have been dominated by supply side issues, and as a result the software industry is oriented towards developers rather than users. Existing software maintenance processes are simply too slow to meet the needs of many businesses. To achieve the levels of functionality, flexibility and time to market of changes and updates required by users, a radical shift is required in the development of software, with a more demand-centric view leading to software which will be delivered as a service, within the framework of an open marketplace. Although there are some signs that this approach is being adopted by industry, it is in a very limited and restricted form. We summarise research that has resulted in a long term strategic view of software engineering innovation. Based on this foundation, we describe more recent work that has resulted in an innovative demand-led model for the future of software. We describe a service architecture in which components may be bound instantly, just at the time they are needed and then the binding may be disengaged. Such ultra late binding requires that many non-functional attributes of the software are capable of automatic negotiation and resolution. Some of these attributes have been demonstrated and amplified through a prototype implementation based on existing and available technology.}}
@ARTICLE{Robinson_2005,title={Implementing Rule-Based Monitors within a Framework for Continuous Requirements Monitoring},year={2005},author={William Robinson and William N. Robinson},doi={10.1109/hicss.2005.306},pmid={null},pmcid={null},mag_id={2113978656},journal={null},abstract={With the increasing complexity of information systems, it is becoming increasingly unclear as to how information system behaviors relate to stated requirements. Although requirements documents and Business Activity Monitoring can provide static and dynamic evidence for requirements compliance, neither provides a formal, real-time presentation of requirements satisfaction. The REQMON research project is constructing and validating methods and tools for requirements specification and real-time monitoring. The challenge is to simplify monitoring system construction while ensuring the fidelity and expressiveness of its feedback. To address this challenge, our integrative approach leverages a formal monitoring abstraction layer, dynamically configurable distributed monitors, and commercial software to define a theory for specifying, developing, and analyzing requirements monitoring systems. This article presents an implementation of rule-based monitors, which are derived from system requirements. Such an implementation can simplify the specification of temporal requirements monitors and can be efficient, as our analysis shows.}}
@ARTICLE{Antón_2003,title={Functional paleontology: the evolution of user-visible system services},year={2003},author={Annie I. Antón and Annie I. Antón and Colin Potts and Colin Potts},doi={10.1109/tse.2003.1178053},pmid={null},pmcid={null},mag_id={2114207521},journal={IEEE Transactions on Software Engineering},abstract={It has long been accepted that requirements analysis should precede architectural design and implementation, but in software evolution and reverse engineering this concern with black-box analysis of function has necessarily been de-emphasized in favor of code-based analysis and designer-oriented interpretation. In this paper, we redress this balance by describing "functional paleontology," an approach to analyzing the evolution of user-visible features or services independent of architecture and design intent. We classify the benefits and burdens of interpersonal communication services into core and peripheral categories and investigate the telephony services available to domestic subscribers over a 50-year period. We report that services were introduced in discrete bursts, each of which emphasized different benefits and burdens. We discuss the general patterns of functional evolution that this "fossil record" illustrates and conclude by discussing their implications for forward engineering of software products.}}
@ARTICLE{Roth_2004,title={Dialectical arguments and case comparison},year={2004},author={Bram Roth and Bram Roth and Bart Verheij and Bart Verheij},doi={null},pmid={null},pmcid={null},mag_id={2114288113},journal={null},abstract={The basis of legal case-based reasoning is the doctrine of stare decisis: decisions in new cases should follow decisions in similar old cases. This paper takes as a starting point the 'case comparison' interpretation of the stare decisis doctrine. In this interpretation one establishes by case c omparison which previously decided cases are sufficiently similar to a new case, after which the old conclusions are adopted in the new case. The paper shows how one ca n formally account for case comparison in terms of the dialectical arguments that cases give rise to. An innovation over previous work is that dialectical arguments are now formally defined, yielding a more transparent formal treatment of case comparison.}}
@ARTICLE{Hepler_2007,title={Object-Oriented Graphical Representations of Complex Patterns of Evidence},year={2007},author={Amanda B. Hepler and Amanda B. Hepler and A. Philip Dawid and A. Philip Dawid and Valentina Leucari and Valentina Leucari},doi={10.1093/lpr/mgm005},pmid={null},pmcid={null},mag_id={2115089538},journal={Law, Probability and Risk},abstract={We reconsider two graphical aids to handling complex mixed masses of evidence in a legal case: Wigmore charts and Bayesian networks. Our aim is to forge a synthesis of their best features and to develop this further to overcome remaining limitations. One important consideration is the multilayered nature of a complex case, which can involve direct evidence, ancillary evidence, evidence about ancillary evidence, etc. all of a number of different kinds. If all these features are represented in one diagram, the result can be messy and hard to interpret. In addition, there are often recurrent features and patterns of evidence and evidential relations, e.g. credibility processes or match identification (DNA, eyewitness evidence, etc.), that may appear, in identical or similar form, at many different places within the same network, or within several different networks, and it is wasteful to model all these individually. The recently introduced technology of ‘object-oriented BNs’ suggests a way of dealing with these problems. Any network can itself contain instances of other networks, the details of which can be hidden from view until information on their detailed structure is desired. Moreover, generic networks to represent recurrent patterns of evidence can be constructed once and for all and copied or edited for reuse as needed. We describe the potential of this mode of description to simplify the construction and display of complex legal cases. To facilitate our narrative, the celebrated Sacco and Vanzetti murder case is used to illustrate the various methods discussed.}}
@ARTICLE{Dunne_2009,title={The computational complexity of ideal semantics},year={2009},author={Paul E. Dunne and Paul E. Dunne},doi={10.1016/j.artint.2009.09.001},pmid={null},pmcid={null},mag_id={2115268056},journal={Artificial Intelligence},abstract={We analyse the computational complexity of the recently proposed ideal semantics within both abstract argumentation frameworks (afs) and assumption-based argumentation frameworks (abfs). It is shown that while typically less tractable than credulous admissibi-lity semantics, the natural decision problems arising with this extension-based model can, perhaps surprisingly, be decided more efficiently than sceptical preferred semantics. In particular the task of finding the unique ideal extension is easier than that of deciding if a given argument is accepted under the sceptical semantics. We provide efficient algorithmic approaches for the class of bipartite argumentation frameworks and, finally, present a number of technical results which offer strong indications that typical problems in ideal argumentation are complete for the class p"@?^C of languages decidable by polynomial time algorithms allowed to make non-adaptive queries to a C oracle, where C is an upper bound on the computational complexity of deciding credulous acceptance: C=np for afs and logic programming (lp) instantiations of abfs; [emailprotected]"2^p for abfs modelling default theories.}}
@ARTICLE{Bench‐Capon_2001,title={A Quantitative Approach to Theory Coherence},year={2001},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Giovanni Sartor and Giovanni Sartor},doi={null},pmid={null},pmcid={null},mag_id={2115327068},journal={null},abstract={In this paper we describe an approach which can attempt to give a quanti- tative measure of the coherence of legal theories. Starting from the notion of a theory as described in (3), we show how some ideas of Thagard can be adapted to give a measure of how coherent such a theory is. Some experiments, using the wild animals domain and theories concerning it discussed in (3) are described. We then identify some issues, both of detail and principle, that are raised by this work.}}
@ARTICLE{Vanek_2008,title={Systems engineering metrics and applications in product development: A critical literature review and agenda for further research},year={2008},author={Francis Vanek and Francis Vanek and Peter L. Jackson and Peter L. Jackson and Richard Grzybowski and Richard R. Grzybowski},doi={10.1002/sys.v11:2},pmid={null},pmcid={null},mag_id={2115805127},journal={Systems Engineering},abstract={The military-aerospace industries have several decades of experience in the application of systems engineering. In the world of complex systems developed under contract, systems engineering has a well-articulated, generally accepted, client-mandated methodology. This is not the case currently in most commercially oriented research and development organizations. Unlike the military-aerospace environment, it is not widely accepted that systems engineering can or will deliver value within commercial enterprises. This said, it is possible that a systems engineering-like methodology may be routinely employed but not so stated, or is recognized under a different label. This seems likely since complex products continue to be successfully developed and commercialized via the use of methods that are closely related to systems engineering. The purpose of this paper is to establish the groundwork for measuring the effectiveness of the systems engineering methodology, and closely associated analogs, as part of the product development process in a commercial research and development organization. In this paper, we review the literature in systems engineering and related fields to identify measurement issues and methodologies, specific metrics proposed and in use, and case studies and best practices. We also identify gaps in the literature and propose the next steps in a research agenda. © 2008 Wiley Periodicals, Inc. Syst Eng}}
@ARTICLE{Kogan_2009,title={Predicting Risk from Financial Reports with Regression},year={2009},author={Shimon Kogan and Shimon Kogan and Dimitry Levin and Dimitry Levin and Bryan R. Routledge and Bryan R. Routledge and Jacob S. Sagi and Jacob S. Sagi and Noah A. Smith and Noah A. Smith},doi={10.3115/1620754.1620794},pmid={null},pmcid={null},mag_id={2116209939},journal={null},abstract={We address a text regression problem: given a piece of text, predict a real-world continuous quantity associated with the text's meaning. In this work, the text is an SEC-mandated financial report published annually by a publicly-traded company, and the quantity to be predicted is volatility of stock returns, an empirical measure of financial risk. We apply well-known regression techniques to a large corpus of freely available financial reports, constructing regression models of volatility for the period following a report. Our models rival past volatility (a strong baseline) in predicting the target variable, and a single model that uses both can significantly outperform past volatility. Interestingly, our approach is more accurate for reports after the passage of the Sarbanes-Oxley Act of 2002, giving some evidence for the success of that legislation in making financial reports more informative.}}
@ARTICLE{Irvine_2002,title={An Approach to Security Requirements Engineering for a High Assurance System},year={2002},author={Cynthia E. Irvine and Cynthia E. Irvine and Timothy E. Levin and Timothy E. Levin and Jeffery D. Wilson and Jeffery D. Wilson and David J. Shifflett and David J. Shifflett and Bárbara Bruna Maniçoba Pereira and Barbara Pereira},doi={10.1007/s007660200015},pmid={null},pmcid={null},mag_id={2116447054},journal={Requirements Engineering},abstract={Requirements specifications for high-assurance secure systems are rare in the open literature. This paper examines the development of a requirements document for a multilevel secure system that must meet stringent assurance and evaluation requirements. The system is designed to be secure, yet combines popular commercial components with specialised high-assurance ones. Functional and non-functional requirements pertinent to security are discussed. A multidimensional threat model is presented. The threat model accounts for the developmental and operational phases of system evolution and for each phase accounts for both physical and non-physical threats. We describe our team-based method for developing a requirements document and relate that process to techniques in requirements engineering. The system requirements document presented provides a calibration point for future security requirements engineering techniques intended to meet both functional and assurance goals.}}
@ARTICLE{Lockerbie_2010,title={Using i* Modelling as a Bridge between Air Traffic Management Operational Concepts and Agent-based Simulation Analysis},year={2010},author={James Lockerbie and James Lockerbie and David M. Bush and David Bush and Neil Maiden and Neil Maiden and H.A.P. Blom and Henk A. P. Blom and Henk A. P. Blom and M.H.C. Everdij and Mariken H. C. Everdij},doi={10.1109/re.2010.50},pmid={null},pmcid={null},mag_id={2117288675},journal={null},abstract={This paper presents our experiences of exploring how i* modelling can be used as a bridge between informal air traffic management (ATM) operational concepts and formal agent-based simulation. We report our work on an EU airspace design project that describes a revised concept of operations for lower level airspace around airports, and includes simulation based safety analysis of the critical scenarios. We describe our research towards using i* to address two challenges – how to model the revised concept from the informal concept of operations document, and how to present safety critical scenarios to operational experts. Modelling strategic aspects of a concept of operations is new to ATM, and we draw upon our experiences to provide lessons learned and directions for future work.}}
@ARTICLE{Perry_1992,title={Foundations for the study of software architecture},year={1992},author={Dewayne E. Perry and Dewayne E. Perry and Alexander L. Wolf and Alexander L. Wolf},doi={10.1145/141874.141884},pmid={null},pmcid={null},mag_id={2118023438},journal={ACM Sigsoft Software Engineering Notes},abstract={The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architecture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements --- that is, the constraints on the elements. The rationale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system requirements. We discuss the components of the model in the context of both architectures and architectural styles and present an extended example to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, summarizing our contributions, and relating our approach to other current work.}}
@ARTICLE{Cysneiros_2002,title={Requirements engineering in the health care domain},year={2002},author={Luiz Marcio Cysneiros and Luiz Marcio Cysneiros},doi={10.1109/icre.2002.1048548},pmid={null},pmcid={null},mag_id={2118154574},journal={null},abstract={There are many different approaches to elicit requirements, each having strengths and weaknesses. Hence, some approaches may be more suitable to one domain than another. Moreover, some domains may require these approaches to be carefully applied or even adapted to work efficiently. Health care domain is one of these domains. It is a complex domain with many subtleties, such as political and legal issues that have to be taken into account. This work brings some of the lessons learned in more than six years working with several hospitals and laboratories. Particularly, this paper presents some elicitation techniques that had to be adapted in order to comply with the constraints imposed by several peculiarities intrinsic to this domain. It also points out some special considerations that must be taken into account regardless the method one chooses to elicit requirements.}}
@ARTICLE{Altman_1990,title={Practical statistics for medical research},year={1990},author={Douglas G. Altman and Douglas G. Altman},doi={null},pmid={null},pmcid={null},mag_id={2118202495},journal={null},abstract={Most medical researchers, whether clinical or non-clinical, receive some background in statistics as undergraduates. However, it is most often brief, a long time ago, and largely forgotten by the time it is needed. Furthermore, many introductory texts fall short of adequately explaining the underlying concepts of statistics, and often are divorced from the reality of conducting and assessing medical research.

Practical Statistics for Medical Research is a problem-based text for medical researchers, medical students, and others in the medical arena who need to use statistics but have no specialized mathematics background. 

The author draws on twenty years of experience as a consulting medical statistician to provide clear explanations to key statistical concepts, with a firm emphasis on practical aspects of designing and analyzing medical research. The text gives special attention to the presentation and interpretation of results and the many real problems that arise in medical research}}
@ARTICLE{Montazeri_2011,title={From Contracts in Structured English to CL Specifications},year={2011},author={Seyed Morteza Montazeri and Seyed Morteza Montazeri and Nivir K.S. Roy and Nivir Kanti Singha Roy and Gerardo Schneider and Gerardo Schneider},doi={10.4204/eptcs.68.6},pmid={null},pmcid={null},mag_id={2119998517},journal={arXiv: Computation and Language},abstract={In this paper we present a framework to analyze conflicts of contracts written in structured English. A contract that has manually been rewritten in a structured English is automatically translated into  a formal language using the Grammatical Framework (GF). In particular we use the contract language CL as a target formal language for this translation. In our framework CL specifications could then be input into the tool CLAN to detect the presence of conflicts (whether there are contradictory obligations, permissions, and prohibitions. We also use GF to get a version in (restricted) English of CL formulae. We discuss the implementation of such a framework.}}
@ARTICLE{Hall_2001,title={Acknowledging insufficiency in the evaluation of legal knowledge-based systems: strategies towards a broadbased evaluation model},year={2001},author={Martin Hall and Maria Jean Johnstone Hall and John Zeleznikow and John Zeleznikow},doi={10.1145/383535.383553},pmid={null},pmcid={null},mag_id={2120581177},journal={null},abstract={This paper considers the need for evaluation of knowledge-based systems in general and legal knowledge-based systems in particular. Some special features of legal knowledge-based systems pertinent to their evaluation are presented. The expected benefits of such evaluations are discussed. and some of the difficulties likely to be encountered are outlined.  The proceedings of four International Conferences on Artificial Intelligence and Law are analysed to determine the rate of reporting evaluations in non-theoretical papers. These papers had a low rate of consideration of evaluation issues reflecting common practice in research biased development environments. These results confirm that more attention to evaluation is needed in the legal knowledge based systems domain.  This paper foreshadows the development of an evaluation methodology tailored specifically for legal knowledge-based systems. Evaluation strategies beyond verification and validation are drawn upon, both from the international ISO/IEC 14598 and 9126 standards and also from previous work on evaluation models for knowledge-based systems.}}
@ARTICLE{Amato_2008,title={Building RDF Ontologies from Semi-Structured Legal Documents},year={2008},author={Flora Amato and Flora Amato and Antonino Mazzeo and Antonino Mazzeo and Antonio Penta and Antonio Penta and Antonio Picariello and Antonio Picariello and A. Picariello},doi={10.1109/cisis.2008.146},pmid={null},pmcid={null},mag_id={2121204325},journal={null},abstract={The increasing interest in the context of e-government requires intelligent techniques for legal information and knowledge management. In this paper we describe a system that, given a number of legal paper documents, automatically transforms them into suitable RDF statements, using several ontological and linguistic knowledge levels. Although we describe a general methodology for a number of application domains, our system is particularly suitable for the notary realm.}}
@ARTICLE{Lucia_2005,title={ADAMS Re-Trace: a traceability recovery tool},year={2005},author={Andrea De Lucia and A. De Lucia and Fausto Fasano and Fausto Fasano and Rocco Oliveto and Rocco Oliveto and Genoveffa Tortora and Genoveffa Tortora},doi={10.1109/csmr.2005.7},pmid={null},pmcid={null},mag_id={2121230168},journal={null},abstract={We present the traceability recovery tool developed in the ADAMS artefact management system. The tool is based on an information retrieval technique, namely latent semantic indexing and aims at supporting the software engineer in the identification of the traceability links between artefacts of different types. We also present a case study involving seven student projects, which represented an ideal workbench for the tool. The results emphasise the benefits provided by the tool in terms of new traceability links discovered, in addition to the links manually traced by the software engineer. Moreover, the tool was also helpful in identifying cases of lack of similarity between artefacts manually traced by the software engineer, thus revealing inconsistencies in the usage of domain terms in these artefacts. This information is valuable to assess the quality of the produced artefacts.}}
@ARTICLE{Bench‐Capon_2007,title={Argumentation in artificial intelligence},year={2007},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Paul E. Dunne and Paul E. Dunne},doi={10.1016/j.artint.2007.05.001},pmid={null},pmcid={null},mag_id={2121835362},journal={Artificial Intelligence},abstract={Over the last ten years, argumentation has come to be increasingly central as a core study within Artificial Intelligence (AI). The articles forming this volume reflect a variety of important trends, developments, and applications covering a range of current topics relating to the theory and applications of argumentation. Our aims in this introduction are, firstly, to place these contributions in the context of the historical foundations of argumentation in AI and, subsequently, to discuss a number of themes that have emerged in recent years resulting in a significant broadening of the areas in which argumentation based methods are used. We begin by presenting a brief overview of the issues of interest within the classical study of argumentation: in particular, its relationship-in terms of both similarities and important differences-to traditional concepts of logical reasoning and mathematical proof. We continue by outlining how a number of foundational contributions provided the basis for the formulation of argumentation models and their promotion in AI related settings and then consider a number of new themes that have emerged in recent years, many of which provide the principal topics of the research presented in this volume.}}
@ARTICLE{Verheij_2009,title={Accepting the Truth of a Story about the Facts of a Criminal Case},year={2009},author={Bart Verheij and Bart Verheij and Floris Bex and Floris Bex},doi={null},pmid={null},pmcid={null},mag_id={2122042124},journal={null},abstract={null}}
@ARTICLE{Yu_2009,title={Combining Vector Space Model and Category Hierarchy Model for TV Content Similarity Measure},year={2009},author={Zhiwen Yu and Zhiwen Yu and Xingshe Zhou and Xingshe Zhou},doi={10.1109/mue.2009.33},pmid={null},pmcid={null},mag_id={2122205066},journal={null},abstract={In this paper, we propose a new approach for TV content similarity measure, which combines both vector space model and category hierarchy model. The hybrid measure proposed here makes the most of TV metadata information and takes advantage of the two similarity measurements. It measures TV content similarity from the semantic level other than the physical level. Furthermore, we propose an adaptive strategy for setting the combination parameters. The experimental results showed that using the combining approach proposed here is superior to using either similarity measure alone for example-based retrieval of TV content.}}
@ARTICLE{Nekvi_2015,title={Impediments to Regulatory Compliance of Requirements in Contractual Systems Engineering Projects: A Case Study},year={2015},author={Rashed Iqbal Nekvi and Rashed I. Nekvi and Nazim H. Madhavji and Nazim H. Madhavji},doi={10.1145/2629432},pmid={null},pmcid={null},mag_id={2122243664},journal={null},abstract={Large-scale contractual systems engineering projects often need to comply with myriad government regulations and standards as part of contractual obligations. A key activity in the requirements engineering (RE) process for such a project is to demonstrate that all relevant requirements have been elicited from the regulatory documents and have been traced to the contract as well as to the target system components. That is, the requirements have met regulatory compliance. However, there are impediments to achieving this level of compliance due to such complexity factors as voluminous contract, large number of regulatory documents, and multiple domains of the system. Little empirical research has been conducted in the scientific community on identifying these impediments. Knowing these impediments is a driver for change in the solutions domain (i.e., creating improved or new methods, tools, processes, etc.) to deal with such impediments. Through a case study of an industrial RE project, we have identified a number of key impediments to achieving regulatory compliance in a large-scale, complex, systems engineering project. This project is an upgrade of a rail infrastructure system. The key contribution of the article is a number of hitherto uncovered impediments described in qualitative and quantitative terms. The article also describes an artefact model, depicting key artefacts and relationships involved in such a compliance project. This model was created from data gathered and observations made in this compliance project. In addition, the article describes emergent metrics on regulatory compliance of requirements that can possibly be used for estimating the effort needed to achieve regulatory compliance of system requirements.}}
@ARTICLE{Robles_2005,title={Evolution and growth in large libre software projects},year={2005},author={Gregório Robles and Gregorio Robles and Juan José Amor and Juan Jose Amor and Jesús M. González-Barahona and Jesus M. Gonzalez-Barahona and Israel Herraiz and Israel Herraiz},doi={10.1109/iwpse.2005.17},pmid={null},pmcid={null},mag_id={2122944092},journal={null},abstract={Software evolution research has recently focused on new development paradigms, studying whether laws found in more classic development environments also apply. Previous works have pointed out that at least some laws seem not to be valid for these new environments and even Lehman has labeled those (up to the moment few) cases as anomalies and has suggested that further research is needed to clarify this issue. In this line, we consider in this paper a large set of libre (free, open source) software systems featuring a large community of users and developers. In particular, we analyze a number of projects found in literature up to now, including the Linux kernel. For comparison, we include other libre software kernels from the BSD family, and for completeness we consider a wider range of libre software applications. In the case of Linux and the other operating system kernels we have studied growth patterns also at the subsystem level. We have observed in the studied sample that super-linearity occurs only exceptionally, that many of the systems follow a linear growth pattern and that smooth growth is not that common. These results differ from the ones found generally in classical software evolution studies. Other behaviors and patterns give also a hint that development in the libre software world could follow different laws than those known, at least in some cases.}}
@ARTICLE{Ji_2006,title={Poirot: A Distributed Tool Supporting Enterprise-Wide Automated Traceability},year={2006},author={Lin Ji and Jun Lin and Chan Chou Lin and Chan Chou Lin and J.C. Huang and Jui-Ya Huang and Jane Cleland-Huang and J.C. Huang and J.C. Huang and J.C. Huang and J.C. Huang and J.C. Huang and J.C. Huang and Raffaella Settimi and Raffaella Settimi and J. Amaya and J. Amaya and G. Bedford and G. Bedford and Brian Berenbach and Brian Berenbach and O. Ben Khadra and O.B. Khadra and Chuan Duan and Chuan Duan and Xuchang Zou and Xuchang Zou},doi={10.1109/re.2006.48},pmid={null},pmcid={null},mag_id={2123253300},journal={null},abstract={Poirot is a web-based tool supporting traceability of distributed heterogeneous software artifacts. A probabilistic network model is used to generate traces between requirements, design elements, code and other artifacts stored in distributed 3rd party case tools such as DOORS, rational rose, and source code repositories. The tool is designed with extensibility in mind, so that additional artifact types and 3rd party case tools can be easily added. Trace results are displayed in both a textual and visual format. This paper briefly describes the underlying probabilistic model, and the user interface of the tool, and then discusses Poirot's deployment and use in an industrial setting.}}
@ARTICLE{Brooks_1975,title={The Mythical Man-Month},year={1975},author={Fred Brooks and Frederick P. Brooks},doi={null},pmid={null},pmcid={null},mag_id={2123586642},journal={null},abstract={The book, The Mythical Man-Month, Addison-Wesley, 1975 (excerpted in Datamation, December 1974), gathers some of the published data about software engineering and mixes it with the assertion of a lot of personal opinions. In this presentation, the author will list some of the assertions and invite dispute or support from the audience. This is intended as a public discussion of the published book, not a regular paper.}}
@ARTICLE{Gardner_1985,title={An artificial intelligence approach to legal reasoning},year={1985},author={Anne von der Lieth Gardner and Anne von der Lieth Gardner},doi={null},pmid={null},pmcid={null},mag_id={2123761774},journal={null},abstract={For artificial intelligence, understanding the forms of human reasoning is a central goal. Legal reasoning is a form that makes a new set of demands on artificial intelligence methods. Most importantly, a computer program that reasons about legal problems must be able to distinguish between questions it is competent to answer and questions that human lawyers could seriously argue either way. In addition, a program for analyzing legal problems should be able to use both general legal rules and decisions in past cases; and it should be able to work with technical concepts that are only partly defined and subject to shifts of meaning. Each of these requirements has wider applications in artificial intelligence, beyond the legal domain.
This dissertation presents a computational framework for legal reasoning, within which such requirements can be accommodated. The development of the framework draws significantly on the philosophy of law, in which the elucidation of legal reasoning is an important topic. A key element of the framework is the legal distinction between hard cases and clear cases. In legal writing, this distinction has been taken for granted more often than it has been explored. Here, some initial heuristics are proposed by which a program might make the distinction.
The dissertation also describes an implemented program based on the framework. For definiteness, the program is given the task of analyzing law examination problems concerning the formation of contracts by offer and acceptance. The program performs adequately on a narrow class of such problems and appears to be extensible to other areas. Thus, the research opens up a line of investigation that may prove fruitful for both artifical intelligence and legal philosophy.}}
@ARTICLE{Nuseibeh_2000,title={Requirements engineering: a roadmap},year={2000},author={Bashar Nuseibeh and Bashar Nuseibeh and Steve Easterbrook and Steve Easterbrook},doi={10.1145/336512.336523},pmid={null},pmcid={null},mag_id={2124405605},journal={null},abstract={This paper presents an overview of the field of software systems requirements engineering (RE). It describes the main areas of RE practice, and highlights some key open research issues for the future.}}
@ARTICLE{Rifaut_2011,title={Compliance management with measurement frameworks},year={2011},author={André Rifaut and André Rifaut},doi={10.1109/relaw.2011.6050268},pmid={null},pmcid={null},mag_id={2124832057},journal={null},abstract={New regulatory regimes advocate the use of “goaloriented” regulations that are more flexible during regulatory conversations occurring between the regulators and the regulatees when new regulations are introduced. In that context, long-term “compliance agreements” between regulators and regulatees are needed. Using recent developments of the Measurement Theory, this paper shows that the concept of Measurement Framework (MF) for soft-systems is of particular importance for providing those compliance agreements. We show that with two kinds of goals and softgoals based on MF, one can improve (a) the elicitation of compliance requirements, (b) the structure of the compliance arguments for compliant requirements, and (c) the consistency between actual compliance at run-time and the intentional compliance at early stages of Requirements Engineering.}}
@ARTICLE{Bench‐Capon_2002,title={Representation of Case Law as an Argumentation Framework},year={2002},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2125048061},journal={null},abstract={Since their introduction by Dung, Argumentation Frameworks have pro- vided a fruitful basis for studying reasoning in defeasible contexts, including law. As yet, however, no realistic body of case law has been represented as an Argumenta- tion Framework. In this paper we develop an Argumentation Framework of a much discussed body of case law, and draw attention to a number of questions concern- ing approaches to reasoning with cases in AI and Law that can be informed by this exercise.}}
@ARTICLE{Ingolfo_2013,title={Choosing compliance solutions through stakeholder preferences},year={2013},author={Silvia Ingolfo and Silvia Ingolfo and Alberto Siena and Alberto Siena and Ivan Jureta and Ivan Jureta and Angelo Susi and Angelo Susi and Anna Perini and Anna Perini and John Mylopoulos and John Mylopoulos},doi={10.1007/978-3-642-37422-7_15},pmid={null},pmcid={null},mag_id={2127000137},journal={null},abstract={[Context and motivation] Compliance to relevant laws is increasingly recognized as a critical, but also expensive, quality for software requirements. [Question/Problem] Laws contain elements such as conditions and derogations that generate a space of possible compliance alternatives. During requirements engineering, an analyst has to select one of these compliance alternatives and ensure that the requirements specification she is putting together complies with that alternative. However, the space of such alternatives is often large. [Principal ideas and results] This paper extends Nomos 2, a modeling framework for laws, to support modeling of and reasoning with stakeholder preferences and priorities. The problem of preferred regulatory compliance is then defined as a problem of finding a compliance alternative that matches best stakeholder preferences. [Contribution] The paper defines the concept of preference between situations and integrates it with the Nomos 2 modeling language. It also presents a reasoning tool for preferences and illustrates its use with an extract from a use case concerning the Italian law on Electronic Health Record.}}
@ARTICLE{Chantree_2006,title={Identifying Nocuous Ambiguities in Natural Language Requirements},year={2006},author={Francis Chantree and Francis Chantree and Bashar Nuseibeh and Bashar Nuseibeh and Anne De Roeck and A. de Roeck and Anne De Roeck and Alistair Willis and Alistair Willis},doi={10.1109/re.2006.31},pmid={null},pmcid={null},mag_id={2127997520},journal={null},abstract={We present a novel technique that automatically alerts authors of requirements to the presence of potentially dangerous ambiguities. We first establish the notion of nocuous ambiguities, which are those that are likely to lead to misunderstandings. We test our approach on coordination ambiguities, which occur when words such as and and or are used. Our starting point is a dataset of ambiguous phrases from a requirements corpus and associated human judgements about their interpretation. We then use heuristics, based largely on word distribution information, to automatically replicate these judgements. The heuristics eliminate ambiguities which people interpret easily, leaving the nocuous ones to be analysed and rewritten by hand. We report on a series of experiments that evaluate our heuristics' performance against the human judgements. Many of our heuristics achieve high precision, and recall is greatly increased when they are used in combination.}}
@ARTICLE{Boer_2003,title={Using ontologies for comparing and harmonizing legislation},year={2003},author={Alexander Boer and Alexander Boer and Tom van Engers and Tom M. van Engers and Radboud Winkels and Radboud Winkels},doi={10.1145/1047788.1047798},pmid={null},pmcid={null},mag_id={2128466244},journal={null},abstract={In the last decades the interst in the problem of comparing and harmonizing legislation has been steadily increasing. One reason is the increasing legal convergence between governments in the European Union, and the increasing traffic of people over borders of jurisdictions. Another reason is the increasing globalization of companies; Products and services are offered in many jurisdictions at the same time, and the product or service has to meet the provisions of all jurisdictions in which it is offered. In the E-POWER project relevant tax legislation and business processes are modeled in UML to improve the speed and efficiency with which the Dutch Tax and Customs Administration can implement decision support systems for internal use and for its clients. These conceptual models have also proven their usefulness for efficient and effective analysis of draft legislation. We are currently researching whether conceptual modeling can also be used to compare 'similar' legislation from different jurisdictions to improve the capacity of the Dutch Tax and Customs Administration to react to future consequences of increased movement of people, products, and money between EU member states and increased harmonization between tax authorities in Europe. In addition, addressing the problem of comparing models is also expected to improve our methodology for modeling legislation. This paper discusses problems and requirements of comparing legislation as we understand them now, and attempts to relate them to relevant research.}}
@ARTICLE{Lam_2009,title={A Formalization of HIPAA for a Medical Messaging System},year={2009},author={Peifung E. Lam and Peifung E. Lam and John C. Mitchell and John C. Mitchell and Sharada Sundaram and Sharada Sundaram},doi={10.1007/978-3-642-03748-1_8},pmid={null},pmcid={null},mag_id={2128755207},journal={null},abstract={The complexity of regulations in healthcare, financial services, and other industries makes it difficult for enterprises to design and deploy effective compliance systems. We believe that in some applications, it may be practical to support compliance by using formalized portions of applicable laws to regulate business processes that use information systems. In order to explore this possibility, we use a stratified fragment of Prolog with limited use of negation to formalize a portion of the US Health Insurance Portability and Accountability Act (HIPAA). As part of our study, we also explore the deployment of our formalization in a prototype hospital Web portal messaging system.}}
@ARTICLE{Harker_1993,title={The change and evolution of requirements as a challenge to the practice of software engineering},year={1993},author={Susan Harker and S.D.P. Harker and Ken Eason and K.D. Eason and John Dobson and J.E. Dobson},doi={10.1109/isre.1993.324847},pmid={null},pmcid={null},mag_id={2129103796},journal={Requirements Engineering},abstract={The difficulty of handling changing requirements within traditional development processes is described. The origins of changing user and organizational requirements are discussed and different types are classified. The author identifies a number of ways in which different approaches to design may help to deal with change as well as mechanisms which should underpin effective communication between users and designers. >}}
@ARTICLE{Searle_1969,title={Speech Acts: An Essay in the Philosophy of Language},year={1969},author={John R. Searle and John R. Searle},doi={null},pmid={null},pmcid={null},mag_id={2129264276},journal={null},abstract={Part I. A Theory of Speech Acts: 1. Methods and scope 2. Expressions, meaning and speech acts 3. The structure of illocutionary acts 4. Reference as a speech act 5. Predication Part II. Some Applications of the Theory: 6. Three fallacies in contemporary philosophy 7. Problems of reference 8. Deriving 'ought' from 'is' Index.}}
@ARTICLE{Lamsweerde_2009,title={Requirements Engineering: From System Goals to UML Models to Software Specifications},year={2009},author={Axel van Lamsweerde and Axel van Lamsweerde},doi={null},pmid={null},pmcid={null},mag_id={2129289644},journal={null},abstract={Essential comprehensive coverage of the fundamentals of requirements engineering Requirements engineering (RE) deals with the variety of prerequisites that must be met by a software system within an organization in order for that system to produce stellar results. With that explanation in mind, this must-have book presents a disciplined approach to the engineering of high-quality requirements. Serving as a helpful introduction to the fundamental concepts and principles of requirements engineering, this guide offers a comprehensive review of the aim, scope, and role of requirements engineering as well as best practices and flaws to avoid. Shares state-of-the-art techniques for domain analysis, requirements elicitation, risk analysis, conflict management, and more Features in-depth treatment of system modeling in the specific context of engineering requirements Presents various forms of reasoning about models for requirements quality assurance Discusses the transitions from requirements to software specifications to software architecture In addition, case studies are included that complement the many examples provided in the book in order to show you how the described method and techniques are applied in practical situations.}}
@ARTICLE{Galinsky_2008,title={Why It Pays to Get Inside the Head of Your Opponent The Differential Effects of Perspective Taking and Empathy in Negotiations},year={2008},author={Adam D. Galinsky and Adam D. Galinsky and William W. Maddux and William W. Maddux and William W. Maddux and Debra Gilin and Debra Gilin and Judith B. White and Judith B. White},doi={10.1111/j.1467-9280.2008.02096.x},pmid={18399891},pmcid={null},mag_id={2129497853},journal={Psychological Science},abstract={The current research explored whether two re- lated yet distinct social competencies—perspective taking (the cognitive capacity to consider the world from another individual's viewpoint) and empathy (the ability to connect emotionally with another individual)—have differential effects in negotiations. Across three studies, using both individual difference measures and experimental manip- ulations, we found that perspective taking increased in- dividuals' ability to discover hidden agreements and to both create and claim resources at the bargaining table. However, empathy did not prove nearly as advantageous and at times was detrimental to discovering a possible deal and achieving individual profit. These results held re- gardless of whether the interaction was a negotiation in which a prima facie solution was not possible or a multiple- issue negotiation that required discovering mutually ben- eficial trade-offs. Although empathy is an essential tool in many aspects of social life, perspective taking appears to be a particularly critical ability in negotiations.}}
@ARTICLE{Lindahl_2006,title={Intermediate concepts in normative systems},year={2006},author={Lars Lindahl and Lars Lindahl and Jan Odelstad and Jan Odelstad},doi={10.1007/11786849_16},pmid={null},pmcid={null},mag_id={2129871240},journal={null},abstract={In legal theory, a well-known idea is that an intermediate concept like “ownership” joins a set of legal consequences to a set of legal grounds. In our paper, we attempt to make the idea of a joining between grounds and consequences more precise by using an algebraic representation of normative systems earlier developed by the authors. In the first main part, the idea of intermediate concepts is presented and earlier discussions of the subjects are outlined. Subsequently, in the second main part, we introduce a more rigorous framework and develop the formal theory. In the third part, the formal framework is applied to examples and some remarks on a methodology of intermediate concepts are given.}}
@ARTICLE{Giorgini_2005,title={Modeling security requirements through ownership, permission and delegation},year={2005},author={Paolo Giorgini and Paolo Giorgini and Fabio Massacci and Fabio Massacci and John Mylopoulos and John Mylopoulos and Nicola Zannone and Nicola Zannone},doi={10.1109/re.2005.43},pmid={null},pmcid={null},mag_id={2130310983},journal={null},abstract={Security requirements engineering is emerging as a branch of software engineering, spurred by the realization that security must be dealt with early on during the requirements phase. Methodologies in this field are challenging, as they must take into account subtle notions such as trust (or lack thereof), delegation, and permission; they must also model entire organizations and not only systems-to-be. In our previous work we introduced Secure Tropos, a formal framework for modeling and analyzing security requirements. Secure Tropos is founded on three main notions: ownership, trust, and delegation. In this paper, we refine Secure Tropos introducing the notions of at-least delegation and trust of execution; also, at-most delegation and trust of permission. We also propose monitoring as a security design pattern intended to overcome the problem of lack of trust between actors. The paper presents a semantic for these notions, and describes an implemented formal reasoning tool based on Datalog.}}
@ARTICLE{Avesani_2005,title={Facing scalability issues in requirements prioritization with machine learning techniques},year={2005},author={Paolo Avesani and Paolo Avesani and Cinzia Bazzanella and Cinzia Bazzanella and Anna Perini and Anna Perini and Angelo Susi and Angelo Susi},doi={10.1109/re.2005.30},pmid={null},pmcid={null},mag_id={2131207289},journal={null},abstract={Case-based driven approaches to requirements prioritization proved to be much more effective than first-principle methods in being tailored to a specific problem, that is they take advantage of the implicit knowledge that is available, given a problem representation. In these approaches, first-principle prioritization criteria are replaced by a pairwise preference elicitation process. Nevertheless case-based approaches, using the analytic hierarchy process (AHP) technique, become impractical when the size of the collection of requirements is greater than about twenty since the elicitation effort grows as the square of the number of requirements. We adopt a case-based framework for requirements prioritization, called case-based ranking, which exploits machine learning techniques to overcome the scalability problem. This method reduces the acquisition effort by combining human preference elicitation and automatic preference approximation. Our goal in this paper is to describe the framework in details and to present empirical evaluations which aim at showing its effectiveness in overcoming the scalability problem. The results prove that on average our approach outperforms AHP with respect to the trade-off between expert elicitation effort and the requirement prioritization accuracy.}}
@ARTICLE{Allenby_2001,title={Deriving safety requirements using scenarios},year={2001},author={K. Allenby and K. Allenby and Tim Kelly and Tim Kelly},doi={10.1109/isre.2001.948563},pmid={null},pmcid={null},mag_id={2131320354},journal={Requirements Engineering},abstract={Elicitation of requirements for safety critical aero-engine control systems is dependent on the capture of core design intent and the systematic derivation of requirements addressing hazardous deviations from that intent. Derivation of these requirements is inextricably linked to the safety assessment process. Conventional civil aerospace practice (as advocated by guidelines such as ARP4754 and ARP4671) promotes the application of Functional Hazard Assessment (FHA) to sets of statements of functional intent. Systematic hazard analysis of scenario-based requirements representations is less well understood. This paper discusses the principles and problems of hazard analysis and proposes an approach to conducting hazard analysis on use case requirements representations. Using the approach, it is possible to justifiably derive hazard-mitigation use cases as first class requirements from systematic hazard analysis of core design intent scenarios. An industrial example is used to illustrate the technique.}}
@ARTICLE{Leidner_2010,title={Hunting for the Black Swan: Risk Mining from Text},year={2010},author={Jochen L. Leidner and Jochen L. Leidner and Frank Schilder and Frank Schilder},doi={null},pmid={null},pmcid={null},mag_id={2131499132},journal={null},abstract={In the business world, analyzing and dealing with risk permeates all decisions and actions. However, to date, risk identification, the first step in the risk management cycle, has always been a manual activity with little to no intelligent software tool support. In addition, although companies are required to list risks to their business in their annual SEC filings in the USA, these descriptions are often very high-level and vague.

In this paper, we introduce Risk Mining, which is the task of identifying a set of risks pertaining to a business area or entity. We argue that by combining Web mining and Information Extraction (IE) techniques, risks can be detected automatically before they materialize, thus providing valuable business intelligence.

We describe a system that induces a risk taxonomy with concrete risks (e.g., interest rate changes) at its leaves and more abstract risks (e.g., financial risks) closer to its root node. The taxonomy is induced via a bootstrapping algorithms starting with a few seeds. The risk taxonomy is used by the system as input to a risk monitor that matches risk mentions in financial documents to the abstract risk types, thus bridging a lexical gap. Our system is able to automatically generate company specific "risk maps", which we demonstrate for a corpus of earnings report conference calls.}}
@ARTICLE{Governatori_2006,title={A FORMAL ANALYSIS OF A BUSINESS CONTRACT LANGUAGE},year={2006},author={Guido Governatori and Guido Governatori and Zoran Milošević and Zoran Milosevic},doi={10.1142/s0218843006001529},pmid={null},pmcid={null},mag_id={2131518018},journal={International Journal of Cooperative Information Systems},abstract={This paper presents a formal system for reasoning about violations of obligations in contracts. The system is based on the formalism for the representation of contrary-to-duty obligations. These are the obligations that take place when other obligations are violated as typically applied to penalties in contracts. The paper shows how this formalism can be mapped onto the key policy concepts of a contract specification language, called Business Contract Language (BCL), previously developed to express contract conditions for run time contract monitoring. The aim of this mapping is to establish a formal underpinning for this key subset of BCL.}}
@ARTICLE{Earp_2005,title={Examining Internet privacy policies within the context of user privacy values},year={2005},author={Julia B. Earp and Julia B. Earp and Annie I. Antón and Annie I. Antón and Lynda Aiman-Smith and Lynda Aiman-Smith and William H. Stufflebeam and William H. Stufflebeam},doi={10.1109/tem.2005.844927},pmid={null},pmcid={null},mag_id={2132024542},journal={IEEE Transactions on Engineering Management},abstract={Internet privacy policies describe an organization's practices on data collection, use, and disclosure. These privacy policies both protect the organization and signal integrity commitment to site visitors. Consumers use the stated website policies to guide browsing and transaction decisions. This paper compares the classes of privacy protection goals (which express desired protection of consumer privacy rights) and vulnerabilities (which potentially threaten consumer privacy) with consumer privacy values. For this study, we looked at privacy policies from nearly 50 websites and surveyed over 1000 Internet users. We examined Internet users' major expectations about website privacy and revealed a notable discrepancy between what privacy policies are currently stating and what users deem most significant. Our findings suggest several implications to privacy managers and software project managers. Results from this study can help managers determine the kinds of policies needed to both satisfy user values and ensure privacy-aware website development efforts.}}
@ARTICLE{Mu_2009,title={Extracting Software Functional Requirements from Free Text Documents},year={2009},author={Yunhe Mu and Yunhe Mu and Yinglin Wang and Yinglin Wang and Jianmei Guo and Jianmei Guo},doi={10.1109/icimt.2009.47},pmid={null},pmcid={null},mag_id={2132914604},journal={null},abstract={The acquisition of requirements assets are important in software product line (SPL) engineering for it help enhancing the effectiveness of reuse. Traditional methods are heavily based on manual effort. This appears to be a barrier for many organizations which tend to launch a SPL. In this paper, we propose an approach to extract functional requirements by analyzing text-based software requirements specifications (SRSs). We analyze the linguistic characterization of SRSs. According to it we define extended functional requirements framework (EFRF) which consists of 10 semantic cases, then we generate converting rules. We introduce an NLP (Natural Language Process) approach to build EFRFs from documents based on the concept of EFRF and the converting rules. The extracted EFRFs are suitable for expression and modeling of functional requirements variability. We apply our method to an auto-marker software product line. The result shows the approach has high accuracy and efficiency, and the approach is readily scalable and extensible.}}
@ARTICLE{Gruber_1995,title={Toward principles for the design of ontologies used for knowledge sharing},year={1995},author={Tobias Gruber and Thomas R. Gruber},doi={10.1006/ijhc.1995.1081},pmid={null},pmcid={null},mag_id={2133109597},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.}}
@ARTICLE{Scheuer_2010,title={Computer-Supported Argumentation: A Review of the State of the Art},year={2010},author={Oliver Scheuer and Oliver Scheuer and Frank Loll and Frank Loll and Niels Pinkwart and Niels Pinkwart and Bruce M. McLaren and Bruce M. McLaren},doi={10.1007/s11412-009-9080-x},pmid={null},pmcid={null},mag_id={2133391912},journal={null},abstract={Argumentation is an important skill to learn. It is valuable not only in many professional contexts, such as the law, science, politics, and business, but also in everyday life. However, not many people are good arguers. In response to this, researchers and practitioners over the past 15–20 years have developed software tools both to support and teach argumentation. Some of these tools are used in individual fashion, to present students with the “rules” of argumentation in a particular domain and give them an opportunity to practice, while other tools are used in collaborative fashion, to facilitate communication and argumentation between multiple, and perhaps distant, participants. In this paper, we review the extensive literature on argumentation systems, both individual and collaborative, and both supportive and educational, with an eye toward particular aspects of the past work. More specifically, we review the types of argument representations that have been used, the various types of interaction design and ontologies that have been employed, and the system architecture issues that have been addressed. In addition, we discuss intelligent and automated features that have been imbued in past systems, such as automatically analyzing the quality of arguments and providing intelligent feedback to support and/or tutor argumentation. We also discuss a variety of empirical studies that have been done with argumentation systems, including, among other aspects, studies that have evaluated the effect of argument diagrams (e.g., textual versus graphical), different representations, and adaptive feedback on learning argumentation. Finally, we conclude by summarizing the “lessons learned” from this large and impressive body of work, particularly focusing on lessons for the CSCL research community and its ongoing efforts to develop computer-mediated collaborative argumentation systems.}}
@ARTICLE{Mohler_2011,title={Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments},year={2011},author={Michael Mohler and Michael Mohler and Răzvan Bunescu and Razvan Bunescu and Rada Mihalcea and Rada Mihalcea},doi={null},pmid={null},pmcid={null},mag_id={2133436118},journal={null},abstract={In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.}}
@ARTICLE{Ying_2004,title={Predicting source code changes by mining change history},year={2004},author={Annie T. T. Ying and Annie T. T. Ying and Gail C. Murphy and Gail C. Murphy and Raymond T. Ng and Raymond T. Ng and Mark C. Chu-Carroll and Mark C. Chu-Carroll},doi={10.1109/tse.2004.52},pmid={null},pmcid={null},mag_id={2133961160},journal={IEEE Transactions on Software Engineering},abstract={Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.}}
@ARTICLE{Damian_2003,title={RE challenges in multi-site software development organisations},year={2003},author={Daniela Damian and Daniela Damian and Didar Zowghi and Didar Zowghi},doi={10.1007/s00766-003-0173-1},pmid={null},pmcid={null},mag_id={2135399200},journal={Requirements Engineering},abstract={The increasing globalisation of the software industry demands an investigation of requirements engineering (RE) in multi-site software development organisations. Requirements engineering is a task difficult enough when done locally--but it is even more difficult when cross-functional stakeholder groups specify requirements across cultural, language and time zone boundaries. This paper reports on a field study that investigated RE challenges introduced by the stakeholders' geographical distribution in a multi-site organisation. The goal was to examine RE practices in global software development, and to formulate recommendations for improvement as well as to provide directions for future research on methods and tools. Based on the empirical evidence, we have constructed a model of how remote communication and knowledge management, cultural diversity and time differences negatively impact requirements gathering, negotiations and specifications. Findings reveal that aspects such as a lack of a common understanding of requirements, together with a reduced awareness of a working local context, a trust level and an ability to share work artefacts significantly challenge the effective collaboration of remote stakeholders in negotiating a set of requirements that satisfies geographically distributed customers. The paper concludes with recommendations for improving RE practices in this setting.}}
@ARTICLE{Bobkowska_2010,title={On efficient collaboration between lawyers and software engineers when transforming legal regulations to law-related requirements},year={2010},author={Anna Bobkowska and Anna Bobkowska and Magdalena Kowalska and Magdalena Kowalska},doi={null},pmid={null},pmcid={null},mag_id={2135635650},journal={null},abstract={In order to develop information systems which comply with law, cooperation between lawyers and software engineers is necessary. The problem is how to transform legal regulations to law-related systems requirements efficiently. In this paper, we present both lawyer's perspective and software engineer's perspective on this problem and then we attempt to capture a common information space. The lawyer's perspective delivers a method for identifying and analyzing relevant laws and legal regulations. The software engineer's perspective discovers the specifics of dealing with law-related requirements with the use of requirements engineering as a frame of reference. The common information space includes a process of transforming legal regulations to legal requirements and a description of knowledge which must be shared in order to facilitate efficient collaboration.}}
@ARTICLE{Hevner_2004,title={Design science in information systems research},year={2004},author={Alan R. Hevner and Alan R. Hevner and Salvatore T. March and Salvatore T. March and Jin-Soo Park and Jinsoo Park and Jinsoo Park and Sudha Ram and Sudha Ram},doi={10.2307/25148625},pmid={null},pmcid={null},mag_id={2136451344},journal={Management Information Systems Quarterly},abstract={Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.}}
@ARTICLE{Surdeanu_2011,title={Risk analysis for intellectual property litigation},year={2011},author={Mihai Surdeanu and Mihai Surdeanu and Ramesh Nallapati and Ramesh Nallapati and G.D. Gregory and George D. Gregory and Joshua H. Walker and Joshua H. Walker and Joshua H. Walker and Joshua H. Walker and Christopher D. Manning and Christopher D. Manning},doi={10.1145/2018358.2018375},pmid={null},pmcid={null},mag_id={2137216461},journal={null},abstract={We introduce the problem of risk analysis for Intellectual Property (IP) lawsuits. More specifically, we focus on estimating the risk for participating parties using solely prior factors, i. e., historical and concurrent behavior of the entities involved in the case. This work represents a first step towards building a comprehensive legal risk assessment system for parties involved in litigation. This technology will allow parties to optimize their case parameters to minimize their own risk, or to settle disputes out of court and thereby ease the burden on the judicial system. In addition, it will also help U.S. courts detect and fix any inherent biases in the system.   We model risk estimation as a relational classification problem using conditional random fields [6] to jointly estimate the risks of concurrent cases. We evaluate our model on data collected by the Stanford Intellectual Property Litigation Clearinghouse, which consists of over 4,200 IP lawsuits filed across 88 U.S. federal districts and ranging over 8 years, probably the largest legal data set reported in data mining research. Despite being agnostic to the merits of the case, our best model achieves a classification accuracy of 64%, 22% (relative) higher than the majority-class baseline.}}
@ARTICLE{Laurent_2007,title={Towards Automated Requirements Triage},year={2007},author={Paula Laurent and Paula Laurent and Jane Cleland‐Huang and Jane Cleland-Huang and Chuan Duan and Chuan Duan},doi={10.1109/re.2007.63},pmid={null},pmcid={null},mag_id={2137653412},journal={null},abstract={Budgetary restrictions and time-to-market deadlines often require stakeholders to prioritize requirements and decide which ones to include in a given product release. Lack of an effective prioritization and triage process can lead to problems such as missed deadlines, disorganized development efforts, and late discovery of architecturally significant requirements. Existing prioritization techniques do not provide sufficient automation for large projects with hundreds of stakeholders and thousands of potentially conflicting requests and requirements. This paper therefore proposes an approach for automating a significant part of the prioritization process. The proposed method utilizes a probabilistic traceability model combined with a standard hierarchical clustering algorithm to cluster incoming stakeholder requests into hierarchical feature sets. Additional cross-cutting clusters are then generated to represent factors such as architecturally significant requirements or impacted business goals. Prioritization decisions are initially made at the feature level and then more critical requirements are promoted according to their relationships with the identified cross-cutting concerns. The approach is illustrated and evaluated through a case study applied to the requirements of the ice breaker system.}}
@ARTICLE{Lucia_2007,title={Recovering traceability links in software artifact management systems using information retrieval methods},year={2007},author={Andrea De Lucia and Andrea De Lucia and Fausto Fasano and Fausto Fasano and Rocco Oliveto and Rocco Oliveto and Genoveffa Tortora and Genoveffa Tortora},doi={10.1145/1276933.1276934},pmid={null},pmcid={null},mag_id={2138378644},journal={ACM Transactions on Software Engineering and Methodology},abstract={The main drawback of existing software artifact management systems is the lack of automatic or semi-automatic traceability link generation and maintenance. We have improved an artifact management system with a traceability recovery tool based on Latent Semantic Indexing (LSI), an information retrieval technique. We have assessed LSI to identify strengths and limitations of using information retrieval techniques for traceability recovery and devised the need for an incremental approach. The method and the tool have been evaluated during the development of seventeen software projects involving about 150 students. We observed that although tools based on information retrieval provide a useful support for the identification of traceability links during software development, they are still far to support a complete semi-automatic recovery of all links. The results of our experience have also shown that such tools can help to identify quality problems in the textual description of traced artifacts.}}
@ARTICLE{Dori_2010,title={Model-based meta-standardization},year={2010},author={Dov Dori and Dov Dori and Richard A. Martin and Richard Martin and Richard Martin and Richard Martin and Alex Blekhman and Alex Blekhman},doi={10.1109/systems.2010.5482321},pmid={null},pmcid={null},mag_id={2138924730},journal={null},abstract={To cope with current inconsistencies and incompleteness of technical documents, we propose a combined, model-based structured graphical and textual meta-standard approach for specification, verification and validation of complex systems in general and ISO enterprise standards in particular. This methodology, developed under the auspices of the ISO TC 184/SC 5 OPM Study Group, is presented along with MBASE—Model-Based Authoring of Specifications Environment, which is designed to support authors of technical specifications while creating and editing model-based technical documents.}}
@ARTICLE{Lieberherr_1993,title={Object-oriented software evolution},year={1993},author={Karl Lieberherr and Karl Lieberherr and Cun Xiao and C. Xiao},doi={10.1109/32.223802},pmid={null},pmcid={null},mag_id={2139219581},journal={IEEE Transactions on Software Engineering},abstract={The authors review propagation patterns for describing object-oriented software at a higher level of abstraction than one used by today's programming languages. A propagation pattern defines a family of programs from which one can select a member by giving a class dictionary graph that details the structure of behavior through part-of and inheritance relationships between classes. Three concepts are introduced: evolution histories, growth-plans and a propagation-directive calculus. Evolution histories describe a sequence of development phases of an object-oriented program, each phase being executable and therefore testable. To keep the programs flexible and short, they are described in terms of propagation patterns. Each phase of an evolution history is tested in small steps that are constrained by class dictionary graphs belonging to a growth-plan. Propagation directives are useful for describing both propagation patterns and growth-plans and are therefore endowed with sufficient expressiveness by being given a formal calculus applicable to object-oriented programming in general. A propagation directive is a succinct description of a family of submodels for a given family of data models. >}}
@ARTICLE{Ge_2008,title={Concept Similarity Matching Based on Semantic Distance},year={2008},author={Jike Ge and Jike Ge and Yuhui Qiu and Yuhui Qiu},doi={10.1109/skg.2008.24},pmid={null},pmcid={null},mag_id={2139549667},journal={null},abstract={With the application of semantic Web service and semantic Grid service, the similarity measure between services are more and more important in the processing of service matching. By formally defining the similarity of semantic services, useful information can be obtained about their similarity and compatibility. In this paper, we propose a concept similarity matching method based on semantic distance in service matching. Uses OWL-S to descript service, the algorithm computes semantic similarity of service in four macro steps. At last, we provide an experimental comparison of our method against traditional similarity measures, and prove empirically the benefits of our approach.}}
@ARTICLE{Atkinson_2005,title={A Dialogue Game Protocol for Multi-Agent Argument over Proposals for Action},year={2005},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Peter McBurney and Peter McBurney},doi={10.1007/s10458-005-1166-x},pmid={null},pmcid={null},mag_id={2140236793},journal={Autonomous Agents and Multi-Agent Systems},abstract={We present the syntax and semantics for a multi-agent dialogue game protocol which permits argument over proposals for action. The protocol, called the Persuasive Argument for Multiple Agents (PARMA) Protocol, embodies an earlier theory by the authors of persuasion over action which enables participants to rationally propose, attack, and defend, an action or course of actions (or inaction). We present an outline of both an axiomatic and a denotational semantics, and discuss implementation of the protocol, in the context of both human and artificial agents.}}
@ARTICLE{Sartor_1994,title={A Formal Model of Legal Argumentation},year={1994},author={Giovanni Sartor and Giovanni Sartor},doi={10.1111/j.1467-9337.1994.tb00175.x},pmid={null},pmcid={null},mag_id={2140256766},journal={Ratio Juris},abstract={. The paper gives a formal reconstruction of some fundamental patterns of legal reasoning, intended to reconcile symbolic logic and argumentation theory. Legal norms are represented as unidirectional inference rules which can be combined into arguments. The value of each argument (its qualification as justified, defensible, or defeated) is determined by the importance of the rules it contains. Applicability arguments, intended to contest or support the applicability of norms, preference arguments, purporting to establish preference relations among norms, and interpretative arguments are also formalised. All those argument types are connected in a unitary model, which relates legal reasoning to the indeterminacy of legal systems, intended as the possibility to develop incompatible defensible arguments. The model is applied to permissive norms and normative hierarchies, and is implemented in a Prolog program.}}
@ARTICLE{Valerdi_2009,title={Empirical research in systems engineering: challenges and opportunities of a new frontier},year={2009},author={Ricardo Valerdi and Heidi L. Davidz},doi={10.1002/sys.v12:2},pmid={null},pmcid={null},mag_id={2140480183},journal={Systems Engineering},abstract={This paper aims to advance the pedagogy of systems engineering by identifying opportunities and challenges in empirical research in the field. After an introduction to how empirical research could be further utilized in systems engineering, this paper discusses challenges faced when conducting empirical research in the field, threats to validity associated with systems engineering data collection, and considerations for empirical mixed-methods research. Two recently completed systems engineering empirical studies are used to illustrate specific examples. Finally, suggestions are given on how a professional society might provide additional support for researchers completing empirical research in systems engineering. The overarching goal of this paper is to describe how the increased use of empirical methods can be used to enrich the quality of research results which will in turn enhance the position of systems engineering as a widely recognized academic field. It is proposed that utilizing well-grounded, valid theory will improve understanding of systems engineering phenomena and advance the maturity of the field. © 2008 Wiley Periodicals, Inc. Syst Eng}}
@ARTICLE{Massey_2008,title={Aligning Requirements with HIPAA in the iTrust System},year={2008},author={Aaron K. Massey and Aaron K. Massey and Paul N. Otto and Paul N. Otto and Annie I. Antón and Annie I. Antón},doi={10.1109/re.2008.53},pmid={null},pmcid={null},mag_id={2141255867},journal={null},abstract={We describe a case study in which we evaluated an open-source electronic health record (EHR) systempsilas requirements for compliance with the U.S. Health Insurance Portability and Accountability Act (HIPAA). Our findings suggest that legal compliance must be requirements-driven, while establishing due diligence under the law must be test-driven.}}
@ARTICLE{Brachman_2004,title={Knowledge Representation and Reasoning},year={2004},author={Ronald J. Brachman and Ronald J. Brachman and Hector J. Levesque and Hector J. Levesque},doi={null},pmid={null},pmcid={null},mag_id={2141312052},journal={null},abstract={Knowledge representation is at the very core of a radical idea for understanding intelligence. Instead of trying to understand or build brains from the bottom up, its goal is to understand and build intelligent behavior from the top down, putting the focus on what an agent needs to know in order to behave intelligently, how this knowledge can be represented symbolically, and how automated reasoning procedures can make this knowledge available as needed. 

This landmark text takes the central concepts of knowledge representation developed over the last 50 years and illustrates them in a lucid and compelling way. Each of the various styles of representation is presented in a simple and intuitive form, and the basics of reasoning with that representation are explained in detail. This approach gives readers a solid foundation for understanding the more advanced work found in the research literature. The presentation is clear enough to be accessible to a broad audience, including researchers and practitioners in database management, information retrieval, and object-oriented systems as well as artificial intelligence. This book provides the foundation in knowledge representation and reasoning that every AI practitioner needs.

*Authors are well-recognized experts in the field who have applied the techniques to real-world problems 
* Presents the core ideas of KR&R in a simple straight forward approach, independent of the quirks of research systems 
*Offers the first true synthesis of the field in over a decade

Table of Contents

1 Introduction * 2 The Language of First-Order Logic *3 Expressing Knowledge * 4 Resolution * 5 Horn Logic * 6 Procedural Control of Reasoning * 7 Rules in Production Systems * 8 Object-Oriented Representation * 9 Structured Descriptions * 10 Inheritance * 11 Numerical Uncertainty *12 Defaults *13 Abductive Reasoning *14 Actions * 15 Planning *16 A Knowledge Representation Tradeoff * Bibliography * Index}}
@ARTICLE{Sesen_2010,title={An ontological framework for automated regulatory compliance in pharmaceutical manufacturing},year={2010},author={M. Berkan Sesen and M. Berkan Sesen and Pradeep Suresh and Pradeep Suresh and René Bañares‐Alcántara and René Bañares-Alcántara and René Bañares-Alcántara and Venkat Venkatasubramanian and Venkat Venkatasubramanian},doi={10.1016/j.compchemeng.2009.09.004},pmid={null},pmcid={null},mag_id={2141416082},journal={Computers & Chemical Engineering},abstract={null}}
@ARTICLE{Zimmermann_2005,title={Mining version histories to guide software changes},year={2005},author={Thomas Zimmermann and Thomas Zimmermann and Andreas Zeller and Andreas Zeller and Peter Weißgerber and P. Weissgerber and Stephan Diehl and Stephan Diehl},doi={10.1109/tse.2005.72},pmid={null},pmcid={null},mag_id={2141558501},journal={IEEE Transactions on Software Engineering},abstract={We apply data mining to version histories in order to guide programmers along related changes: "Programmers who changed these functions also changed...." Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.}}
@ARTICLE{Pedersen_2004,title={WordNet::Similarity: measuring the relatedness of concepts},year={2004},author={Ted Pedersen and Ted Pedersen and Siddharth V. Patwardhan and Siddharth Patwardhan and Jason Michelizzi and Jason Michelizzi},doi={10.3115/1614025.1614037},pmid={null},pmcid={null},mag_id={2142120379},journal={null},abstract={WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.}}
@ARTICLE{Loucopoulos_2013,title={A Systematic Classification and Analysis of NFRs},year={2013},author={Pericles Loucopoulos and Pericles Loucopoulos and Jianhua Sun and Jie Sun and Liping Zhao and Liping Zhao and Farideh Heidari and Farideh Heidari},doi={null},pmid={null},pmcid={null},mag_id={2142191692},journal={null},abstract={The main agenda of Requirements Engineering (RE) is the development of tools, techniques and languages for the elicitation, specification, negotiation, and validation of software requirements. However, this development has traditionally been focused on functional requirements (FRs), rather than non-functional requirements (NFRs). Consequently, NFR approaches developed over the years have been fragmental and there is a lack of clear understanding of the positions of these approaches in the RE process. This paper provides a systematic classification and analysis of 89 NFR approaches.}}
@ARTICLE{Easterbrook_1995,title={Managing inconsistencies in an evolving specification},year={1995},author={Steve Easterbrook and Steve Easterbrook and Bashar Nuseibeh and Bashar Nuseibeh},doi={10.1109/isre.1995.512545},pmid={null},pmcid={null},mag_id={2142255038},journal={Requirements Engineering},abstract={In an evolving specification, considerable effort is spent handling recurrent inconsistencies. Detecting and resolving inconsistencies is only part of the problem: a resolved inconsistency might not stay resolved. Frameworks in which inconsistency is tolerated help by allowing resolution to be delayed. However, evolution of a specification may affect both resolved and unresolved inconsistencies. We address these problems by explicitly recording relationships between partial specifications (ViewPoints), representing both resolved and unresolved inconsistencies. We assume that ViewPoints will often be inconsistent with one another, and we ensure that a complete work record is kept, detailing any inconsistencies that have been detected, and what actions, if any, have been taken to resolve them. The work record is then used to reason about the effects of subsequent changes to ViewPoints, without constraining the development process.}}
@ARTICLE{Barry_2007,title={How software process automation affects software evolution: a longitudinal empirical analysis},year={2007},author={Evelyn Barry and Evelyn J. Barry and Chris F. Kemerer and Chris F. Kemerer and Sandra A. Slaughter and Sandra A. Slaughter},doi={10.1002/smr.342},pmid={null},pmcid={null},mag_id={2142698935},journal={Journal of Software Maintenance and Evolution: Research and Practice},abstract={SUMMARY This research analyzes longitudinal empirical data on commercial software applications to test and better understand how software evolves over time, and to measure the likely long-term effects of a software process automation tool on software productivity and quality. The research consists of two parts. First, weuse data from source control systems, defect tracking systems, andarchived project documentation to test a series of hypotheses developed by Belady and Lehman about software evolution. We find empirical support for many of these hypotheses, but not all. We then further analyze the data using moderated regression analysis to discern how software process automation efforts at the research site influenced the software evolution lifecycles of the applications. Our results support the claim that automation has enabled the organization to accomplish more work activities with greater productivity, thereby significantly increasing the functionality of the applications portfolio. Despite the growth in software functionality, the analysis suggests that automation has helped to manage software complexity levels and to improve quality by reducing errors over time. Our models and their results demonstrate how longitudinal empirical software data can be used to reveal the often elusive long-term benefits of investments in software process improvement, and to help managers make more informed resource-allocation decisions. Copyright c � 2007 John Wiley & Sons, Ltd.}}
@ARTICLE{Cleland‐Huang_2003,title={Event-based traceability for managing evolutionary change},year={2003},author={Jane Cleland‐Huang and Jane Cleland-Huang and Carl K. Chang and Carl K. Chang and Mark Christensen and M. Christensen},doi={10.1109/tse.2003.1232285},pmid={null},pmcid={null},mag_id={2142809450},journal={IEEE Transactions on Software Engineering},abstract={Although the benefits of requirements traceability are widely recognized, the actual practice of maintaining a traceability scheme is not always entirely successful. The traceability infrastructure underlying a software system tends to erode over its lifetime, as time-pressured practitioners fail to consistently maintain links and update impacted artifacts each time a change occurs, even with the support of automated systems. This paper proposes a new method of traceability based upon event-notification and is applicable even in a heterogeneous and globally distributed development environment. Traceable artifacts are no longer tightly coupled but are linked through an event service, which creates an environment in which change is handled more efficiently, and artifacts and their related links are maintained in a restorable state. The method also supports enhanced project management for the process of updating and maintaining the system artifacts.}}
@ARTICLE{Antón_1998,title={A Representational Framework for Scenarios of System Use},year={1998},author={Annie I. Antón and Annie I. Antón and Colin Potts and Colin Potts},doi={10.1007/s007660050006},pmid={null},pmcid={null},mag_id={2142911102},journal={Requirements Engineering},abstract={null}}
@ARTICLE{Lamond_2005,title={DO PRECEDENTS CREATE RULES},year={2005},author={Grant Lamond and Grant Lamond},doi={10.1017/s1352325205050019},pmid={null},pmcid={null},mag_id={2142954222},journal={Legal Theory},abstract={The doctrine of precedent is one of the most distinctive features of the modern common law. Understanding the operation of precedent is important for our theorizing about the nature of law, since any adequate theory must be compatible with the practice. In this paper I will explore the conventional view of precedent endorsed by practitioners and many legal philosophers alike. I will argue that for all its attractions, it provides a distorted view of the nature of precedent. The distortion grows out of the basic assumption that precedents create rules, and thus that the common law can be understood as a form of rule-based decision-making. Instead, the common law is a form of case-by-case decision-making, and the doctrine of precedent constrains this decision-making by requiring later courts to treat earlier cases as correctly decided. The relevance of earlier cases is not well understood in terms of rules—they are better understood as a special type of reason.}}
@ARTICLE{Belady_1976,title={A model of large program development},year={1976},author={Laszlo A. Belady and Laszlo A. Belady and M. M. Lehman and Meir M. Lehman},doi={10.1147/sj.153.0225},pmid={null},pmcid={null},mag_id={2143232930},journal={Ibm Systems Journal},abstract={Discussed are observations made on the development of OS/360 and its subsequent enhancements and releases. Some modeling approaches to organizing these observations are also presented.}}
@ARTICLE{Cartwright_2009,title={Using Computational Argumentation to Support E-participation},year={2009},author={D. Cartwright and Dan Cartwright and Katie Atkinson and Katie Atkinson},doi={10.1109/mis.2009.104},pmid={null},pmcid={null},mag_id={2143260818},journal={IEEE Intelligent Systems},abstract={Internet-based tools that encourage public participation in debates concerning policy issues have been recognized as a good way to engage the electorate with political issues. In addition, such systems for e-participation can gather, make available, and analyze the public's contributions to political debate. In this article we discuss a system called Parmenides, which we designed to exploit technological developments to bring democratic processes into the online world. Parmenides is primarily a forum by which government bodies can present policy proposals to the public so that users can submit their opinions on the justification presented for the particular policy. Within Parmenides, the justification for action is structured to exploit a specific representation of persuasive argument based on the use of argumentation schemes and critical questions.}}
@ARTICLE{Elgammal_2012,title={Using patterns for the analysis and resolution of compliance violations},year={2012},author={Amal Elgammal and Amal Elgammal and Oktay Türetken and Oktay Turetken and Willem‐Jan van den Heuvel and Willem-Jan van den Heuvel},doi={10.1142/s0218843012400023},pmid={null},pmcid={null},mag_id={2143396069},journal={null},abstract={Today's enterprises demand a high degree of compliance of business processes to meet laws and regulations, such as Sarbanes-Oxley and Basel II. Compliance should be enforced during all phases of business process lifecycle, from the phases of analysis and design to deployment, monitoring and evaluation. In this paper, a taxonomy of compliance constraints for business processes is introduced based on the notion of compliance patterns. Patterns facilitate the formal specification of compliance constraints that enable their verification and analysis against business process models. This taxonomy serves as the backbone of the root-cause analysis, which is conducted to reason about and eventually to resolve design-time compliance violations, by providing appropriate guidelines as remedies to alleviate design-time compliance deviations. We have developed and integrated a set of tools to observe and evaluate the applicability of our approach, and experiment with it in case studies.}}
@ARTICLE{Bex_2007,title={Sense-making software for crime investigation: how to combine stories and arguments?},year={2007},author={Floris Bex and Floris Bex and S.W. van den Braak and S.W. van den Braak and Herre van Oostendorp and H. van Oostendorp and Henry Prakken and Henry Prakken and Hendrik Prakken and Bart Verheij and Bart Verheij and Gerard A. W. Vreeswijk and Gerard A. W. Vreeswijk},doi={10.1093/lpr/mgm007},pmid={null},pmcid={null},mag_id={2143450754},journal={Law, Probability and Risk},abstract={Sense-making software for crime investigation should be based on a model of reasoning about evidence that is both natural and rationally well-founded. A formal model is proposed that combines artificial intelligence formalisms for abductive inference to the best explanation and for defeasible argumentation. Stories about what might have happened in a case are represented as causal networks and possible hypotheses can be inferred by abductive reasoning. Links between stories and the available evidence are expressed with evidential generalizations that express how observations can be inferred from evidential sources with defeasible argumentation. It is argued that this approach unifies two well-known accounts of reasoning about evidence, namely, anchored narratives theory and new evidence theory. After the reasoning model is defined, a design is presented for sense-making software that allows crime investigators to visualize their thinking about a case in terms of the reasoning model.}}
@ARTICLE{Sovacool_2004,title={From sectoral systems of innovation to socio-technical systems: Insights about dynamics and change from sociology and institutional theory},year={2004},author={Benjamin K. Sovacool and Frank W. Geels and Frank W. Geels},doi={10.1016/j.respol.2004.01.015},pmid={null},pmcid={null},mag_id={2143834698},journal={Research Policy},abstract={In the last decade ‘sectoral systems of innovation’ have emerged as a new approach in innovation studies. This article makes four contributions to the approach by addressing some open issues. The first contribution is to explicitly incorporate the user side in the analysis. Hence, the unit of analysis is widened from sectoral systems of innovation to socio-technical systems. The second contribution is to suggest an analytical distinction between systems, actors involved in them, and the institutions which guide actor’s perceptions and activities. Thirdly, the article opens up the black box of institutions, making them an integral part of the analysis. Institutions should not just be used to explain inertia and stability. They can also be used to conceptualise the dynamic interplay between actors and structures. The fourth contribution is to address issues of change from one system to another. The article provides a coherent conceptual multi-level perspective, using insights from sociology, institutional theory and innovation studies. The perspective is particularly useful to analyse long-term dynamics, shifts from one socio-technical system to another and the co-evolution of technology and society.}}
@ARTICLE{Palau_2009,title={Argumentation mining: the detection, classification and structure of arguments in text},year={2009},author={Raquel Mochales Palau and Raquel Mochales Palau and Marie‐Francine Moens and Marie-Francine Moens},doi={10.1145/1568234.1568246},pmid={null},pmcid={null},mag_id={2144232471},journal={null},abstract={Argumentation is the process by which arguments are constructed and handled. Argumentation constitutes a major component of human intelligence. The ability to engage in argumentation is essential for humans to understand new problems, to perform scientific reasoning, to express, to clarify and to defend their opinions in their daily lives. Argumentation mining aims to detect the arguments presented in a text document, the relations between them and the internal structure of each individual argument. In this paper we analyse the main research questions when dealing with argumentation mining and the different methods we have studied and developed in order to successfully confront the challenges of argumentation mining in legal texts.}}
@ARTICLE{Kiyavitskaya_2008,title={Why Eliciting and Managing Legal Requirements Is Hard},year={2008},author={Nadzeya Kiyavitskaya and Nadzeya Kiyavitskaya and Alžběta Krausová and Alžběta Krausová and Nicola Zannone and Nicola Zannone},doi={10.1109/relaw.2008.10},pmid={null},pmcid={null},mag_id={2144369660},journal={null},abstract={The increasing complexity of IT systems and the growing demand for regulation compliance are main issues for the design of IT systems. Addressing these issues requires the developing of effective methods to support the analysis of regulations and the elicitation of any organizational and system requirements from them. This work investigates the problem of designing regulation-compliant systems and, in particular, the challenges in eliciting and managing legal requirements.}}
@ARTICLE{Smartt_2011,title={Advancing systems engineering in support of the bid and proposal process},year={2011},author={Clement Smartt and Clement Smartt and Susan Ferreira and Susan Ferreira},doi={10.1002/sys.20177},pmid={null},pmcid={null},mag_id={2144789018},journal={Systems Engineering},abstract={Qualifying strategic opportunities to pursue and capitalizing on those opportunities is key to the very survival of an organization. Successful organizations that thrive on contract work capture contracts and earn a profit executing those contracts. Systems engineers are generally the “cradle to grave” technical points of contact for projects and are charged with guiding the definition of the technical solutions in the early lifecycle stages. This paper examines existing research that may guide systems engineering in support of bid and proposal pursuits. Systems engineering, software engineering, bid management, economics, finance, game theory, and cognitive psychology literature are all mined for areas related to bidding. As a result, a number of research opportunities are identified. © 2010 Wiley Periodicals, Inc. Syst Eng 14}}
@ARTICLE{Best_2007,title={Convergence and harmonization of standards organizations},year={2007},author={Karl F. Best and K.F. Best},doi={10.1109/siit.2007.4629321},pmid={null},pmcid={null},mag_id={2145457847},journal={null},abstract={Accredited and non-accredited standards organizations, commonly referred to as ldquoSDOsrdquo and ldquoSSOsrdquo respectively, are more alike than usually thought. Both rely on volunteers to provide their time and effort and to apply their domain expertise to create industry, national, and international standards. Both face similar challenges with regards to IPR issues, governance, business models, membership recruitment and retention, and promotion of work products. Accreditation by itself is not a valid distinguishing feature; many unaccredited organizations are fully qualified to become accredited and would do so if only they saw the need or benefit. The perceived differences between these types of organizations, while loosely based on historical practices, depend mostly upon whom one asks. In addition, the two types of organizations are becoming more like each other as their practices become more closely aligned and as they reach out to work with each other, recognizing the value of each otherspsila efforts.}}
@ARTICLE{Antón_2001,title={Deriving Goals from a Use-Case Based Requirements Specification},year={2001},author={Annie I. Antón and Annie I. Antón and Ryan A. Carter and Ryan A. Carter and Aldo Dagnino and Aldo Dagnino and John Dempster and John H. Dempster and Devon F. Siege and Devon F. Siege},doi={10.1007/pl00010356},pmid={null},pmcid={null},mag_id={2145605190},journal={Requirements Engineering},abstract={Use cases and scenarios have emerged as prominent analysis tools during requirements engineering activities due to both their richness and informality. In some instances, for example when a project’s budget or schedule time is reduced at short notice, practitioners have been known to adopt a collection of use cases as a suitable substitute for a requirements specification. Given the challenges inherent in managing large collections of scenarios, this shortcut is cause for concern and deserves focused attention. We describe our experiences during a goal-driven requirements analysis effort for an electronic commerce application. In particular, we identify the specific risks incurred, focusing more on the challenges imposed due to traceability, inconsistent use of terminology, incompleteness and consistency, rather than on traditional software project management risks. We conclude by discussing the impact of the lessons learned for requirements engineering in the context of building quality systems during goal and scenario analysis}}
@ARTICLE{Ghanavati_2008,title={Comparative Analysis between Document-based and Model-based Compliance Management Approaches},year={2008},author={Sepideh Ghanavati and Sepideh Ghanavati and Daniel Amyot and Daniel Amyot and Liam Peyton and Liam Peyton},doi={10.1109/relaw.2008.2},pmid={null},pmcid={null},mag_id={2145717981},journal={null},abstract={Compliance of an organization's business processes with legislation is difficult to assess, and even more so as laws and processes evolve. Using seven criteria, we evaluate five different approaches for documenting and managing such compliance that involve models and/or legal documents, with or without tool support for traceability. It is argued that a mixed approach that combines models and original documents, with tool support, offers the best trade-off. We also discuss an example of such an approach based on the user requirements notation and a commercial requirements management system.}}
@ARTICLE{Karacapilidis_2001,title={Computer supported argumentation and collaborative decision making: the HERMES system},year={2001},author={Nikos Karacapilidis and Nikos Karacapilidis and Dimitris Papadias and Dimitris Papadias},doi={10.1016/s0306-4379(01)00020-5},pmid={null},pmcid={null},mag_id={2146340995},journal={Information Systems},abstract={Abstract   Collaborative decision making problems can be addressed through argumentative discourse and collaboration among the users involved. Consensus is achieved through the process of collaboratively considering alternative understandings of the problem, competing interests, priorities and constraints. The application of formal modeling and analysis tools to solve the related processes is impossible before the problem can be articulated in a concise and agreed upon manner. This paper describes H ermes , a system that augments classical decision making approaches by supporting argumentative discourse among decision makers. It is fully implemented in Java and runs on the Web, thus providing relatively inexpensive access to a broad public. Using an illustrative example, we present the argumentation elements, discourse acts and reasoning mechanisms involved in H ermes . We also describe the integration of advanced features to the system; these enable users to retrieve data stored in remote databases in order to further warrant their arguments, and stimulate them to perform acts that best reflect their interests and intentions.}}
@ARTICLE{Wyner_2009,title={Modelling Judicial Context in Argumentation Frameworks},year={2009},author={Adam Wyner and Adam Wyner and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1093/logcom/exp009},pmid={null},pmcid={null},mag_id={2146555293},journal={Journal of Logic and Computation},abstract={Much work using argumnentation frameworks treats arguments as entirely abstract, related by a uniform attack relation which always succeeds unless the attacker can itself be defeated. However, this does not seem adequate for legal argumentation. Some proposals have suggested regulating attack relations using preferences or values on arguments and which filter the attack relation, so that some attacks fail and so can be removed from the framework. This does not capture several important context related features of legal reasoning, such as how an audience can prefer or value an argument, yet be constrained by precedent or authority not to accept it. Nor does it explain how certain types of attack may not be allowed in a particular procedural context. For this reason, evaluation of the status of arguments within a given framework must be allowed to depend not only on the attack relations along with the preference or value of arguments, but also on the nature of the attacks and the context in which they are made. We present a means to represent these features, enabling us to account for a number of factors currently considered to be beyond the remit of formal argumentation frameworks. We give three examples of the use of approach: appealing a case, overruling a precedent, and rehearing of a case as a civil rather than criminal proceeding.}}
@ARTICLE{Fléchais_2010,title={Towards Tool-Support for Usable Secure Requirements Engineering with CAIRIS},year={2010},author={Ivan Fléchais and Ivan Flechais and Shamal Faily and Shamal Faily},doi={10.4018/jsse.2010070104},pmid={null},pmcid={null},mag_id={2147391500},journal={International Journal of Secure Software Engineering},abstract={Understanding how to better elicit, specify, and manage requirements for secure and usable software systems is a key challenge in security software engineering, however, there lacks tool-support for specifying and managing the voluminous amounts of data the associated analysis yields. Without these tools, the subjectivity of analysis may increase as design activities progress. This paper describes CAIRIS Computer Aided Integration of Requirements and Information Security, a step toward tool-support for usable secure requirements engineering. CAIRIS not only manages the elements associated with task, requirements, and risk analysis, it also supports subsequent analysis using novel approaches for analysing and visualising security and usability. The authors illustrate an application of CAIRIS by describing how it was used to support requirements analysis in a critical infrastructure case study.}}
@ARTICLE{Oviedo_2004,title={La definición del concepto de percepción en psicología con base en la teoría Gestalt},year={2004},author={Gilberto Leonardo Oviedo and Gilberto Leonardo Oviedo},doi={10.7440/res18.2004.08},pmid={null},pmcid={null},mag_id={2147533244},journal={Revista De Estudios Sociales},abstract={El presente articulo define el concepto de percepcion en psicologia, con base en los aportes de la teoria Gestalt (teoria de la forma). Se presentan los antecedentes filosoficos del concepto de percepcion en la obra de los pensadores asociacionistas y posteriormente se establece el debate introducido por los representantes del movimiento Gestalt. Se hace una revision de las principales leyes de la percepcion con especial enfasis en el principio de los buenos contornos y el holismo perceptual. Finalmente, la conclusion presenta la importancia que tiene para la psicologia como disciplina cientifica la presentacion de la percepcion como un proceso caracterizado por los procesos de abstraccion y busqueda de la simplicidad o pregnancia.}}
@ARTICLE{Petroski_1985,title={To Engineer is Human},year={1985},author={Henry Petroski and Henry Petroski and W. Brent Hall and W. Brent Hall},doi={null},pmid={null},pmcid={null},mag_id={2147750390},journal={null},abstract={null}}
@ARTICLE{Jackson_2003,title={Information extraction from case law and retrieval of prior cases},year={2003},author={Paul Jackson and Peter Jackson and Khalid Al-Kofahi and Khalid Al-Kofahi and Alex Tyrrell and Alex Tyrrell and Arun Vachher and Arun Vachher},doi={10.1016/s0004-3702(03)00106-1},pmid={null},pmcid={null},mag_id={2148167777},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Bohner_1996,title={Impact analysis in the software change process: a year 2000 perspective},year={1996},author={Bohner and Bohner},doi={10.1109/icsm.1996.564987},pmid={null},pmcid={null},mag_id={2148724520},journal={null},abstract={Software change impact analysis has gained considerable attention with recent challenges of the Year 2000 Date situation. As the software community recognizes the growing need to identify consequences of these changes, impact analysis is making its way into the software process. Dependencies between software life cycle objects are becoming more numerous and complex as many software systems grow beyond a million of lines of code. Software change efforts are plagued with widely varying estimates for implementing software changes, since the impacts of the change are not readily known in advance. The introduction of software change impact analysis into the software process adds more fidelity to software change visibility, enabling more accurate software change estimates. The paper examines where impact analysis is applied during software changes and describes how impact analysis can be addressed in the software process.}}
@ARTICLE{Rose_1989,title={Legal information retrieval a hybrid approach},year={1989},author={Daniel E. Rose and D. E. Rose and Richard K. Belew and Richard K. Belew},doi={10.1145/74014.74033},pmid={null},pmcid={null},mag_id={2149816565},journal={null},abstract={SCALIR is a legal information retrieval system which uses a combination of symbolic and connectionist artificial intelligence techniques. Traditional systems used for automating legal research have many difficulties which make them “brittle”; SCALIR is an attempt to rectify many of these problems. SCALIR's hybrid nature is especially appropriate for the legal domain, which requires both logical and associative inferences. The system also benefits from a unique direct-manipulation style interface and the ability to improve its performance based on user feedback.}}
@ARTICLE{Lloyd_1982,title={Least squares quantization in PCM},year={1982},author={Sheelagh Lloyd and S. P. Lloyd and S. P. Lloyd and Seth Lloyd},doi={10.1109/tit.1982.1056489},pmid={null},pmcid={null},mag_id={2150593711},journal={IEEE Transactions on Information Theory},abstract={It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for 2^{b} quanta, b=1,2, \cdots, 7 , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.}}
@ARTICLE{Lamsweerde_2001,title={Goal-oriented requirements engineering: a guided tour},year={2001},author={Axel van Lamsweerde and A. van Lamsweerde},doi={10.1109/isre.2001.948567},pmid={null},pmcid={null},mag_id={2151451947},journal={Requirements Engineering},abstract={Goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve. Goal-oriented requirements engineering is concerned with the use of goals for eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements. This area has received increasing attention. The paper reviews various research efforts undertaken along this line of research. The arguments in favor of goal orientation are first briefly discussed. The paper then compares the main approaches to goal modeling, goal specification and goal-based reasoning in the many activities of the requirements engineering process. To make the discussion more concrete, a real case study is used to suggest what a goal-oriented requirements engineering method may look like. Experience, with such approaches and tool support are briefly discussed as well.}}
@ARTICLE{Jørgensen_2007,title={A Systematic Review of Software Development Cost Estimation Studies},year={2007},author={Magne Jørgensen and Magne Jørgensen and Martin Shepperd and Martin Shepperd},doi={10.1109/tse.2007.3},pmid={null},pmcid={null},mag_id={2151511199},journal={IEEE Transactions on Software Engineering},abstract={This paper aims to provide a basis for the improvement of software-estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A Web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) increase the breadth of the search for relevant studies, 2) search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) conduct more studies on estimation methods commonly used by the software industry, and 4) increase the awareness of how properties of the data sets impact the results when evaluating estimation methods}}
@ARTICLE{McCarthy_2012,title={DSS: Text Similarity Using Lexical Alignments of Form, Distributional Semantics and Grammatical Relations},year={2012},author={Diana McCarthy and Diana McCarthy and Spandana Gella and Spandana Gella and Siva Reddy and Siva Reddy},doi={null},pmid={null},pmcid={null},mag_id={2151714692},journal={null},abstract={In this paper we present our systems for the STS task. Our systems are all based on a simple process of identifying the components that correspond between two sentences. Currently we use words (that is word forms), lemmas, distributional similar words and grammatical relations identified with a dependency parser. We submitted three systems. All systems only use open class words. Our first system (alignheuristic) tries to obtain a mapping between every open class token using all the above sources of information. Our second system (wordsim) uses a different algorithm and unlike alignheuristic, it does not use the dependency information. The third system (average) simply takes the average of the scores for each item from the other two systems to take advantage of the merits of both systems. For this reason we only provide a brief description of that. The results are promising, with Pearson's coefficients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets. We provide some analysis of the results and also provide results for our data using Spearman's, which as a nonparametric measure which we argue is better able to reflect the merits of the different systems (average is ranked between the others).}}
@ARTICLE{Jones_1996,title={Strategies for managing requirements creep},year={1996},author={Capers Jones and C. Jones and Capers Jones and C. Jones},doi={10.1109/2.507640},pmid={null},pmcid={null},mag_id={2151976504},journal={IEEE Computer},abstract={One of the most chronic problems in software development is the fact that application requirements are almost never stable and fixed. Frequent changes in requirements are not always caused by capricious clients (although sometimes they are). The root cause of requirements volatility is that many applications are attempting to automate domains that are only partly understood. As software design and development proceeds, the process of automation begins to expose these ill-defined situations. Therefore, although creeping requirements are troublesome, they are often a technical necessity. Several threads of research and some emerging technologies are aimed at either clarifying requirements earlier in development or minimizing the disruptive effect of changing requirements later.}}
@ARTICLE{Verheij_2003,title={Artificial argument assistants for defeasible argumentation},year={2003},author={Bart Verheij and Bart Verheij},doi={10.1016/s0004-3702(03)00107-3},pmid={null},pmcid={null},mag_id={2152212999},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Nilsson_1980,title={Principles of Artificial Intelligence},year={1980},author={Nils J. Nilsson and Nils J. Nilsson},doi={null},pmid={null},pmcid={null},mag_id={2152475379},journal={null},abstract={A classic introduction to artificial intelligence intended to bridge the gap between theory and practice, "Principles of Artificial Intelligence" describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval. Rather than focusing on the subject matter of the applications, the book is organized around general computational concepts involving the kinds of data structures used, the types of operations performed on the data structures, and the properties of the control strategies used.   "Principles of Artificial Intelligence"evolved from the author's courses and seminars at Stanford University and University of Massachusetts, Amherst, and is suitable for text use in a senior or graduate AI course, or for individual study.}}
@ARTICLE{Sindre_2000,title={Eliciting security requirements by misuse cases},year={2000},author={Guttorm Sindre and Guttorm Sindre and Andreas L. Opdahl and Andreas L. Opdahl},doi={10.1007/s00766-004-0194-4},pmid={null},pmcid={null},mag_id={2153177282},journal={null},abstract={Use case diagrams (L. Jacobson et al., 1992) have proven quite helpful in requirements engineering, both for eliciting requirements and getting a better overview of requirements already stated. However, not all kinds of requirements are equally well supported by use case diagrams. They are good for functional requirements, but poorer at e.g., security requirements, which often concentrate on what should not happen in the system. With the advent of e- and m-commerce applications, security requirements are growing in importance, also for quite simple applications where a short lead time is important. Thus, it would be interesting to look into the possibility for applying use cases on this arena. The paper suggests how this can be done, extending the diagrams with misuse cases. This new construct makes it possible to represent actions that the system should prevent, together with those actions which it should support.}}
@ARTICLE{Tashi_2009,title={Regulatory Compliance and Information Security Assurance},year={2009},author={Igli Tashi and Igli Tashi},doi={10.1109/ares.2009.29},pmid={null},pmcid={null},mag_id={2154427952},journal={null},abstract={According to all security studies recently published, the regulatory compliance appears as one of the most important drivers in security spending, representing the bigger part of security operating costs. Regulatory compliance is very often mentioned to attest an improved performance and accountability in security mechanism and procedures. This paper aims to analyse the contribution of different laws, rules, regulations standards, frameworks related to the IT Security. Compliance and conformity concepts in information security are discussed : • to enlighten the relationship between the regulatory compliance and the overall security level for a given organization • to reveal the importance of the regulatory compliance for the information security assurance.}}
@ARTICLE{Prakken_2005,title={Coherence and Flexibility in Dialogue Games for Argumentation},year={2005},author={Henry Prakken and Henry Prakken},doi={10.1093/logcom/exi046},pmid={null},pmcid={null},mag_id={2154849078},journal={Journal of Logic and Computation},abstract={This article carries out a formal study of dialogue games for argumentation. A formal framework for such games is proposed which imposes an explicit reply structure on dialogues, where each dialogue move either attacks or surrenders to some earlier move of the other participant. The framework is flexible in several respects. It allows for different underlying logics, alternative sets of locutions and more or less strict rules for when they are allowed. In particular, it allows for varying degrees of coherence and flexibility when it comes to maintaining the focus of a dialogue. Its formal nature supports the study of formal properties of specific dialogue protocols, especially on how they respect the underlying logic.}}
@ARTICLE{Sim_1998,title={The ramp-up problem in software projects: a case study of how software immigrants naturalize},year={1998},author={Susan Elliott Sim and Susan Elliott Sim and Richard C. Holt and Richard C. Holt},doi={10.1109/icse.1998.671389},pmid={null},pmcid={null},mag_id={2155223492},journal={null},abstract={Joining a software development team is like moving to a new country to start employment; the immigrant has a lot to learn about the job, the local customs, and sometimes a new language. In an exploratory case study, we interviewed four software immigrants, in order to characterize their naturalization process. Seven patterns in four major categories were found. In this paper, these patterns are substantiated, and their implications discussed. The lessons learned from this study can be applied equally to improving the naturalization process, and to the formulation of further research questions.}}
@ARTICLE{Reiter_1987,title={A logic for default reasoning},year={1987},author={Raymond Reiter and Raymond Reiter},doi={10.1016/0004-3702(80)90014-4},pmid={null},pmcid={null},mag_id={2155322595},journal={Artificial Intelligence},abstract={The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults.

In this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.}}
@ARTICLE{Simari_1992,title={A mathematical treatment of defeasible reasoning and its implementation},year={1992},author={Guillermo Ricardo Simari and Guillermo Ricardo Simari and Ronald P. Loui and Ronald P. Loui},doi={10.1016/0004-3702(92)90069-a},pmid={null},pmcid={null},mag_id={2156092566},journal={Artificial Intelligence},abstract={In this dissertation I present a formal approach to defeasible reasoning. This mathematical approach is based on the notion of specificity introduced by Poole and the general theory of warrant as presented by Pollock. General background information on the subject of Nonmonotonic Reasoning is presented and some of the shortcomings of existing systems are analyzed. We believe that the approach presented here represents a definite improvement over past systems.
The main contribution of this thesis is a formally precise, elegant, clean, well-defined system which exhibits a correct behavior when applied to the benchmark examples in the literature. Model-theoretic semantical issues have been addressed. The investigation on the theoretical issues has aided the study of how this kind of reasoner can be realized on a computer. An interpreter of a restricted language, an extension of Horn clauses with defeasible rules, has been implemented. Finally, the implementation details are discussed.}}
@ARTICLE{Holyoak_1989,title={Analogical mapping by constraint satisfaction},year={1989},author={Keith J. Holyoak and Keith J. Holyoak and Paul Thagard and Paul Thagard},doi={10.1207/s15516709cog1303_1},pmid={null},pmcid={null},mag_id={2156159857},journal={Cognitive Science},abstract={A theory of analogical mapping between source and target analogs based upon interacting structural, semantic, and pragmatic constraints is proposed here. The structural constraint of isomorphism encourages mappings that maximize the consistency of relational corresondences between the elements of the two analogs. The constraint of semantic similarity supports mapping hypotheses to the degree that mapped predicates have similar meanings. The constraint of pragmatic centrality favors mappings involving elements the analogist believes to be important in order to achieve the purpose for which the analogy is being used. The theory is implemented in a computer program called ACME (Analogical Constraint Mapping Engine), which represents constraints by means of a network of supporting and competing hypotheses regarding what elements to map. A cooperative algorithm for parallel constraint satisfaction identities mapping hypotheses that collectively represent the overall mapping that best fits the interacting constraints. ACME has been applied to a wide range of examples that include problem analogies, analogical arguments, explanatory analogies, story analogies, formal analogies, and metaphors. ACME is sensitive to semantic and pragmatic information if it is available, and yet able to compute mappings between formally isomorphic analogs without any similar or identical elements. The theory is able to account for empirical findings regarding the impact of consistency and similarity on human processing of analogies.}}
@ARTICLE{Hage_2000,title={Goal-based theory evaluation},year={2000},author={Jaap Hage and Jaap Hage},doi={null},pmid={null},pmcid={null},mag_id={2156345911},journal={null},abstract={This paper describes how part of Dworkin’s Model of Principles, a coherence theory of the law, can be made sufficiently precise to capture it in a logical theory. The result is formalised by means of an addition to Reason-based Logic.}}
@ARTICLE{Lee_2007,title={Towards a Requirements-Driven Workbench for Supporting Software Certification and Accreditation},year={2007},author={Seok-Won Lee and Seok-Won Lee and Seok-Won Lee and Robin A. Gandhi and Robin A. Gandhi and Siddharth Wagle and Siddharth Wagle},doi={10.1109/sess.2007.11},pmid={null},pmcid={null},mag_id={2156452334},journal={null},abstract={Security certification activities for software systems rely heavily on requirements mandated by regulatory documents and their compliance evidences to support accreditation decisions. Therefore, the design of a workbench to support these activities should be grounded in a thorough understanding of the characteristics of certification requirements and their relationships with certification activities. To this end, we utilize our findings from the case study of a certification process of The United States Department of Defense (DoD) to identify the design objectives of a requirements-driven workbench for supporting certification analysts. The primary contributions of this paper are: identifying key areas of automation and tool support for requirements-driven certification activities; an ontology-driven dynamic and flexible workbench architecture to address process variability; and a prototype implementation.}}
@ARTICLE{Allen_1997,title={Achieving fluency in modernized and formalized Hohfeld: puzzles and games for the LEGAL RELATIONS Language},year={1997},author={Layman E. Allen and Layman E. Allen and Charles S. Saxon and Charles S. Saxon},doi={10.1145/261618.261623},pmid={null},pmcid={null},mag_id={2156919195},journal={null},abstract={A significant refinement has been made in the AHOHFELD representation language, linlciig it realistically to decisions being made in the legal system, and it has been renamed to become the LEGAL RELATIONS Language. Similar changes have been made in the underlying AHOHFELD logic, which has become the Logic of LEGAL RELATIONS. A series of 27 puzzles and games, designed to enable legal problem solvers to become fluent in the LEGAL RELATIONS Language, are described and illustrated briefly in this article. More detailed presentation of the Play-A-Round puzzles and the Clever Plaintiff and the Legal Argument games are available at Internet site: http://thinkers.law.umich.edu.}}
@ARTICLE{Toval_2002,title={Legal requirements reuse: a critical success factor for requirements quality and personal data protection},year={2002},author={Ambrosio Toval and Ambrosio Toval and Ángel Prior Olmos and A. Olmos and Mario Piattini and Mario Piattini},doi={10.1109/icre.2002.1048511},pmid={null},pmcid={null},mag_id={2157329240},journal={null},abstract={Information technology misuse has increased the vulnerability of personal data, which has lead to growing concern about issues of personal privacy among political leaders, IT managers, information security consultants and the millions of people currently online. Many countries have developed, or are preparing, laws and regulations to combat the related threats and to guarantee personal data protection. Despite efforts to construct secure systems, few papers have, as yet, focused on security from the very outset of the system development life-cycle. This paper presents a pragmatic proposal to incorporate the legal and regulatory measures to guarantee personal data protection as a part of the requirements engineering process, instead of an addendum to system deployment. The authors investigate how recent efforts in the requirements engineering field can contribute to improving security issues in information systems, in particular those dealing with personal data. A reusable collection of security requirements and, as a novelty, personal data protection requirements (including information on related software components links) are provided. The pre-defined requirements, together with a simple process model based on requirements reuse, provide a strategy that organizations can use to become privacy-compliant.}}
@ARTICLE{Yu_1997,title={Towards modelling and reasoning support for early-phase requirements engineering},year={1997},author={Eric Yu and Eric Yu},doi={10.1109/isre.1997.566873},pmid={null},pmcid={null},mag_id={2157437711},journal={Requirements Engineering},abstract={Requirements are usually understood as stating what a system is supposed to do, as apposed to how it should do it. However, understanding the organizational context and rationales (the "Whys") that lead up to systems requirements can be just as important for the ongoing success of the system. Requirements modelling techniques can be used to help deal with the knowledge and reasoning needed in this earlier phase of requirements engineering. However most existing requirements techniques are intended more for the later phase of requirements engineering, which focuses on completeness, consistency, and automated verification of requirements. In contrast, the early phase aims to model and analyze stakeholder interests and how they might be addressed, or compromised, by various system-and-environment alternatives. This paper argues, therefore, that a different kind of modelling and reasoning support is needed for the early phase. An outline of the i* framework is given as an example of a step in this direction. Meeting scheduling is used as a domain example.}}
@ARTICLE{Horty_2012,title={Reasons as Defaults},year={2012},author={John F. Horty and John F. Horty},doi={null},pmid={null},pmcid={null},mag_id={2157503655},journal={null},abstract={Introduction I Default logic 1 A Primer on Default Logic 1.1 Basic concepts 1.1.1 Default rules 1.1.2 Priority relations 1.1.3 Theories and scenarios 1.2 Central definitions 1.2.1 Binding defaults 1.2.2 Proper scenarios and extensions 1.3 Extensions and conclusions 1.3.1 Theories with multiple extensions 1.3.2 Theories without extensions 2 From Defaults to Reasons 2.1 An austere theory of reasons 2.2 Developing the theory 2.2.1 Conflict, strength, and defeat 2.2.2 Reasons and enablers 2.2.3 Reason amalgamation II Deontic Logic 3 Reasons and Oughts 3.1 The two logics 3.1.1 Simple oughts 3.1.2 Conditional oughts 3.1.3 Some history 3.2 Properties of the logics 4 Moral Conflicts 4.1 Logical objections 4.1.1 Agglomeration 4.1.2 Other logical objections 4.2 Conceptual objections 4.3 Objections based on reasons as moral forces III Some Elaborations 5 Variable Priorities and Exclusion 5.1 Variable priority default theories 5.1.1 The definition 5.1.2 Some examples 5.2 Exclusionary default theories 5.2.1 The definition 5.2.2 Some examples 5.3 Discussion 5.3.1 Downward closure of exclusion 5.3.2 Exclusion by weaker defaults 5.3.3 Excluders, intensifiers, and attenuators 6 Particularism 6.1 Dancy's argument 6.2 Evaluating the argument 6.3 Discussion 6.3.1 Pragmatic considerations 6.3.2 Borrowing a book 6.3.3 Moderate particularism IV Some complications 7 Skepticism and Floating Conclusions 7.1 Floating conclusions 7.1.1 Arguments and paths 7.1.2 Two versions of skepticism 7.2 The problem with floating conclusions 7.2.1 An example 7.2.2 Objections to the example 7.3 Discussion 7.3.1 Other examples 7.3.2 Skepticism 8 Problems with Priorities 8.1 Refining the concept of defeat 8.2 Controlling the order of application 8.3 Discussion 8.3.1 Inappropriate equilibria 8.3.2 Other orderings 8.3.3 Reinstatement V Appendices A Notes on the default logics A.1 Proper scenarios A.2 Some observations on defeat A.3 Normal default theories B Notes on the deontic logics B.1 A comparison with van Fraassen's logic B.2 A comparison with standard deontic logic}}
@ARTICLE{Kamsties_2001,title={Detecting Ambiguities in Requirements Documents Using Inspections},year={2001},author={Erik Kamsties and Erik Kamsties and Daniel M. Berry and Daniel M. Berry and Barbara Paech and Barbara Paech},doi={null},pmid={null},pmcid={null},mag_id={2158076786},journal={null},abstract={Natural language is the most used representation for stating requirements on computer-based systems in industry. However, natural language is inherently ambiguous. Customers and software developers can disagree on the interpretation of a requirement without being aware of this fact. This disagreement can result in disastrous software failures. We argue that ambiguity is a more complex phenomenon than is often recognized in the literature. While inconsistencies and some types of incompleteness can be mechanically detected in formal specifications, ambiguities in informal specicifications often result in incorrect formal specifications. These misinterpretations can be detected only by execution or inspection of the formal specification. We suggest an inspection technique for detecting ambiguities in informal requirement documents before formal specifications are produced in order to avoid misinterpretations, rather than searching for them afterwards. We show how this technique can be tailored to different project contexts with the help of metamodels to increase its effectiveness. Finally, we report on experimental validation of the technique.}}
@ARTICLE{Lame_2005,title={Using NLP techniques to identify legal ontology components: concepts and relations},year={2005},author={Guiraude Lame and Guiraude Lame},doi={null},pmid={null},pmcid={null},mag_id={2158912860},journal={null},abstract={A method to identify ontology components is presented in this article. The method relies on Natural Language Processing (NLP) techniques to extract concepts and relations among these concepts. This method is applied in the legal field to build an ontology dedicated to information retrieval. Legal texts on which the method is performed are carefully chosen as describing and conceptualizing the legal domain. We suggest that this method can help legal ontology designers and may be used while building ontologies dedicated to other tasks than information retrieval}}
@ARTICLE{Waard_2008,title={Modeling Scientific Research Articles - Shifting Perspectives and Persistent Issues},year={2008},author={Anita de Waard and Anita de Waard and Joost Kircz and Joost Kircz},doi={null},pmid={null},pmcid={null},mag_id={2159035778},journal={null},abstract={We review over 10 years of research at Elsevier and various Dutch academic institutions on establishing a new format for the scientific research article. Our work rests on two main theoretical principles: the concept of modular documents, consisting of content elements that can exist and be published independently and are linked by meaningful relations, and the use of semantic data standards allowing access to heterogeneous data. We discuss the application of these concepts in five different projects: a modular format for physics articles, an XML encyclopedia in pharmacology, a semantic data integration project, a modular format for computer science proceedings papers, and our current work on research articles in cell biology.}}
@ARTICLE{Lehman_1997,title={Metrics and laws of software evolution-the nineties view},year={1997},author={M. M. Lehman and Meir M. Lehman and Juan F. Ramil and Juan F. Ramil and Paul Wernick and P.D. Wernick and P D Wernick and Paul Wernick and Dewayne E. Perry and Dewayne E. Perry and Władyslaw M. Turski and Wladyslaw M. Turski},doi={10.1109/metric.1997.637156},pmid={null},pmcid={null},mag_id={2159682002},journal={null},abstract={The process of E-type software development and evolution has proven most difficult to improve, possibly due to the fact that the process is a multi-input, multi-output system involving feedback at many levels. This observation, first recorded in the early 1970s during an extended study of OS/360 evolution, was recently captured in a FEAST (Feedback, Evolution And Software Technology) hypothesis: a hypothesis being studied in on-going two-year project, FEAST/1. Preliminary conclusions based on a study of a financial transaction system-Logica's Fastwire (FW)-are outlined and compared with those reached during the earlier OS/360 study. The new analysis supports, or better does not contradict, the laws of software evolution, suggesting that the 1970s approach to metric analysis of software evolution is still relevant today. It is hoped that FEAST/1 will provide a foundation for mastering the feedback aspects of the software evolution process, opening up new paths for process modelling and improvement.}}
@ARTICLE{Califf_2003,title={Health Insurance Portability and Accountability Act (HIPAA) Must There Be a Trade-Off Between Privacy and Quality of Health Care, or Can We Advance Both?},year={2003},author={Robert M. Califf and Robert M. Califf and Lawrence H. Muhlbaier and Lawrence H. Muhlbaier},doi={10.1161/01.cir.0000085720.65685.90},pmid={12939241},pmcid={null},mag_id={2159817467},journal={Circulation},abstract={The Health Insurance Portability and Accountability Act of 1996 (HIPAA) presents an important challenge to the healthcare system in its evolution from a cottage industry to a new, yet-to-be determined form. To guide the system rationally requires clinical research on a massive scale. Will HIPAA stimulate an advance in medical research so that we have evidence to guide our medical decisions and policies, or will it lead to Draconian restrictions that push the healthcare system toward a less rational, less informed approach?

Medical practice evolved in the 20th century from an almost purely anecdote-based apprenticeship system to a system in which doctors were trained by learning the mechanisms of disease and serving apprenticeships in which they learned the “tools of the trade.” Records were kept on paper and, although classification of disorders has been critical to all areas of scientific study, attention to nomenclature has been largely unsystematic in medicine. In parallel, a complex array of computerized systems has been developed for the business side of medicine, with little connection to the actual delivery of care. The dissociation between clinical and billing systems reflects the incorrect idea, basic in medical training, that doctors, armed with knowledge of disease mechanisms, can practice medicine by using deductive reasoning with little need for empirical decision support.

Most recently, the Internet and supporting information systems have revolutionized the nonmedical world. Compare the functioning of the banking industry versus that of the healthcare industry. Almost anyone with a bank account can withdraw funds or pay bills from anywhere on Earth via computer. Making sensitive information about people’s finances available is possible because of common nomenclature and data standards adopted by the finance industry. In contrast, the medical community is a haphazard mix of paper- and computer-based systems with multiple nomenclatures that do not allow the …}}
@ARTICLE{Galgani_2010,title={LEXA: Towards Automatic Legal Citation Classification},year={2010},author={Filippo Galgani and Filippo Galgani and Achim Hoffmann and Achim Hoffmann},doi={10.1007/978-3-642-17432-2_45},pmid={null},pmcid={null},mag_id={2160571468},journal={null},abstract={In this paper we present our approach towards legal citation classification using incremental knowledge acquisition. This forms a part of our more ambitious goal of automatic legal text summarization. We created a large training and test corpus from court decision reports in Australia. We showed that, within less than a week, it is possible to develop a good quality knowledge base which considerably outperforms a baseline Machine Learning approach. We note that the problem of legal citation classification allows the use of Machine Learning as classified training data is available. For other subproblems of legal text summarization this is unlikely to be the case.}}
@ARTICLE{Finin_1994,title={KQML as an agent communication language},year={1994},author={Tim Finin and Tim Finin and Richard Fritzson and Richard Fritzson and Don McKay and Donald P. McKay and Don McKay and Robin McEntire and Robin McEntire},doi={10.1145/191246.191322},pmid={null},pmcid={null},mag_id={2160851181},journal={null},abstract={This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part of a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML focuses on an extensible set of  performatives , which defines the permissible “speech acts” agents may use and comprise a substrate on which to develop higher-level models of interagent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called  communication facilitors  which coordinate the interactions of other agents. The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling.}}
@ARTICLE{Timmer_2015,title={A structure-guided approach to capturing bayesian reasoning about legal evidence in argumentation},year={2015},author={Sjoerd T. Timmer and Sjoerd T. Timmer and John‐Jules Ch. Meyer and John-Jules Ch. Meyer and Henry Prakken and Henry Prakken and Silja Renooij and Silja Renooij and Bart Verheij and Bart Verheij},doi={10.1145/2746090.2746093},pmid={null},pmcid={null},mag_id={2161465833},journal={null},abstract={Over the last decades the rise of forensic sciences has led to an increase in the availability of statistical evidence. Reasoning about statistics and probabilities in a forensic science setting can be a precarious exercise, especially so when independencies between variables are involved. To facilitate the correct explanation of such evidence we investigate how argumentation models can help in the interpretation of statistical information. In this paper we focus on the connection between argumentation models and Bayesian belief networks, the latter being a common model to represent and reason with complex probabilistic information. We introduce the notion of a support graph as an intermediate structure between Bayesian networks and argumentation models. A support graph disentangles the complicating graphical properties of a Bayesian network and enhances its intuitive interpretation. Moreover, we show that this model can provide a suitable template for argumentative analysis. Especially in the context of legal reasoning, the correct treatment of statistical evidence is important.}}
@ARTICLE{Kitchenham_1995,title={Towards a framework for software measurement validation},year={1995},author={Barbara Kitchenham and Barbara Kitchenham and Shari Lawrence Pfleeger and Shari Lawrence Pfleeger and Norman Fenton and Norman Fenton},doi={10.1109/32.489070},pmid={null},pmcid={null},mag_id={2161477496},journal={IEEE Transactions on Software Engineering},abstract={In this paper we propose a framework for validating software measurement. We start by defining a measurement structure model that identifies the elementary component of measures and the measurement process, and then consider five other models involved in measurement: unit definition models, instrumentation models, attribute relationship models, measurement protocols and entity population models. We consider a number of measures from the viewpoint of our measurement validation framework and identify a number of shortcomings; in particular we identify a number of problems with the construction of function points. We also compare our view of measurement validation with ideas presented by other researchers and identify a number of areas of disagreement. Finally, we suggest several rules that practitioners and researchers can use to avoid measurement problems, including the use of measurement vectors rather than artificially contrived scalars.}}
@ARTICLE{Prakken_2005,title={Dialogues about the burden of proof},year={2005},author={Henry Prakken and Henry Prakken and Chris Reed and Chris Reed and Douglas Walton and Douglas Walton},doi={10.1145/1165485.1165503},pmid={null},pmcid={null},mag_id={2161664509},journal={null},abstract={This paper analyses the phenomenon of a shift of the burden of proof in legal persuasion dialogues. Some sample dialogues are analysed of types of situations where such a shift may occur, viz. reasoning with defeasible rules, reasoning with argumentation schemes and reasoning with mere presumptions. It is argued that whether a shift in the burden of proof occurs can itself become the subject of dispute and it is shown how a dialogue game protocol for persuasion can be extended to let it regulate persuasion dialogues about the burden of proof. It is also shown that dialogues about the burden of proof are often implicitly about the precise form of the rules used in an argument.}}
@ARTICLE{Dieste_2011,title={Systematic review and aggregation of empirical studies on elicitation techniques},year={2011},author={Óscar Dieste and Oscar Dieste and Natália Juristo and Natalia Juristo},doi={10.1109/tse.2010.33},pmid={null},pmcid={null},mag_id={2162000403},journal={IEEE Transactions on Software Engineering},abstract={We have located the results of empirical studies on elicitation techniques and aggregated these results to gather empirically grounded evidence. Our chosen surveying methodology was systematic review, whereas we used an adaptation of comparative analysis for aggregation because meta-analysis techniques could not be applied. The review identified 564 publications from the SCOPUS, IEEEXPLORE, and ACM DL databases, as well as Google. We selected and extracted data from 26 of those publications. The selected publications contain 30 empirical studies. These studies were designed to test 43 elicitation techniques and 50 different response variables. We got 100 separate results from the experiments. The aggregation generated 17 pieces of knowledge about the interviewing, laddering, sorting, and protocol analysis elicitation techniques. We provide a set of guidelines based on the gathered pieces of knowledge.}}
@ARTICLE{Runeson_2009,title={Guidelines for conducting and reporting case study research in software engineering},year={2009},author={Per Runeson and Per Runeson and Martin Höst and Martin Höst},doi={10.1007/s10664-008-9102-8},pmid={null},pmcid={null},mag_id={2162739315},journal={Empirical Software Engineering},abstract={Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.}}
@ARTICLE{Jackson_1995,title={The world and the machine},year={1995},author={Michael Jackson and Michael Jackson},doi={10.1145/225014.225041},pmid={null},pmcid={null},mag_id={2162773911},journal={null},abstract={As software developers we are engineers because we make useful machines. We are concerned both with the world, in which the machine serves a useful purpose, and with the machine itself. The competing demands and attractions of these two concerns must be appropriately balanced. Failure to balance them harms our work. Certainly it must take some of the blame for the gulf between researchers and practitioners in software development. To achieve proper balance we must sometimes fight against tendencies and inclinations that are deeply ingrained in our customary practices and attitudes in software development. In this paper some aspects of the relationship between the world and the machine are explored; some sources of distortion are identified; and some suggestions are put forward for maintaining a proper balance.}}
@ARTICLE{Konstantinou_1993,title={Can legal knowledge be derived from legal texts},year={1993},author={Vassilis Konstantinou and Vassilis Konstantinou and J. T. Sykes and John Sykes and Georgios N. Yannopoulos and Georgios N. Yannopoulos},doi={10.1145/158976.159004},pmid={null},pmcid={null},mag_id={2162905545},journal={null},abstract={Knowledge acquisition is undoubtedly one of the major bottle-necks in the development of legal expert systems. Usually the knowledge is collected by knowledge engineers who are forced to make their own interpretations of the knowledge in order to map it on a knowledge representation technique, thus resulting into erroneous and legally unacceptable interpretations of the law. The aim of NOMOS (an EC supported project under the ESPRIT II initiative) was to assist the knowledge engineer by providng tools that perform semi-automatic knowledge acquisition from legal texts in Italian and French. This paper reports on the results of the first evaluation of the knowledge collected by these tools. The evaluation was performed by complementing the tools with a fully functional expert system  that accepted the generated knowledge bases and allowed experts to test the completeness of the knowledge through a series of interactive consultations. The knowledge base used for this evaluation was derived from the text for the Italian Value Added Tax Law. The text was pre-processed in its ASCII form by the Nomos tools and the generated knowledge base was filtered through to a conventional expert system shell to generate the evaluation expert system.  Knowledge extracted directly from text was converted into a hybrid of production rules and Conceptual Graphs. [see SOWA 1984] Knowledge collected from other sources, such as previously resolved cases, explanations of terms and examples, were linked to the knowledge base using an automated hypertext technique. [see KONSTANTINOU  &  MORSE 1992] Finally, the expert system was tested using real-life cases supplied by the Italian ministry of finance.}}
@ARTICLE{Penzenstadler_2008,title={Complying with Law for RE in the Automotive Domain},year={2008},author={Birgit Penzenstadler and Birgit Penzenstadler and Jörg Leuser and J. Leuser},doi={10.1109/relaw.2008.3},pmid={null},pmcid={null},mag_id={2163342495},journal={null},abstract={The automotive industry is concerned with developing large and complex embedded systems. The original equipment manufacturers (OEMs) are responsible for the safety of their systems, enforced by law in terms of liability. At the same time, there is a number of laws, for example the Automobile Safety Act, that has to be obeyed by the specification. We give insights into the current state of practice about how the automotive industry performs requirements engineering in order to comply with government laws and regulations. We analyse the challenges, and give ideas from research work in progress for tackling them.}}
@ARTICLE{Ottens_2004,title={Modeling engineering systems as socio-technical systems},year={2004},author={Maarten Ottens and Maarten Ottens and Maarten Franssen and Maarten Franssen and Peter Kroes and Peter Kroes and Ibo van de Poel and I.R. (Ibo) van de Poel},doi={10.1109/icsmc.2004.1401100},pmid={null},pmcid={null},mag_id={2163432018},journal={null},abstract={The IEEE standard for the systems engineering process is problematic when applied to the design of (socio-technical) systems. This is argued using examples of automated vehicle systems. A conceptualization of socio-technical systems is introduced. This distinguishes technical and social elements and agents, as well as four kinds of relations. Next to physical and functional relations, intentional and normative relations play an important role. The IEEE standard defines social elements as contextual and focuses on total design control. Because of the involvement of agents and social elements in socio-technical systems both these viewpoints are problematic.}}
@ARTICLE{Herrmann_2008,title={Requirements Prioritization Based on Benefit and Cost Prediction: An Agenda for Future Research},year={2008},author={Andrea Herrmann and Andrea Herrmann and Andrea Herrmann and Maya Daneva and Maya Daneva},doi={10.1109/re.2008.48},pmid={null},pmcid={null},mag_id={2163518963},journal={null},abstract={In early phases of the software cycle, requirements prioritization necessarily relies on the specified requirements and on predictions of benefit and cost of individual requirements. This paper presents results of a systematic review of literature, which investigates how existing methods approach the problem of requirements prioritization based on benefit and cost. From this review, it derives a set of under-researched issues which warrant future efforts and sketches an agenda for future research in this area.}}
@ARTICLE{Conrad_2007,title={Opinion mining in legal blogs},year={2007},author={Jack G. Conrad and Jack G. Conrad and Frank Schilder and Frank Schilder},doi={10.1145/1276318.1276363},pmid={null},pmcid={null},mag_id={2163696229},journal={null},abstract={We perform a survey into the scope and utility of opinion mining in legal Weblogs (a.k.a. blawgs). The number of 'blogs' in the legal domain is growing at a rapid pace and many potential applications for opinion detection and monitoring are arising as a result. We summarize current approaches to opinion mining before describing different categories of blawgs and their potential impact on the law and the legal profession. In addition to educating the community on recent developments in the legal blog space, we also conduct some introductory opinion mining trials. We first construct a Weblog test collection containing blog entries that discuss legal search tools. We subsequently examine the performance of a language modeling approach deployed for both subjectivity analysis (i.e., is the text subjective or objective?) and polarity analysis (i.e., is the text affirmative or negative towards its subject?). This work may thus help establish early baselines for these core opinion mining tasks.}}
@ARTICLE{Walton_2014,title={On a razor's edge: evaluating arguments from expert opinion},year={2014},author={Douglas Walton and Douglas Walton},doi={10.1080/19462166.2013.858183},pmid={null},pmcid={null},mag_id={2163927353},journal={Argument & Computation},abstract={This paper takes an argumentation approach to find the place of trust in a method for evaluating arguments from expert opinion. The method uses the argumentation scheme for argument from expert opinion along with its matching set of critical questions. It shows how to use this scheme in three formal computational argumentation models that provide tools to analyse and evaluate instances of argument from expert opinion. The paper uses several examples to illustrate the use of these tools. A conclusion of the paper is that from an argumentation point of view, it is better to critically question arguments from expert opinion than to accept or reject them based solely on trust.}}
@ARTICLE{Salton_1975,title={A vector space model for automatic indexing},year={1975},author={Gerard Salton and Gerard Salton and Amy Wong and A. Wong and Cheng‐San Yang and C. S. Yang},doi={10.1145/361219.361220},pmid={null},pmcid={null},mag_id={2165612380},journal={Communications of The ACM},abstract={In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.}}
@ARTICLE{Allen_2000,title={A multi-agent legal argument generator},year={2000},author={Mike Allen and M. Allen and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Geof Staniford and Geof Staniford},doi={10.1109/dexa.2000.875161},pmid={null},pmcid={null},mag_id={2165765899},journal={null},abstract={One of the most significant series of experiments in AI and law is the investigation of case based reasoning carried out in the HYPO, CABERET and CATO projects. It is important to understand what has been achieved in these experiments, and this requires that the techniques be applied to a variety of domains. The techniques are not, however, straightforward to apply. In order to provide an experimental environment for investigating the techniques further, we have re-implemented the central argument generation process as a set of agents written in Java using IBM's Aglet agent building framework. By using this environment it will be possible to explore the general applicability of the techniques. In this paper we give a summary of an algorithmic description of argument building in HYPO and CATO, and describe our implementation of the algorithm.}}
@ARTICLE{Duan_2009,title={Towards automated requirements prioritization and triage},year={2009},author={Chuan Duan and Chuan Duan and Paula Laurent and Paula Laurent and Jane Cleland‐Huang and Jane Cleland-Huang and Charles Kwiatkowski and Charles Kwiatkowski},doi={10.1007/s00766-009-0079-7},pmid={null},pmcid={null},mag_id={2167417301},journal={Requirements Engineering},abstract={Time-to-market deadlines and budgetary restrictions require stakeholders to carefully prioritize requirements and determine which ones to implement in a given product release. Unfortunately, existing prioritization techniques do not provide sufficient automation for large projects with hundreds of stakeholders and thousands of potentially conflicting requests and requirements. This paper therefore describes a new approach for automating a significant part of the prioritization process. The proposed method utilizes data-mining and machine learning techniques to prioritize requirements according to stakeholders’ interests, business goals, and cross-cutting concerns such as security or performance requirements. The effectiveness of the approach is illustrated and evaluated through two case studies based on the requirements of the Ice Breaker System, and also on a set of stakeholders’ raw feature requests mined from the discussion forum of an open source product named SugarCRM.}}
@ARTICLE{Baroni_2009,title={Semantics of Abstract Argument Systems},year={2009},author={Pietro Baroni and Pietro Baroni and Massimiliano Giacomin and Massimiliano Giacomin},doi={10.1007/978-0-387-98197-0_2},pmid={null},pmcid={null},mag_id={2167497429},journal={null},abstract={An abstract argument system or argumentation framework, as introduced in a seminal paper by Dung [13], is simply a pair 〈A ,R〉 consisting of a set A whose elements are called arguments and of a binary relation R on A called attack relation. The set A may be finite or infinite in general, however, given the introductory purpose of this chapter, we will restrict the presentation to the case of finite sets of arguments. An argumentation framework has an obvious representation as a directed graph where nodes are arguments and edges are drawn from attacking to attacked arguments. A simple example of argumentation framework AF1 = 〈{a,b},{(b,a)}〉 is shown in Figure 1. While the word argument may recall several intuitive meanings, like the ones of “line of reasoning leading from some premise to a conclusion” or of “utterance in a dispute”, abstract argument systems are not (even implicitly or indirectly) bound to any of them: an abstract argument is not assumed to have any specific structure but, roughly speaking, an argument is anything that may attack or be attacked by another argument. Accordingly, the argumentation framework depicted in Figure 1 is suitable to represent many different situations. For instance, in a context of reasoning about weather, argument a may be associated with the inferential step}}
@ARTICLE{Ternai_2015,title={Semi-automatic Methodology for Compliance Checking on Business Processes},year={2015},author={Katalin Ternai and Katalin Ternai},doi={10.1007/978-3-319-22389-6_18},pmid={null},pmcid={null},mag_id={2167839143},journal={null},abstract={The paper aims to provide a semi-automatic methodology, which can be used to validate and improve business processes. The main goal is the process ontology matching, based on ontologies derived from business process models and regulations, rules and policies. The paper introduces a method using ontology building and matching for compliance checking on business processes partway automatically. The objective of this approach is to transform the business process into process ontology and to build reference process ontology from unstructured documents in order to apply ontology matching procedure to restructure, validate and improve the business process. Processes in public administration are also complex and changing fast according to the changes in regulatory environment. In the case study, we illustrate the methodology related to the process of “Assessment of the KICs past performance”, to improve the process at the European Institute of Innovation and Technology (EIT).}}
@ARTICLE{Kitchenham_2010,title={Systematic literature reviews in software engineering - A tertiary study},year={2010},author={Barbara Kitchenham and Barbara Kitchenham and Rialette Pretorius and Rialette Pretorius and David Budgen and David Budgen and O. P. Brereton and O. Pearl Brereton and Mark Turner and Mark Turner and Mahmood Niazi and Mahmood Niazi and Stephen Linkman and Stephen Linkman},doi={10.1016/j.infsof.2010.03.006},pmid={null},pmcid={null},mag_id={2168130919},journal={Information & Software Technology},abstract={null}}
@ARTICLE{Krausová_2009,title={Legal Patterns Implement Trust in IT Requirements: When Legal Means are the "Best" Implementation of IT Technical Goals},year={2009},author={Alžběta Krausová and Alzbeta Krausova and Fabio Massacci and Fabio Massacci and Ayda Saïdane and Ayda Saidane},doi={10.1109/relaw.2009.7},pmid={null},pmcid={null},mag_id={2168508333},journal={null},abstract={The traditional approach of computer scientists to Law is that laws (statutes, regulations, etc.) set the requirements, logicians and requirements analysts model them, and finally IT technical solutions or organizational patterns are used to implement them. In this paper we try to answer a radically different question: Can a technical solution (e.g. a requirement in a security and dependability pattern) be implemented by legal means? We show how Legal Patterns, that represent the legal analogy of Software Patterns, can be formally used to implement trust relations required by security and dependability patterns.}}
@ARTICLE{Bench‐Capon_2000,title={Using values and theories to resolve disagreement in law 1},year={2000},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Giovanni Sartor and Giovanni Sartor},doi={null},pmid={null},pmcid={null},mag_id={2168812553},journal={null},abstract={In this paper we describe a novel approach to reasoning with cases and precedents. The approach is intended to address two main problems. First we find that current case based reasoning systems tend to offer relatively little support in determining the outcome of a case. They either present a list of cases which may inform, but cannot determine, the outcome, or else, as in HYPO and its successors ((1), (2), (3)), present arguments for both sides of a question leaving it the user to decide which is the more persuasive. What is lacking from these accounts is a notion of what it is that makes an argument persuasive. This is addressed in the context of AI and Law by Berman and Hafner in (4), and in law generally by Perelman (e.g. (5)). For Berman and Hafner an argument is made persuasive by supporting the purposes that the law is designed for, and for Perelman it is by advancing or protecting values that its audience subscribes to (on teleological argument, see also (6)). We believe these things to be effectively the same: the purpose of a law is typically to advance or promote some desired value, and the audience is the community subject to the law. Thus our first goal is to provide a model of case based reasoning in which we can use purposes and values to explain disagreements and their resolution. The second problem is the lack of the notion of context in many of the existing case based reasoning systems. A given case is decided in the context both of relevant past cases, which can supply precedents which will inform the decision, and in the context of future cases to which it will be relevant and possibly act as a precedent. A case is thus supposed to cohere with both past decisions and future decisions. This context is largely lost if we state the question as being whether one bundle of factors is more similar to the factors of a current case than another bundle, as in HYPO, or whether one rule is preferred to another, as in logical reconstructions of such systems, for example that of (7)). Recognition of context is vital if we are to understand accounts of legal reasoning (e.g. (8)) in which it is 1 This paper represents current on-going work investigating argument in case law. The ideas represent a}}
@ARTICLE{Saaty_1985,title={The Analytic Hierarchy Process},year={1985},author={Thomas L. Saaty and Thomas L. Saaty and Kevin P. Kearns and Kevin P. Kearns and Kevin P. Kearns},doi={10.1016/b978-0-08-032599-6.50008-8},pmid={null},pmcid={null},mag_id={2168936936},journal={null},abstract={null}}
@ARTICLE{Antón_2001,title={The role of policy and stakeholder privacy values in requirements engineering},year={2001},author={Annie I. Antón and Annie I. Antón and Julia B. Earp and Julia B. Earp and Colin Potts and Colin Potts and Thomas A. Alspaugh and Thomas A. Alspaugh},doi={10.1109/isre.2001.948553},pmid={null},pmcid={null},mag_id={2169133134},journal={Requirements Engineering},abstract={Diverse uses of information technology (IT) in organizations affect privacy. Developers of electronic commerce, database management, security mechanisms, telecommunication and collaborative systems should be aware of these effects and acknowledge the need for early privacy planning during the requirements definition activity. Public concerns about the collection of personal information by consumer-based Web sites have led most organizations running such sites to establish and publish privacy policies. However, these policies often fail to align with prevalent societal values on one hand and the operational functioning of Web-based applications on the other. Assuming that such misalignments stem from imperfect appreciation of consequences and not an intent to deceive, we discuss concepts, tools and techniques to help requirements engineers and IT policy makers bring policies and system requirements into better alignment. Our objective is to encourage RE researchers and practitioners to adopt a more holistic view of application and system specification, in which a system or application is seen as an engine of policy enforcement and values attainment.}}
@ARTICLE{Ryan_1997,title={Prioritizing software requirements in an industrial setting},year={1997},author={Kevin Ryan and Kevin Ryan and Jenny Karlsson and Joachim Karlsson},doi={10.1145/253228.253453},pmid={null},pmcid={null},mag_id={2169249088},journal={null},abstract={The planning of additional features and releases is a major concern for commercial software companies. We describe how, in collaboration with Ericsson Radio Systems, we developed and tested an industrially useful approach to software requirements prioritization.}}
@ARTICLE{Kraemer_1981,title={Intergroup concordance: Definition and estimation},year={1981},author={Helena C. Kraemer and Helena C. Kraemer},doi={10.1093/biomet/68.3.641},pmid={null},pmcid={null},mag_id={2170959994},journal={Biometrika},abstract={SUMMARY A coefficient of intergroup concordance is proposed which is consistent with the concept of intragroup concordance as measured by Kendall's coefficient of concordance (Kendall & Babington Smith, 1939). This approach reconciles the approaches of Schucany & Frawley (1973) and of Hollander & Sethuraman (1978) to the problem of two-group concordance. Estimation and test procedures for the population are based on jackknife procedures. Extension to the problem of multiple intergroup concordance when groups have factorial structure is noted.}}
@ARTICLE{Hayes_2007,title={REquirements TRacing On target (RETRO): improving software maintenance through traceability recovery},year={2007},author={Jane Huffman Hayes and Jane Huffman Hayes and Alex Dekhtyar and Alex Dekhtyar and Senthil Karthikeyan Sundaram and Senthil Karthikeyan Sundaram and E. Ashlee Holbrook and E. Ashlee Holbrook and Sravanthi Vadlamudi and Sravanthi Vadlamudi and Alain April and Alain April},doi={10.1007/s11334-007-0024-1},pmid={null},pmcid={null},mag_id={2171144693},journal={Innovations in Systems and Software Engineering},abstract={A number of important tasks in software maintenance require an up-to-date requirements traceability matrix (RTM): change impact analysis, determination of test cases to execute for regression testing, etc. The generation and maintenance of RTMs are tedious and error-prone, and they are hence often not done. In this paper, we present REquirements TRacing On-target (RETRO), a special- purpose requirements tracing tool. We discuss how RETRO automates the generation of RTMs and present the results of a study comparing manual RTM generation to RTM generation using RETRO. The study showed that RETRO found significantly more correct links than manual tracing and took only one third of the time to do so.}}
@ARTICLE{Winkels_2005,title={Constructing a semantic network for legal content},year={2005},author={Radboud Winkels and Radboud Winkels and Alexander Boer and Alexander Boer and Emile de Maat and Emile de Maat and Tom van Engers and Tom M. van Engers and Matthijs Breebaart and Matthijs Breebaart and Henri Melger and Henri Melger},doi={10.1145/1165485.1165505},pmid={null},pmcid={null},mag_id={2171150668},journal={null},abstract={The Dutch Tax and Customs Administration (DTCA) is one of many organizations that deal with a multitude of electronic legal data, from various sources and in different formats. In this paper, we describe the results of a study aimed at better access to these sources by having a supplier and format independent knowledge store that describes the sources and their interrelations in a semantic network. Furthermore we developed parsers to automatically detect the identity of sources and typed references within the sources to other legal documents. These parsers can be used to fill and update the semantic network as new documents are added.}}
@ARTICLE{Šmite_2008,title={Reporting Empirical Research in Global Software Engineering: A Classification Scheme},year={2008},author={Darja Šmite and Darja Šmite and Claes Wohlin and Claes Wohlin and Robert Feldt and Robert Feldt and Tony Gorschek and Tony Gorschek},doi={10.1109/icgse.2008.22},pmid={null},pmcid={null},mag_id={2171194960},journal={null},abstract={Increased popularity of global software engineering (GSE) has resulted in quite a number of research and industrial studies. As the area matures, an increased focus on empirically supported results leads to a greater potential impact on future research and industrial practice. However, since GSE scenarios are diverse, what works in one context might not directly apply in another. Thus it is necessary to understand, how GSE-related empirical findings should be reported to be useful for practitioners and researchers. Furthermore, itdasias important to summarize progress and get the big picture of published research to identify gaps and commonalities. In this paper we analyze differentiating factors of GSE scenarios and offer a classification scheme for describing the context of a GSE study. In addition, we report initial results of a systematic review on GSE-related empirical literature using papers from ICGSE 2006 and 2007, at the same time illustrating and evaluating the proposed scheme.}}
@ARTICLE{Dijkman_2009,title={Aligning Business Process Models},year={2009},author={Remco Dijkman and Remco M. Dijkman and Marlon Dumas and Marlon Dumas and Luciano García‐Bañuelos and Luciano García-Bañuelos and Reina Käärik and Reina Kaarik},doi={10.1109/edoc.2009.11},pmid={null},pmcid={null},mag_id={2171275211},journal={null},abstract={This paper studies the following problem: given a pair of business process models, determine which elements in one model are related to which elements in the other model. This problem arises in the context of merging different versions or variants of a business process model or when comparing business process models in order to display their similarities and differences. The paper investigates two approaches to this alignment problem: one based purely on lexical matching of pairs of elements and another based on error-correcting graph matching. Using a set of models taken from real-life scenarios, the paper empirically shows that graph matching techniques yield a significantly higher precision than pure lexical matching, while achieving comparable recall.}}
@ARTICLE{Lamsweerde_1998,title={Managing conflicts in goal-driven requirements engineering},year={1998},author={Axel van Lamsweerde and A. van Lamsweerde and Robert Darimont and Robert Darimont and Emmanuel Letier and Emmanuel Letier},doi={10.1109/32.730542},pmid={null},pmcid={null},mag_id={2171627300},journal={IEEE Transactions on Software Engineering},abstract={A wide range of inconsistencies can arise during requirements engineering as goals and requirements are elicited from multiple stakeholders. Resolving such inconsistencies sooner or later in the process is a necessary condition for successful development of the software implementing those requirements. The paper first reviews the main types of inconsistency that can arise during requirements elaboration, defining them in an integrated framework and exploring their interrelationships. It then concentrates on the specific case of conflicting formulations of goals and requirements among different stakeholder viewpoints or within a single viewpoint. A frequent, weaker form of conflict called divergence is introduced and studied in depth. Formal techniques and heuristics are proposed for detecting conflicts and divergences from specifications of goals/requirements and of domain properties. Various techniques are then discussed for resolving conflicts and divergences systematically by the introduction of new goals or by transforming the specifications of goals/objects toward conflict-free versions. Numerous examples are given throughout the paper to illustrate the practical relevance of the concepts and techniques presented. The latter are discussed in the framework of the KAOS methodology for goal-driven requirements engineering.}}
@ARTICLE{Gervasi_2011,title={Mining requirements links},year={2011},author={Vincenzo Gervasi and Vincenzo Gervasi and Didar Zowghi and Didar Zowghi},doi={10.1007/978-3-642-19858-8_19},pmid={null},pmcid={null},mag_id={2171775828},journal={null},abstract={[Context & motivation] Obtaining traceability among requirements and between requirements and other artifacts is an extremely important activity in practice, an interesting area for theoretical study, and a major hurdle in common industrial experience. Substantial effort is spent on establishing and updating such links in any large project - even more so when requirements refer to a product family. [Question/problem]While most research is concerned with ways to reduce the effort needed to establish and maintain traceability links, a different question can also be asked: how is it possible to harness the vast amount of implicit (and tacit) knowledge embedded in already-established links? Is there something to be learned about a specific problem or domain, or about the humans who establish traces, by studying such traces?

[Principal ideas/results] In this paper, we present preliminary results from a study applying different machine learning techniques to an industrial case study, and test to what degree common hypothesis hold in our case. [Contribution] Reshaping traceability data into knowledge can contribute to more effective automatic tools to suggest candidates for linking, to inform improvements in writing style, and at the same time provide some insight into both the domain of interest and the actual implementation techniques.}}
@ARTICLE{Bex_2008,title={Investigating Stories in a Formal Dialogue Game},year={2008},author={Floris Bex and Floris Bex and Henry Prakken and Henry Prakken},doi={null},pmid={null},pmcid={null},mag_id={2171957060},journal={null},abstract={In this paper we propose a formal dialogue game in which two players aim to determine the best explanation for a set of observations. By assuming an adversarial setting, we force the players to advance and improve their own explanations as well as criticize their opponent's explanations, thus hopefully preventing the well-known problem of 'tunnel vision'. A main novelty of our approach is that the game supports the combination of argumentation with abductive inference to the best explanation.}}
@ARTICLE{Boehm_1996,title={Identifying quality-requirement conflicts},year={1996},author={Barry Boehm and Barry Boehm and Taek Lee and Hoh Peter In},doi={10.1109/52.506460},pmid={null},pmcid={null},mag_id={2174366823},journal={IEEE Software},abstract={Without a well-defined set of quality-attribute requirements, software projects are vulnerable to failure. The authors have developed QARCC, a knowledge-based tool that helps users, developers, and customers analyze requirements and identify conflicts among them.}}
@ARTICLE{Prakken_1999,title={Dialectical Proof Theory for Defeasible Argumentation with Defeasible Priorities (Preliminary Report)},year={1999},author={Henry Prakken and Henry Prakken},doi={10.1007/3-540-46581-2_14},pmid={null},pmcid={null},mag_id={2178244880},journal={null},abstract={In this paper a dialectical proof theory is proposed for logical systems for defeasible argumentation that fit a certain format. This format is the abstract theory developed by Dung, Kowalski and others A main feature of the proof theory is that it also applies to systems in which reasoning about the standards for comparing arguments is possible The proof theory could serve as the 'logical core' of protocols for dispute in multi-agent decision making processes.}}
@ARTICLE{Sannier_2017,title={An automated framework for detection and resolution of cross references in legal texts},year={2017},author={Nicolas Sannier and Nicolas Sannier and Morayo Adedjouma and Morayo Adedjouma and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand},doi={10.1007/s00766-015-0241-3},pmid={null},pmcid={null},mag_id={2178561563},journal={Requirements Engineering},abstract={When identifying and elaborating compliance requirements, analysts need to follow the cross references in legal texts and consider the additional information in the cited provisions. Enabling easier navigation and handling of cross references requires automated support for the detection of the natural language expressions used in cross references, the interpretation of cross references in their context, and the linkage of cross references to the targeted provisions. In this article, we propose an approach and tool support for automated detection and resolution of cross references. The approach leverages the structure of legal texts, formalized into a schema, and a set of natural language patterns for legal cross reference expressions. These patterns were developed based on an investigation of Luxembourg's legislation, written in French. To build confidence about their applicability beyond the context where they were observed, these patterns were validated against the Personal Health Information Protection Act (PHIPA) by the Government of Ontario, Canada, written in both French and English. We report on an empirical evaluation where we assess the accuracy and scalability of our framework over several Luxembourgish legislative texts as well as PHIPA.}}
@ARTICLE{Antón_2003,title={The Use of Goals to Extract Privacy and Security Requirements from Policy Statements},year={2003},author={Annie I. Antón and Annie I. Antón and Davide Bolchini and Davide Bolchini and Qingfeng He and Qingfeng He},doi={null},pmid={null},pmcid={null},mag_id={2187202451},journal={null},abstract={This paper addresses the use of goals to extract non-functional requirements from policy statements. Goals are important precursors to software requirements, but the process of abstracting them from security and policy policies has not been thoroughly researched. We present a summary of a goal-based approach for extracting standard security and privacy requirements from policy statements and illustrate its application to analyze 40 financial privacy policies. We present heuristics to support goal analysis, goal refinement, and the development of tool support, including the establishment of a goal repository that can be used in future goal analyses. To gain a deeper understanding of the goal set, and to identify potential conflicts and inconsistencies between goals, we used i* to model semantic relationships between goals, their actors and strategic dependencies. The goal-based process will assist software engineers in the specification of system requirements that are in alignment an organization’s policies.}}
@ARTICLE{Loureiro_2004,title={A systems engineering framework for integrated automotive development: Regular Papers},year={2004},author={Geilson Loureiro and Geilson Loureiro and Paul Leaney and P.G. Leaney and Mike Hodgson and Mike Hodgson},doi={10.1002/sys.v7:2},pmid={null},pmcid={null},mag_id={2196834917},journal={Systems Engineering},abstract={Automotive development faces tightening regulatory requirements, shortening development cycle times, and growing complexity. To cope with such an environment, it is moving from a traditional evolutionary to a systems engineering approach. Great effort is being made for a shift from the traditional component focus, which has been enhanced by concurrent engineering, to a more broadened view supported by systems thinking. This broader view, however, is in practice strongly focused on the product elements of the system. This paper proposes a systems engineering framework for integrated automotive development—the total view approach. It is a modeling framework that integrates the product, its life cycle processes and their associated organizations throughout the requirements, functional and physical analysis processes, at all levels of the product breakdown structure, deriving attributes as emergent properties of a whole integrated system. The paper justifies the framework through a review of traditional and current automotive development and two case studies. A major benefit of the application of the framework is the ability to investigate early in the product development process the interactions between requirements and attributes not only of the product, but also of its life cycle processes and their associated organisations. This can lead to better product quality, lower life cycle cost, shorter development time, and manageable complexity. © 2004 Wiley Periodicals, Inc. Syst Eng 7: 153–166, 2004}}
@ARTICLE{Petroski_1986,title={The Engineer Is Human},year={1986},author={Henry Petroski and Henry Petroski and H. Petroski},doi={10.1115/1.3264777},pmid={null},pmcid={null},mag_id={2204641453},journal={Journal of Pressure Vessel Technology-transactions of The Asme},abstract={null}}
@ARTICLE{Faily_2011,title={Bridging User−Centered Design and Requirements Engineering with GRL and Persona Cases},year={2011},author={Shamal Faily and Shamal Faily},doi={null},pmid={null},pmcid={null},mag_id={2248208176},journal={null},abstract={Despite the large body of i* research, there has been com- paratively little work on how goal-modelling techniques can help identify usability concerns. Recent work has considered how goal models might better integrate with User-Centered Design. This paper takes an alterna- tive perspective by examining how work in User-Centered Design, specifi- cally Persona Cases, can be re-framed as goal models. We briefly describe an approach for doing  this,  and present  some preliminary results  from applying this approach using the Goal-oriented Requirements Language and existing tool support.}}
@ARTICLE{Agirre_2012,title={SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity},year={2012},author={Eneko Agirre and Eneko Agirre and Daniel Cer and Daniel Cer and Mona Diab and Mona Diab and Aitor González-Agirre and Aitor Gonzalez-Agirre},doi={null},pmid={null},pmcid={null},mag_id={2251861449},journal={null},abstract={Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation >80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.}}
@ARTICLE{Sartor_2014,title={Argumentation Schemes for Statutory Interpretation: A Logical Analysis},year={2014},author={Giovanni Sartor and Giovanni Sartor and Douglas Walton and Douglas Walton and Fabrizio Macagno and Fabrizio Macagno and Antonino Rotolo and Antonino Rotolo},doi={10.3233/978-1-61499-468-8-11},pmid={null},pmcid={null},mag_id={2254623187},journal={null},abstract={null}}
@ARTICLE{Hoffmann_2012,title={Towards the Use of Software Requirement Patterns for Legal Requirements},year={2012},author={Axel Hoffmann and Axel Hoffmann and Thomas Schulz and Thomas Schulz and Holger Hoffmann and Holger Hoffmann and Silke Jandt and Silke Jandt and Alexander Roßnagel and Alexander Roßnagel and Jan Marco Leimeister and Jan Marco Leimeister},doi={10.2139/ssrn.2484455},pmid={null},pmcid={null},mag_id={2258902651},journal={null},abstract={Laws and regulations play an increasingly important role for requirements engineering and system development. The challenge of interpreting the law to elicit legal requirements for a novel application calls for legal expertise. In this paper, we investigate if the effort of compiling a list of legal software requirements can be reduced by reusing recurring legal requirements. Therefore, we collected legal requirements that are stable concerning changes due to their origin in fundamental, higher-ranked laws, and derived software requirement patterns from them. This paper contributes by presenting those software requirement patterns consisting of the name, the goal and the pre-defined requirement template. We argue that under certain circumstances they can be used as a lightweight approach to specify legal requirements in system development projects and hence reduce the need for legal advice.}}
@ARTICLE{Hashmi_2015,title={Evaluating business process compliance management frameworks},year={2015},author={Mustafa Hashmi and Mustafa Hashmi},doi={null},pmid={null},pmcid={null},mag_id={2259286959},journal={null},abstract={This research contributes a formal framework to evaluate whether existing CMFs can model and reason about various types of normative requirements. The framework can be used to determine the level of coverage of concepts provided by CMFs, establish mappings between CMF languages and the semantics for the normative concepts and evaluate the suitability of a CMF for issuing a certification of compliance. The developed framework is independent of any specific formalism and it has been formally defined and validated through the examples of such mappings of CMFs.}}
@ARTICLE{Munro_2011,title={The mysteries of goal decomposition},year={2011},author={Scott Munro and Scott Munro and Sotirios Liaskos and Sotirios Liaskos and Jorge Aranda and Jorge Aranda},doi={null},pmid={null},pmcid={null},mag_id={2290071815},journal={null},abstract={Goal decomposition structures lie at the heart of goal model- ing languages such as i* . High-level goals of stakeholders are recursively decomposed into lower level ones and eventually into leaf level tasks to be performed by agents. The decomposition structure can also develop through a bottom up approach whereby higher-level goals are introduced as justifications for existing low-level ones. The very concept of decom- position, however, both as process and as artefact is rarely questioned in requirements engineering. In this paper, we argue that it may be of value to give a closer look into goal decomposition and clarify what we actually know about it and what is yet to be understood. We report on an on-going effort to identify empirical work on decomposition coming from various research fields, hoping to find such evidence. We then pose some research questions that we believe need to be pursued in order to improve our understanding of goal decomposition.}}
@ARTICLE{Liu_2015,title={Research on Domain-Oriented Latent Policy Lineage Mining Method},year={2015},author={Gang Liu and Gang Liu and Gang Liu and Gang Liu and Wray L. Buntine and Wray Buntine and Xiaoxiao Yang and Xiaoxiao Yang and Wei Fu and Weiping Fu},doi={10.1109/icicse.2015.27},pmid={null},pmcid={null},mag_id={2292999995},journal={null},abstract={On the foundation of policy research and semantic analysis of documents, we put forward a mining method of effective latent policy lineage relationship. We apply the factor space theory to policy research and propose a concept-factor decomposition method. With the combine concepts, we put forward the generic extraction method to mine latent genes. Finally, we conduct an experimental verification on the policy text sets of two legal policies. The test comparison demonstrates the method's feasibility and effectiveness.}}
@ARTICLE{Brewka_2013,title={Abstract dialectical frameworks revisited},year={2013},author={Gerhard Brewka and Gerhard Brewka and Stefan Ellmauthaler and Stefan Ellmauthaler and Hannes Straß and Hannes Strass and Johannes Wallner and Johannes Peter Wallner and Stefan Woltran and Stefan Woltran},doi={null},pmid={null},pmcid={null},mag_id={2293504531},journal={null},abstract={We present various new concepts and results related to abstract dialectical frameworks (ADFs), a powerful generalization of Dung's argumentation frameworks (AFs). In particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar ADFs can be improved and generalized to arbitrary frameworks. Furthermore, we introduce preference handling methods for ADFs, allowing for both reasoning with and about preferences. Finally, we present an implementation based on an encoding in answer set programming.}}
@ARTICLE{Amyot_2013,title={Improved GRL Modeling and Analysis with jUCMNav 5},year={2013},author={Daniel Amyot and Daniel Amyot and Rouzbahan Rashidi-Tabrizi and Rouzbahan Rashidi-Tabrizi and Gunter Mussbacher and Gunter Mussbacher and Jason Kealey and Jason Kealey and Etienne Tremblay and Etienne Tremblay and Jennifer Horkoff and Jennifer Horkoff},doi={null},pmid={null},pmcid={null},mag_id={2293852067},journal={null},abstract={jUCMNav is an open-source Eclipse tool for modeling and analyzing stakeholder goals, scenarios, and requirements with the User Requirements No- tation (URN) standard. This paper gives a brief overview of this tool, with an emphasis on recent improvements targeting URN's Goal-oriented Requirement Language (GRL) found in version 5.x.}}
@ARTICLE{Rissland_1984,title={Explaining and arguing with examples},year={1984},author={Edwina L. Rissland and Edwina L. Rissland and Eduardo M. Valcarce and Eduardo M. Valcarce and Kevin D. Ashley and Kevin D. Ashley},doi={null},pmid={null},pmcid={null},mag_id={2294125392},journal={null},abstract={In this paper, we discuss two tasks - on-line help and legal argument - that involve use of examples. In the case of on-line HELP, we discuss how to make it more intelligent by embedding custom-tailored examples in the explanations it gives its user. In the case of legal argumentation, we discuss how hypotheticals serve a central role in analyzing the strengths and weakness of a case and describe the generation of hypotheticals, stronger or weaker for one of the parties with respect to a doctrinal aspect, through modification of already existing cases or hypotheticals.}}
@ARTICLE{Parnas_1994,title={Software aging},year={1994},author={David Lorge Parnas and David Lorge Parnas},doi={null},pmid={null},pmcid={null},mag_id={2294305189},journal={null},abstract={Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limits its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable. A sign that the software engineering profession has matured will be that we lose our preoccupation with the first release and focus on the long-term health of our products. Researchers and practitioners must change their perception of the problems of software development. Only then will software engineering deserve to be called "engineering". >}}
@ARTICLE{Rao_2015,title={Automatic Identification of Conceptual Structures using Deep Boltzmann Machines},year={2015},author={Pattabhi R. K. Rao and Pattabhi R. K. Rao and Sobha Lalitha Devi and Sobha Lalitha Devi},doi={10.1145/2838706.2838711},pmid={null},pmcid={null},mag_id={2296223178},journal={null},abstract={This paper presents an approach to automatically extract Conceptual Graphs (CGs) from patent documents using Over-Replicated Softmax model of Deep Boltzman Machines (DBMs). The main challenge in the extraction of conceptual graphs from the natural language texts is the automatic identification of concepts and conceptual relations. The text analyzed in this work are patent documents, focused mainly on the claim's section (Claim) of the documents. The task of automatically identifying the concept and conceptual relation becomes difficult due to the complexities in the writing style of these documents as they are technical as well as legal. The analysis we have done shows that the general in-depth parsers available in the open domain fail to parse the 'claims section' sentences in patent documents. The failure of in-depth parsers led us, to develop a methodology to extract CGs using shallow parsed text. Thus in the present work we came up with a methodology which uses shallow parsed text in conjunction with DBMs, a deep learning technique for extracting CGs from sentences in the claim/novelty section of patent documents. The results obtained in our experiments are encouraging with a significant improvement over the state -of-art and are discussed in detail in this paper. We have obtained a precision of 79.34 % and a recall of 72.54%.}}
@ARTICLE{Grabmair_2010,title={Argumentation with Value JudgmentsAn Example of Hypothetical Reasoning},year={2010},author={Matthias Grabmair and Matthias Grabmair and Kevin D. Ashley and Kevin D. Ashley},doi={null},pmid={null},pmcid={null},mag_id={2296270591},journal={null},abstract={This paper presents a formalism modeling legal reasoning with fact patterns and their substantive effects on legal values. It centers on the concept of a value judgment, i.e. a determination that one factual situation is preferable over another by virtue of their respective effects on values. This allows the modeling of legal sources as sets of value judgments and legal methodologies as collections of argumentation schemes. The paper briefly derives the formalism from legal theory and elaborates on its use in the context of an example of hypothetical reasoning.}}
@ARTICLE{Stuckenschmidt_2016,title={Knowledge graph refinement: A survey of approaches and evaluation methods},year={2016},author={Heiner Stuckenschmidt and Heiko Paulheim and Heiko Paulheim},doi={10.3233/sw-160218},pmid={null},pmcid={null},mag_id={2300469216},journal={Sprachwissenschaft},abstract={In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term "Knowledge Graph" in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.}}
@ARTICLE{Hirst_1963,title={Sense and Sensibilia.},year={1963},author={R. J. Hirst and J. L. Austin and G. J. Warnock},doi={10.2307/2217192},pmid={null},pmcid={null},mag_id={2315358109},journal={The Philosophical Quarterly},abstract={null}}
@ARTICLE{Pham_2016,title={An Ontology-based Approach for Business Process Compliance Checking},year={2016},author={Tuan Anh Pham and Tuan Anh Pham and Nhân Lê Thành and Nhan Le Thanh},doi={10.1145/2857546.2857603},pmid={null},pmcid={null},mag_id={2317732477},journal={null},abstract={The early detection of flaws and errors has become a significant feature of a business process modeling tool. This paper proposes an ontology-based approach for business process compliance checking. The business processes and the business rules are represented in a machine understandable form, a reasoner is used to reason on this knowledge base for detecting the potential semantic error by using a set of predefined rules.}}
@ARTICLE{Antón_2012,title={Legal requirements metrics for compliance analysis},year={2012},author={Annie I. Antón and Ana I. Anton and Aaron K. Massey and Aaron K. Massey},doi={null},pmid={null},pmcid={null},mag_id={2327057663},journal={null},abstract={Laws and regulations safeguard citizens’ security and privacy. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) 1 governs the security and privacy of electronic health records (EHR) systems. The U.S. Department of Health and Human Services (HHS), which is charged with creating, maintaining, and enforcing regulations pursuant to HIPAA, has required systematic changes in institutional privacy practices as a result of nearly 15,000 resolved HIPAA investigations.2 HIPAA violations can result in serious monetary penalties. HHS recently fined one healthcare provider $4.3 million dollars for violations of the HIPAA Privacy Rule.3 Ensuring EHR systems are legally compliant is challenging for software engineers because the laws and regulations governing EHR systems are written by lawyers with little to no understanding of the underlying technology. 
This dissertation examines how software engineers can evaluate software requirements for compliance with laws and regulations. The main objective of this work is to help software engineers perform a legal compliance analysis for software intended to be deployed in domains goverened by law by developing empirically validated: (a) techniques for determining which requirements are legally implementation ready (LIR);4 (b) metrics to estimate which requirements are LIR automatically; and (c) a prototype tool that supports identifying LIR requirements using legal requirements metrics. 
My empirical studies suggest that the average graduate-level software engineer is illprepared to identify legally compliant software requirements with any confidence and that domain experts are an absolute necessity. When working together as a team graduate-level software engineers make extremely conservative legal implementation readiness decisions. Furthermore, we observe that the legal requirements metrics discussed in this dissertation can be used to improve legal implementation readiness decisions. These findings, along with legal and ethical concerns, make the study of legal compliance in software engineering a critical area for continued research.
1 Pub.L.No.104-191,110Stat.1936(1996). 2 http://www.hhs.gov/ocr/privacy/hipaa/enforcement/highlights/index.html. 3 http://www.hhs.gov/news/press/2011pres/02/20110222a.html. 4 Legally implementation ready requirements are requirements that have met or exceeded their obligations under relevant laws and regulations.}}
@ARTICLE{Bodenheimer_1991,title={Theorie der juristischen Argumentation},year={1991},author={Édgar Bodenheimer and Edgar Bodenheimer and Robert Alexy},doi={null},pmid={null},pmcid={null},mag_id={2335502651},journal={null},abstract={Journal Article Theorie Der Juristischen Argumentation Get access Theorie Der Juristischen Argumentation. by Alexy Robert. Frankfurt: Suhrkamp, 1983. Pp. 397. Edgar Bodenheimer Edgar Bodenheimer * Professor of Law Emeritus, University of California, Davis Search for other works by this author on: Oxford Academic Google Scholar The American Journal of Comparative Law, Volume 33, Issue 3, Summer 1985, Pages 541–543, https://doi.org/10.2307/840245 Published: 01 July 1985}}
@ARTICLE{Christidis_2016,title={Blockchains and Smart Contracts for the Internet of Things},year={2016},author={Konstantinos Christidis and Konstantinos Christidis and Michael Devetsikiotis and Michael Devetsikiotis},doi={10.1109/access.2016.2566339},pmid={null},pmcid={null},mag_id={2392113277},journal={IEEE Access},abstract={Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts—scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.}}
@ARTICLE{Wyner_2013,title={A Case Study on Legal Case Annotation.},year={2013},author={Adam Wyner and Adam Wyner and Wim Peters and Wim Peters and Daniel Katz and Daniel Martin Katz},doi={null},pmid={null},pmcid={null},mag_id={2394775532},journal={null},abstract={The paper reports the outcomes of a study with law school students to annotate a corpus of legal cases for a variety of annotation types, e.g. citation indices, legal facts, rationale, judgement, cause of action, and others. An online tool is used by a group of annotators that results in an annotated corpus. Differences amongst the annotations are curated, producing a gold standard corpus of annotated texts. The annotations can be extracted with semantic searches of complex queries. There would be many such uses for the development and analysis of such a corpus for both legal education and legal research.}}
@ARTICLE{Bex_2012,title={Dialogue templates for automatic argument processing},year={2012},author={Floris Bex and Floris Bex and Chris Reed and Chris Reed},doi={10.3233/978-1-61499-111-3-366},pmid={null},pmcid={null},mag_id={2395386371},journal={null},abstract={There is an extensive literature on dialogue systems, which attempts to capture aspects of structured communication with the aim of understanding, improving, and automatically recreating such communication. This paper discusses dialogue templates: blueprints that can be instantiated and combined to form argumentative dialogues. These templates provide a generic way of representing individual dialogue systems and allow us to generalise techniques for investigation, generation and recognition of dialogues.}}
@ARTICLE{Al-Abdulkarim_2014,title={Abstract Dialectical Frameworks for Legal Reasoning.},year={2014},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2395911589},journal={null},abstract={Dialectical Frameworks for Legal Reasoning Latifa AL-ABDULKARIM, Katie ATKINSON, Trevor BENCH-CAPON Department of Computer Science, The University of Liverpool, UK Abstract. In recent years a powerful generalisation of Dung’s abstract argumentation frameworks, Abstract Dialectical Frameworks (ADF), has been developed. ADFs generalise the abstract argumentation frameworks introduced by Dung by replacing Dung’s single acceptance condition (that all attackers be defeated) with acceptance conditions local to each particular node. Such local acceptance conditions allow structured argumentation to be straightforwardly incorporated. Related to ADFs are prioritised ADFs, which allow for reasons pro and con a node. In this paper we show how these structures provide an excellent framework for representing a leading approach to reasoning with legal cases. In recent years a powerful generalisation of Dung’s abstract argumentation frameworks, Abstract Dialectical Frameworks (ADF), has been developed. ADFs generalise the abstract argumentation frameworks introduced by Dung by replacing Dung’s single acceptance condition (that all attackers be defeated) with acceptance conditions local to each particular node. Such local acceptance conditions allow structured argumentation to be straightforwardly incorporated. Related to ADFs are prioritised ADFs, which allow for reasons pro and con a node. In this paper we show how these structures provide an excellent framework for representing a leading approach to reasoning with legal cases.}}
@ARTICLE{Mussbacher_2011,title={Eight Deadly Sins of GRL.},year={2011},author={Gunter Mussbacher and Gunter Mussbacher and Daniel Amyot and Daniel Amyot and Patrick Heymans and Patrick Heymans},doi={null},pmid={null},pmcid={null},mag_id={2396431338},journal={null},abstract={null}}
@ARTICLE{Prakken_2012,title={Formalising a legal opinion on a legislative proposal in the ASPIC + framework.},year={2012},author={Henry Prakken and Henry Prakken},doi={null},pmid={null},pmcid={null},mag_id={2396827330},journal={null},abstract={This paper presents a case study in which an opinion of a legal scholar on a legislative proposal is formally reconstructed in the ASPIC+ framework for argumentation-based inference. The reconstruction uses a version of the argument scheme for good and bad consequences that does not refer to single but to sets of consequences, in order to model aggregation of reasons for and against proposals. The case study is intended to contribute to a comparison between various formal frameworks for argumentation by providing a new benchmark example. It also aims to illustrate the usefulness of two features of ASPIC+: its distinction between deductive and defeasible inference rules and its ability to express arbitrary preference orderings on arguments.}}
@ARTICLE{Bex_2011,title={What Makes a Story Plausible? The Need for Precedents},year={2011},author={Floris Bex and Floris Bex and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Bart Verheij and Bart Verheij},doi={null},pmid={null},pmcid={null},mag_id={2398595547},journal={null},abstract={When reasoning about the facts of a case, we typically use stories to link the known events into coherent wholes. One way to establish coherence is to appeal to past examples, real or fictitious. These examples can be chosen and critiqued using the case-based reasoning (CBR) techniques from the AI and Law literature. In this paper, we apply these techniques to factual stories, assessing a story about the facts using precedents. We thus show how factual and legal reasoning can be combined in a CBR model.}}
@ARTICLE{Atkinson_2014,title={Taking the Long View: Looking Ahead in Practical Reasoning.},year={2014},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2399755922},journal={null},abstract={In this paper we extend an argumentation scheme for practical reasoning with values based on Action-based Alternating Transition Systems. While the original scheme considers only arguments arising from the immediately next state, our proposals will enable long term considerations to be taken into account. We consider the various reasons for and against performing an action that arise from these longer term considerations, and develop a new set of argumentation schemes for practical reasoning which allows a clearer separation between facts, values and preferences, and more precise targeting of attacks.}}
@ARTICLE{Wyner_2011,title={On rule extraction from regulations},year={2011},author={Adam Wyner and Adam Wyner and Wim Peters and Wim Peters},doi={null},pmid={null},pmcid={null},mag_id={2403804881},journal={null},abstract={Rules in regulations such as found in the US Federal Code of Regulations can be expressed using conditional and deontic rules. Identifying and extracting such rules from the language of the source material would be useful for automating rulebook management and translating into an executable logic. The paper presents a linguistically-oriented, rule-based approach, which is in contrast to a machine learning approach. It outlines use cases, discusses the source materials, reviews the methodology, then provides initial results and future steps.}}
@ARTICLE{Bench‐Capon_2015,title={Two Tools for Prototyping Legal CBR.},year={2015},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Thomas F. Gordon and Thomas F. Gordon},doi={null},pmid={null},pmcid={null},mag_id={2403872938},journal={null},abstract={null}}
@ARTICLE{Hulpuş_2015,title={Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation},year={2015},author={Ioana Hulpuş and Ioana Hulpus and Narumol Prangnawarat and Narumol Prangnawarat and Conor Hayes and Conor Hayes},doi={10.1007/978-3-319-25007-6_26},pmid={null},pmcid={null},mag_id={2407642977},journal={null},abstract={Semantic relatedness and disambiguation are fundamental problems for linking text documents to the Web of Data. There are many approaches dealing with both problems but most of them rely on word or concept distribution over Wikipedia. They are therefore not applicable to concepts that do not have a rich textual description. In this paper, we show that semantic relatedness can also be accurately computed by analysing only the graph structure of the knowledge base. In addition, we propose a joint approach to entity and word-sense disambiguation that makes use of graph-based relatedness. As opposed to the majority of state-of-the-art systems that target mainly named entities, we use our approach to disambiguate both entities and common nouns. In our experiments, we first validate our relatedness measure on multiple knowledge bases and ground truth datasets and show that it performs better than related state-of-the-art graph based measures. Afterwards, we evaluate the disambiguation algorithm and show that it also achieves superior disambiguation accuracy with respect to alternative state-of-the-art graph-based algorithms.}}
@ARTICLE{Batty_1999,title={Pass the buck.},year={1999},author={David Batty and David Batty},doi={10.7748/ns.13.45.12.s25},pmid={10633697},pmcid={null},mag_id={2419073521},journal={Nursing Standard},abstract={The confusion surrounding the recent Appeal Court ruling on long-term care centres on definitions of nursing care}}
@ARTICLE{Teufel_2002,title={Summarizing scientific articles: experiments with relevance and rhetorical status},year={2002},author={Simone Teufel and Simone Teufel and Marc Moens and Marc Moens},doi={10.1162/089120102762671936},pmid={null},pmcid={null},mag_id={2442495973},journal={Computational Linguistics},abstract={In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work.We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations.We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.}}
@ARTICLE{Roy_2007,title={Requirement engineering with URN: Integrating goals and scenarios},year={2007},author={Jean-François Roy and Jean-Francois Roy},doi={10.20381/ruor-18978},pmid={null},pmcid={null},mag_id={2460216982},journal={null},abstract={null}}
@ARTICLE{Herik_1999,title={Legal Knowledge Based Systems},year={1999},author={H.J. van den Herik and H.J. van den Herik and Marie‐Francine Moens and Marie-Francine Moens and Jon Bing and J. Bing and Béatrice Van Buggenhout and B. van Buggenhout and John Zeleznikow and John Zeleznikow and C. Gruetters and C. Gruetters},doi={null},pmid={null},pmcid={null},mag_id={2465228153},journal={null},abstract={null}}
@ARTICLE{Dark_2012,title={Data Breach Disclosure: A Policy Analysis},year={2012},author={Melissa Dark and Melissa Dark},doi={10.4018/978-1-61350-323-2.ch302},pmid={null},pmcid={null},mag_id={2490323307},journal={null},abstract={As information technology has become more ubiquitous and pervasive, assurance and security concerns have escalated; in response, we have seen noticeable growth in public policy aimed at bolstering cybertrust. With this growth in public policy, questions regarding the effectiveness of these policies arise. This chapter focuses on policy analysis of the state data breach disclosure laws recently enacted in the United States. The state data breach disclosure laws were chosen for policy analysis for three reasons: the rapid policy growth (the United States have enacted 45 state laws in 6 years); this is the first instantiation of informational regulation for information security; and the importance of these laws to identity theft and privacy. The chapter begins with a brief history in order to provide context. Then, this chapter examines the way in which historical, political and institutional factors have shaped our current data breach disclosure policies, focusing on discovering how patterns of interaction influenced the legislative outcomes we see today. Finally, this chapter considers: action that may result from these policies; the action type(s) being targeted; alternatives that are being considered, and; potential outcomes of the existing and proposed alternative policies.}}
@ARTICLE{Hsu_1996,title={All-pairwise comparisons},year={1996},author={Jason C. Hsu and Jason C. Hsu and Jason C. Hsu},doi={10.1007/978-1-4899-7180-7_5},pmid={null},pmcid={null},mag_id={2492031651},journal={null},abstract={null}}
@ARTICLE{Llewelyn_2014,title={Laboratory test results},year={2014},author={Huw Llewelyn and Huw Llewelyn and Hock Aun Ang and Hock Aun Ang and Keir Lewis and Keir Lewis and Anees Al-Abdullah and Anees Al-Abdullah},doi={10.1093/med/9780199679867.003.0011},pmid={null},pmcid={null},mag_id={2493209507},journal={null},abstract={null}}
@ARTICLE{Yao_2010,title={Human-Inspired Granular Computing},year={2010},author={Yiyu Yao and Yiyu Yao},doi={10.4018/978-1-60566-324-1.ch001},pmid={null},pmcid={null},mag_id={2495817959},journal={null},abstract={In this chapter, I explore a view of granular computing as a paradigm of human-inspired problem solving and information processing, covering human-oriented studies and machine-oriented studies. By exploring the notion of multiple levels of granularity, one can identify, examine and formalize a special family of principles, strategies, heuristics, and methods that are commonly used by humans in daily problem solving. The results may then be used for human and machine problem solving, as well as for implementing human-inspired approaches in machines and systems. The triarchic theory of granular computing unifies both types of studies from three perspectives, namely, a philosophy of structured thinking, a methodology of structured problem solving, and a computation paradigm of structured information processing. The stress on multilevel, hierarchical structures makes granular computing a human-inspired and structured approach to problem solving.}}
@ARTICLE{Perelman_1980,title={Justice, law, and argument},year={1980},author={Chaïm Perelman and Chaïm Perelman and Ch. Perelman and Harold J. Berman and Harold J. Berman},doi={null},pmid={null},pmcid={null},mag_id={2496775934},journal={null},abstract={null}}
@ARTICLE{Prakken_2010,title={Legal Knowledge and Information Systems. JURIX 2010: The Twenty -Third Annual Conference},year={2010},author={Henry Prakken and Hendrik Prakken and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2501410995},journal={null},abstract={null}}
@ARTICLE{Mont_2005,title={Privacy Policy Enforcement in Enterprises: Addressing Regulatory Compliance and Governance Needs},year={2005},author={Marco Casassa Mont and Marco Casassa Mont and Robert Thyne and Robert Thyne and Pete Bramhall and Pete Bramhall and Kwok-Nga Chan and Kwok-Nga Chan},doi={10.1007/978-3-322-85237-3_14},pmid={null},pmcid={null},mag_id={2506501777},journal={null},abstract={null}}
@ARTICLE{Atkinson_2013,title={From Knowledge Representation to Argumentation in AI, Law and Policy Making. a Festscrift in Honour of Trevor Bench-Capon on the Occasion of His 60th},year={2013},author={Katie Atkinson and Katie Atkinson and Henry Prakken and Henry Prakken and Adam Wyner and Adam Wyner},doi={null},pmid={null},pmcid={null},mag_id={2510679183},journal={null},abstract={Trevor Bench-Capon is well recognised as an outstanding figure in Artificial Intelligence and Law, having published extensively over the long course of his career on legal knowledge representation, engineering methods for knowledge-based systems, theoretical and applied argumentation, case-based reasoning, policy-making, reasoning about evidence, and many other related topics. He has deeply influenced many of his colleagues, particularly with his earlier work on principled methods for legal knowledge representation and the engineering of knowledge-based systems, and later with the introduction of values in the study of argumentation. This Festschrift is in honour of Bench-Capon's work and its seminal influence. The articles contained here by his colleagues extensively review, comment on, and extend Bench-Capon's work. As a whole, the volume is a substantive introduction to the main topics and issues to which Bench-Capon has contributed so much.}}
@ARTICLE{Breaux_2007,title={Managing ambiguity and traceability in regulatory requirements: A tool-supported frame-based approach},year={2007},author={Travis D. Breaux and Travis D. Breaux and Annie I. Antón and Ana I. Anton},doi={null},pmid={null},pmcid={null},mag_id={2514850406},journal={null},abstract={null}}
@ARTICLE{Walton_2017,title={Arguments from authority and expert opinion in computational argumentation systems},year={2017},author={Douglas Walton and Douglas Walton and Marcin Koszowy and Marcin Koszowy},doi={10.1007/s00146-016-0666-3},pmid={null},pmcid={null},mag_id={2516237363},journal={Ai & Society},abstract={In this paper we show that an essential aspect of solving the problem of uncritical acceptance of expert opinions that is at the root of the ad verecundiam fallacy is the need to disentangle argument from expert opinion from another kind of appeal to authority. Formal and computational argumentation systems enable us to analyze the fault in which an error has occurred by virtue of a failure to meet one or more of the requirements of the argumentation scheme from argument from expert opinion. We present a method for enhancing this capability by showing how arguments from expert opinion are related to, but different from, arguments from deontic authority.}}
@ARTICLE{Schmidt_2012,title={Specifying Requirements Using Commitment, Privilege, and Right (CPR) Analysis},year={2012},author={Jéssica Schmidt and Jessica Young Schmidt and Jessica Young Schmidt},doi={null},pmid={null},pmcid={null},mag_id={2521492466},journal={null},abstract={null}}
@ARTICLE{Yan_2018,title={A retrospective of knowledge graphs},year={2018},author={Jihong Yan and Jihong Yan and Chengyu Wang and Chengyu Wang and Wenliang Cheng and Wenliang Cheng and Ming Gao and Ming Gao and Aoying Zhou and Aoying Zhou},doi={10.1007/s11704-016-5228-9},pmid={null},pmcid={null},mag_id={2524949478},journal={Frontiers of Computer Science in China},abstract={Information on the Internet is fragmented and presented in different data sources, which makes automatic knowledge harvesting and understanding formidable for machines, and even for humans. Knowledge graphs have become prevalent in both of industry and academic circles these years, to be one of the most efficient and effective knowledge integration approaches. Techniques for knowledge graph construction can mine information from either structured, semi-structured, or even unstructured data sources, and finally integrate the information into knowledge, represented in a graph. Furthermore, knowledge graph is able to organize information in an easy-to-maintain, easy-to-understand and easy-to-use manner.

In this paper, we give a summarization of techniques for constructing knowledge graphs. We review the existing knowledge graph systems developed by both academia and industry. We discuss in detail about the process of building knowledge graphs, and survey state-of-the-art techniques for automatic knowledge graph checking and expansion via logical inferring and reasoning. We also review the issues of graph data management by introducing the knowledge data models and graph databases, especially from a NoSQL point of view. Finally, we overview current knowledge graph systems and discuss the future research directions.}}
@ARTICLE{Loui_2016,title={From Berman and Hafner's teleological context to Baude and Sachs' interpretive defaults: an ontological challenge for the next decades of AI and Law},year={2016},author={Ronald P. Loui and Ronald P. Loui},doi={10.1007/s10506-016-9186-1},pmid={null},pmcid={null},mag_id={2530091854},journal={Artificial Intelligence and Law},abstract={This paper revisits the challenge of Berman and Hafner's "missing link" paper on representing teleological structure in case-based legal reasoning. It is noted that this was mainly an ontological challenge to represent some of what made legal reasoning distinctive, which was given less attention than factual similarity in the dominant AI and Law paradigm, deriving from HYPO. The response to their paper is noted and briefly evaluated. A parallel is drawn to a new challenge to provide deep structure to the legal context of textual meaning, drawing on the forthcoming work of two Constitutional law scholars who appear to place some faith in the ways of thinking that AI and Law has developed.}}
@ARTICLE{Αλέτρας_2016,title={Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective},year={2016},author={Νικόλαος Αλέτρας and Nikolaos Aletras and Dimitrios Tsarapatsanis and Dimitrios Tsarapatsanis and Daniel Preoţiuc-Pietro and Daniel Preoţiuc-Pietro and Vasileios Lampos and Vasileios Lampos},doi={10.7717/peerj-cs.93},pmid={null},pmcid={null},mag_id={2536769020},journal={PeerJ},abstract={Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court’s decisions with a strong accuracy (79% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.}}
@ARTICLE{Bex_2006,title={Legal Knowledge and Information Systems. JURIX 2006},year={2006},author={Floris Bex and Henry Prakken and Hendrik Prakken and Bart Verheij},doi={null},pmid={null},pmcid={null},mag_id={2537284123},journal={null},abstract={null}}
@ARTICLE{French_2011,title={Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI-11)},year={2011},author={Tim French and Wiebe van der Hoek and Petar Iliev and Barteld Kooi},doi={null},pmid={null},pmcid={null},mag_id={2539586701},journal={null},abstract={null}}
@ARTICLE{Governatori_2016,title={Semantic Business Process Regulatory Compliance Checking Using LegalRuleML},year={2016},author={Guido Governatori and Guido Governatori and Mustafa Hashmi and Mustafa Hashmi and Ho-Pun Lam and Ho-Pun Lam and Serena Villata and Serena Villata and Monica Palmirani and Monica Palmirani},doi={10.1007/978-3-319-49004-5_48},pmid={null},pmcid={null},mag_id={2547017614},journal={null},abstract={Legal documents are the source of norms, guidelines, and rules that often feed into different applications. In this perspective, to foster the need of development and deployment of different applications, it is important to have a sufficiently expressive conceptual framework such that various heterogeneous aspects of norms can be modeled and reasoned with. In this paper, we investigate how to exploit Semantic Web technologies and languages, such as LegalRuleML, to model a legal document. We show how the semantic annotations can be used to empower a business process regulatory compliance system and discuss the challenges of adapting a semantic approach to legal domain.}}
@ARTICLE{Brown_2013,title={Privacy and Data Protection},year={2013},author={Ian Brown and Ian Brown and Christopher T. Marsden and Christopher T. Marsden},doi={null},pmid={null},pmcid={null},mag_id={2548203767},journal={null},abstract={This chapter contains sections titled: Public Policy Objectives, Types of Code Regulation, Institutional Political Economy, Outcomes}}
@ARTICLE{Verheij_2016,title={Formalizing value-guided argumentation for ethical systems design},year={2016},author={Bart Verheij and Bart Verheij},doi={10.1007/s10506-016-9189-y},pmid={null},pmcid={null},mag_id={2550619487},journal={Artificial Intelligence and Law},abstract={The persuasiveness of an argument depends on the values promoted and demoted by the position defended. This idea, inspired by Perelman's work on argumentation, has become a prominent theme in artificial intelligence research on argumentation since the work by Hafner and Berman on teleological reasoning in the law, and was further developed by Bench-Capon in his value-based argumentation frameworks. One theme in the study of value-guided argumentation is the comparison of values. Formal models involving value comparison typically use either qualitative or quantitative primitives. In this paper, techniques connecting qualitative and quantitative primitives recently developed for evidential argumentation are applied to value-guided argumentation. By developing the theoretical understanding of intelligent systems guided by embedded values, the paper is a step towards ethical systems design, much needed in these days of ever more pervasive AI techniques.}}
@ARTICLE{Robinson_2015,title={Graph Databases: New Opportunities for Connected Data},year={2015},author={Ian Robinson and Ian Scott Robinson and Jim Webber and Jim Webber and Emil Eifrem and Emil Eifrem},doi={null},pmid={null},pmcid={null},mag_id={2556970231},journal={null},abstract={Discover how graph databases can help you manage and query highly connected data. With this practical book, youll learn how to design and implement a graph database that brings the power of graphs to bear on a broad range of problem domains. Whether you want to speed up your response to user queries or build a database that can adapt as your business evolves, this book shows you how to apply the schema-free graph model to real-world problems.This second edition includes new code samples and diagrams, using the latest Neo4j syntax, as well as information on new functionality. Learn how different organizations are using graph databases to outperform their competitors. With this books data modeling, query, and code examples, youll quickly be able to implement your own solution.Model data with the Cypher query language and property graph modelLearn best practices and common pitfalls when modeling with graphsPlan and implement a graph database solution in test-driven fashionExplore real-world examples to learn how and why organizations use a graph databaseUnderstand common patterns and components of graph database architectureUse analytical techniques and algorithms to mine graph database information}}
@ARTICLE{Teerapong_2014,title={Graphical ways of researching},year={2014},author={K Teerapong},doi={null},pmid={null},pmcid={null},mag_id={2557246539},journal={null},abstract={This paper investigates the ways in which graphic design can be understood as a process of inquiry. Although the use of visuals has been widely accepted in research in various fields, researchers in sciences and humanities tend to position graphic design as a communication tool rather than a research strategy. Thus, the main purpose of adopting graphic works is often for visual presentations. As an emerging discipline with a lack of theoretical engagement (Noble & Bestley 2011), graphic design is overshadowed and its functions and implications have not been fully recognized. This essay aims to explicate knowledge from the graphic design process and understand the term 'graphic design' as a research strategy. In so doing, a clear distinction between two research areas which use visuals in their methodologies needs to be identified. The first is graphic design research which directly links to visuals and graphic design practices. This can refer to a practice-led paradigm so that the designer researchers can discover knowledge through their practices. The second is the research that uses visual data and/or visual presentations. This type of research can vary from the sciences to humanities. In order to reposition graphic design in research methodology, it is important to clarify the difference between these two categories. This would offer potential for graphic design to be accepted as a discipline that has its design methodology.}}
@ARTICLE{Horkoff_2016,title={Goal-Oriented Requirements Engineering: A Systematic Literature Map},year={2016},author={Jennifer Horkoff and Jennifer Horkoff and Fatma Başak Aydemir and Fatma Basak Aydemir and Evellin Cardoso and Evellin Cardoso and Tong Li and Tong Li and Alejandro Maté and Alejandro Maté and Elda Paja and Elda Paja and Mattia Salnitri and Mattia Salnitri and John Mylopoulos and John Mylopoulos and Paolo Giorgini and Paolo Giorgini},doi={10.1109/re.2016.41},pmid={null},pmcid={null},mag_id={2560134888},journal={null},abstract={Over the last two decades, much attention hasbeen paid to the area of Goal-Oriented Requirements Engineering(GORE), where goals are used as a useful conceptualizationto elicit, model and analyze requirements, capturingalternatives and conflicts. Goal modeling has been adaptedand applied to many sub-topics within RE and beyond, suchas agent-orientation, aspect-orientation, business intelligence, model-driven development, security, and so on. Despite extensiveefforts in this field, the RE community lacks a recent, generalsystematic literature review of the area. As a first step towardsproviding a GORE overview, we present a Systematic LiteratureMap, focusing on GORE-related publications at a high-level, categorizing and analyzing paper information in order to answerseveral research questions, while omitting a detailed analysis ofindividual paper quality. Our Literature Map covers the 246 topcitedGORE-related conference and journal papers, accordingto Scopus, classifying them into a number of descriptive papertypes and topics, providing an analysis of the data, which ismade publicly available. We use our analysis results to makerecommendations concerning future GORE research.}}
@ARTICLE{Bhatia_2016,title={A Theory of Vagueness and Privacy Risk Perception},year={2016},author={Jaspreet Bhatia and Jaspreet Bhatia and Travis D. Breaux and Travis D. Breaux and Joël R. Reidenberg and Joel R. Reidenberg and Thomas B. Norton and Thomas B. Norton},doi={10.1109/re.2016.20},pmid={null},pmcid={null},mag_id={2560661849},journal={null},abstract={Ambiguity arises in requirements when astatement is unintentionally or otherwise incomplete, missing information, or when a word or phrase has morethan one possible meaning. For web-based and mobileinformation systems, ambiguity, and vagueness inparticular, undermines the ability of organizations to aligntheir privacy policies with their data practices, which canconfuse or mislead users thus leading to an increase inprivacy risk. In this paper, we introduce a theory ofvagueness for privacy policy statements based on ataxonomy of vague terms derived from an empiricalcontent analysis of 15 privacy policies. The taxonomy wasevaluated in a paired comparison experiment and resultswere analyzed using the Bradley-Terry model to yield arank order of vague terms in both isolation andcomposition. The theory predicts how vague modifiers toinformation actions and information types can becomposed to increase or decrease overall vagueness. Wefurther provide empirical evidence based on factorialvignette surveys to show how increases in vagueness willdecrease users' acceptance of privacy risk and thusdecrease users' willingness to share personal information.}}
@ARTICLE{Gordon_2016,title={Formalizing Balancing Arguments},year={2016},author={Thomas F. Gordon and Thomas F. Gordon and Thomas F. Gordon and Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={2566279597},journal={null},abstract={This paper presents a formal model of structured argument that can handle balancing arguments, arguments were pros and cons are balanced to choose among alternative options. The model can handle cumulative arguments, arguments that provide bits of evidence that can support or undermine hypothesis as new evidence comes in. The model has been fully implemented in version 4 of the Carneades Argumentation System.}}
@ARTICLE{Yu_2017,title={An Ontology-based Approach for Knowledge Integration in Product Collaborative Development},year={2017},author={Guodong Yu and Guodong Yu and Yang Yu and Yu Yang and Yu Yang and Yu Yang and Qingsong Xing and Xing Qing-song and Xing Qingsong},doi={10.1515/jisys-2015-0046},pmid={null},pmcid={null},mag_id={2567376917},journal={Journal of intelligent systems},abstract={Abstract Knowledge integrated model (KIM) is regarded as an effective and critical way to enhance efficiency of product collaborative development (PCD). However, it is much knotty to establish a unified description rule as there exists diverse and heterogeneous designer knowledge. In this paper, an approach based on ontology for KIM is proposed to straightforwardly descript heterogeneous knowledge and achieve a matching of knowledge and design mission objectives easily. First, a KIM of PCD is presented in accordance with the characteristics of product design activities. Hereafter, a knowledge transformation method based on “problem domain – functional domain” model is developed to accurately reflect customer expectations for new products, wherein, knowledge ontology tree including “customer creativity, emotional knowledge, customer needs, and basic knowledge” is constructed to standardize customer needs. Then, a knowledge matching function (KMF) and a Backus–Naur form (BNF) are introduced to measure similarity of request and candidate. Thereafter, a service engine (SE) including KMF and BNF is generated by the knowledge integrated system (KIS). Finally, simulations and experiments illustrate the effectiveness of the proposed methods and models.}}
@ARTICLE{Clancey_1981,title={The epistemology of a rule-based expert system: a framework for explanation},year={1981},author={William J. Clancey and William J. Clancey},doi={10.1016/0004-3702(83)90008-5},pmid={null},pmcid={null},mag_id={2570774740},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Al-Abdulkarim_2016,title={Statement types in legal argument.},year={2016},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={null},pmid={null},pmcid={null},mag_id={2573650477},journal={null},abstract={© 2016 The authors and IOS Press. All rights reserved. In this paper we present an overview of the process of argumentation with legal cases, from evidence to verdict. We identify the various different types of statement involved in the various stages, and describe how the various types relate to one another. In particular we show how we can obtain the legally accepted facts which form the basis for consideration of the law governing the cases from facts about the world. We also explain how we can determine which particular facts are relevant. In so doing we bring together several important pieces of AI and Law research and clarify their relationships.}}
@ARTICLE{Fenves_1995,title={Computer representations of design standards and building codes: a U.S. perspective},year={1995},author={Steven J. Fenves and J.H. Garrett and James H. Garrett and H. Kiliccote and Han Kiliccote and Kincho H. Law and Kent A. Reed and K.A. Reed},doi={null},pmid={null},pmcid={null},mag_id={2575168511},journal={null},abstract={Standards representation and processing in the United States has had a long and interesting history of                       development. The work in the past has focused primarily on representing a standard, evaluating the intrinsic                       properties of that represented standard, and evaluating designs for conformance to that standard. To date, for a                       variety of reasons, standards writing organizations and computer-aided design software vendors have not                       adopted much of the results of this research. The failure of the approach so far in the U.S. can be traced to two                       distinct areas. One major cluster of causes is methodological: the initial concepts were not backed up by usable,                       persistent computer tools; and the initial application and model were not representative. The second cluster of                       causes of failure is professional, and has a lot to do with the dynamics of interaction of individuals and                       organizations. Future research must address the inadequacies of the current representations and create models                       that are able to represent all, or almost all, of the different types of provisions in any given standard; investigate                       and deliver a much richer set of processing functionality's, such as more support for use of design standards in                       earlier phases of design; support the treatment of multiple, heterogeneous standards available from distributed                       sources; and determine what type of support is needed to go from the textual versions of design standards to the                       formal models that can support sophisticated computation.}}
@ARTICLE{Boer_2002,title={METAlex: Legislation in XML},year={2002},author={Alexander Boer and Alexander Boer and Rinke Hoekstra and Rinke Hoekstra and Radboud Winkels and Radboud Winkels and Tom van Engers and T.M. van Engers},doi={null},pmid={null},pmcid={null},mag_id={2575540007},journal={null},abstract={null}}
@ARTICLE{Bex_2004,title={Reinterpreting arguments in dialogue : an application to evidential reasoning},year={2004},author={Floris Bex and Floris Bex and Henry Prakken and Henry Prakken},doi={null},pmid={null},pmcid={null},mag_id={2587822774},journal={null},abstract={This paper presents a formalisation of two typical legal dialogue moves in a formal dialogue game for argumentation. The moves concern two ways of reinterpret- ing a general rule used in an argument, viz. by 'unpacking' and 'refining' the rule. The moves can be made not only by the user but also by the attacker of the rule, in order to reveal new ways to attack it. The new dialogue game is illustrated with examples from legal evidential reasoning, in which these types of moves are particularly common.}}
@ARTICLE{Bench‐Capon_2017,title={Norms and value based reasoning: justifying compliance and violation},year={2017},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Sanjay Modgil and Sanjay Modgil},doi={10.1007/s10506-017-9194-9},pmid={null},pmcid={null},mag_id={2594622565},journal={Artificial Intelligence and Law},abstract={There is an increasing need for norms to be embedded in technology as the widespread deployment of applications such as autonomous driving, warfare and big data analysis for crime fighting and counter-terrorism becomes ever closer. Current approaches to norms in multi-agent systems tend either to simply make prohibited actions unavailable, or to provide a set of rules (principles) which the agent is obliged to follow, either as part of its design or to avoid sanctions and punishments. In this paper we argue for the position that agents should be equipped with the ability to reason about a system's norms, by reasoning about the social and moral values that norms are designed to serve; that is, perform the sort of moral reasoning we expect of humans. In particular we highlight the need for such reasoning when circumstances are such that the rules should arguably be broken, so that the reasoning can guide agents in deciding whether to comply with the norms and, if violation is desirable, how best to violate them. One approach to enabling this is to make use of an argumentation scheme based on values and designed for practical reasoning: arguments for and against actions are generated using this scheme and agents choose between actions based on their preferences over these values. Moral reasoning then requires that agents have an acceptable set of values and an acceptable ordering on their values. We first discuss how this approach can be used to think about and justify norms in general, and then discuss how this reasoning can be used to think about when norms should be violated, and the form this violation should take. We illustrate how value based reasoning can be used to decide when and how to violate a norm using a road traffic example. We also briefly consider what makes an ordering on values acceptable, and how such an ordering might be determined.}}
@ARTICLE{Verheij_2017,title={Proof with and without probabilities},year={2017},author={Bart Verheij and Bart Verheij},doi={10.1007/s10506-017-9199-4},pmid={null},pmcid={null},mag_id={2594682492},journal={Artificial Intelligence and Law},abstract={Evidential reasoning is hard, and errors can lead to miscarriages of justice with serious consequences. Analytic methods for the correct handling of evidence come in different styles, typically focusing on one of three tools: arguments, scenarios or probabilities. Recent research used Bayesian networks for connecting arguments, scenarios, and probabilities. Well-known issues with Bayesian networks were encountered: More numbers are needed than are available, and there is a risk of misinterpretation of the graph underlying the Bayesian network, for instance as a causal model. The formalism presented here models presumptive arguments about coherent hypotheses that are compared in terms of their strength. No choice is needed between qualitative or quantitative analytic styles, since the formalism can be interpreted with and without numbers. The formalism is applied to key concepts in argumentative, scenario and probabilistic analyses of evidential reasoning, and is illustrated with a fictional crime investigation example based on Alfred Hitchcock's film `To Catch A Thief'.}}
@ARTICLE{Apt_1992,title={Logic Programming},year={1992},author={Krzysztof R. Apt and Krzysztof R. Apt},doi={null},pmid={null},pmcid={null},mag_id={2612793397},journal={null},abstract={null}}
@ARTICLE{Tennyson_1984,title={Cognitive-based design guidelines for using video and computer technology in course development},year={1984},author={Robert D. Tennyson and Robert D. Tennyson and Klaus Breuer and Klaus Breuer},doi={null},pmid={null},pmcid={null},mag_id={2614042475},journal={null},abstract={null}}
@ARTICLE{Oconitrillo_2016,title={Business intelligence model to support a judge's decision making about legal situations},year={2016},author={Luis Raúl Rodríguez Oconitrillo and Luis Raúl Rodríguez Oconitrillo and Luis Raul Rodriguez Oconitrillo and Álvaro de la Ossa Osegueda and Alvaro de la Ossa Osegueda},doi={10.1109/concapan.2016.7942362},pmid={null},pmcid={null},mag_id={2622608931},journal={null},abstract={Legal decision-making support is key for legal security in any legal system. This paper describes a new scope and method for Business Intelligence applied in the legal domain. We describe a legal case processing model based on the transformation of relevant data from a legal case file by a judge, to produce what we call the judge's perception of the case. That is a representation of the case that stresses (1) the relevance of the different attributes of the case, and (2) the relationships among attributes that the judge considers relevant for argumentation purposes.}}
@ARTICLE{Carlo_2012,title={Prefazione a: R. Alexy, "Teoria dei diritti fondamentali", traduzione ed edizione italiana a cura di Leonardo Di Carlo; edizione originale: R. Alexy, "Theorie der Grundrechte", Suhrkamp, Frankfurt am Main 1994},year={2012},author={L. Di Carlo},doi={null},pmid={null},pmcid={null},mag_id={2625731548},journal={null},abstract={null}}
@ARTICLE{Abdulkarim_2017,title={Representation of case law for argumentative reasoning},year={2017},author={LM Al Abdulkarim and LM Al Abdulkarim and LM Al-Abdulkarim and Latifa Al-Abdulkarim},doi={null},pmid={null},pmcid={null},mag_id={2663996162},journal={null},abstract={Modelling argumentation based on legal cases has been a central topic of AI and Law since its very beginnings. The current established view is that facts must be determined on the basis of evidence. Next, these facts must be used to ascribe legally significant predicates (factors and issues) to the case, on the basis of which the outcome can be established. This thesis aims to provide a method to encapsulate the knowledge of bodies of case law from various legal domains using a recent development in AI knowledge representation, Abstract Dialectical Frameworks (ADFs), as the central feature of the design method. Three legal domains in the US Courts are used throughout the thesis: The domain of the Automobile Exception to the Fourth Amendment, which has been freshly analysed in terms of factors in this thesis; the US Trade Secrets domain analysed from well-known legal case-based reasoning systems (CATO and IBP); and the Wild Animals domain analysed extensively in AI and Law. In this work, ADFs play a role akin to that of Entity-Relationship models in the design of database systems to design and implement programs intended to decide cases, described as sets of factors, according to a theory of a particular domain based on a set of precedent cases relating to that domain. The ADFs in this thesis are instantiated from different starting points: factor-based representation of oral dialogues and factor-based analysis of legal opinions. A legal dialogue representation model is defined for the US Supreme Court Oral Hearing dialogues. The role of these hearings is to identify the components that can form the basis of an argument that will resolve the case. Dialogue moves used by participants have been identified as the dialogue proceeds to assert and modify argument components in term of issues, factors and facts, and to produce what are called Argument Component Trees (ACTs) for each participant in the dialogue, showing how these components relate to one another. The resulting trees can be then merged and used as input to decide the accepted components using an ADF. The model is illustrated using two landmark case studies in the Automobile Exception domain: Carney v. California and US v. Chadwick. A legal justification model is defined to capture knowledge in a legal domain and to provide justification and transparency of legal decisions. First, a legal domain ADF is instantiated from the factor hierarchy of CATO and IBP, then the method is applied to the other two legal domains. In each domain, the cases are expressed in terms of factors organised into an ADF, from which an executable program can be implemented in a straightforward way by taking advantage of the closeness of the acceptance conditions of the ADF to components of an executable program. The proposed method is evaluated to test the ease of implementation, the efficacy of the resulting program, the ease of refinement, transparency of the reasoning and transferability across legal domains. This evaluation suggests ways of improving the decision by incorporating the case facts, and considering justification and reasoning using portions of precedents. The final result is ANGELIC (ADF for kNowledGe Encapsulation of Legal Information from Cases), a method for producing programs that decide the cases with a high degree of accuracy in multiple domains.}}
@ARTICLE{Verheij_2008,title={About the logical relations between cases and rules},year={2008},author={Bart Verheij and Bart Verheij},doi={10.3233/978-1-58603-952-3-21},pmid={null},pmcid={null},mag_id={2678900356},journal={null},abstract={The two main types of law are legislation and precedents. Both types have a corresponding reasoning pattern determining legal consequences: legislation can be applied and precedents followed. The separate modelling of these two reasoning patterns using logical techniques has recently seen considerable progress. About the logical links between the two less is known, although progress has already been made. This document focuses on such logical relations. The main question is: to what extent can the application of legislation and precedent adherence be considered as two sides of the same logical coin? Findings from the boundaries of logic and law will serve as a starting point. This text is a translated, adapted and extended version of Verheij 2007.}}
@ARTICLE{Neill_2017,title={Classifying sentential modality in legal language: a use case in financial regulations, acts and directives},year={2017},author={James O’ Neill and James O' Neill and Paul Buitelaar and Paul Buitelaar and Cécile Robin and Cecile Robin and Leona O' Brien and Leona O' Brien},doi={10.1145/3086512.3086528},pmid={null},pmcid={null},mag_id={2757079888},journal={null},abstract={Texts expressed in legal language are often difficult and time consuming for lawyers to read through, particularly for the purpose of identifying relevant deontic modalities (obligations, prohibitions and permissions). By nature, the language of law is strict, hence the predominant use of modal logic as a substitute for the syntactical ambiguity in natural language, specifically, deontic and alethic logic for the respective modalities. However, deontic modalities which express obligations, prohibitions and permissions, can have varying degree and preciseness to which they correspond to a matter, strict deontic logic does not allow for such quantitative measures. Therefore, this paper outlines a data-driven approach by classifying deontic modalities using ensembled Artificial Neural Networks (ANN) that incorporate domain specific legal distributional semantic model (DSM) representations, in combination with, a general DSM representation. We propose to use well calibrated probability estimates from these classifiers as an approximation to the degree which an obligation/prohibition or permission belongs to a given class based on SME annotated sentences. Best results show 82.33 % accuracy on a held-out test set.}}
@ARTICLE{Atkinson_2018,title={Taking account of the actions of others in value-based reasoning},year={2018},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1016/j.artint.2017.09.002},pmid={null},pmcid={null},mag_id={2760144600},journal={Artificial Intelligence},abstract={null}}
@ARTICLE{Griffo_2017,title={From an Ontology of Service Contracts to Contract Modeling in Enterprise Architecture},year={2017},author={Cristine Griffo and Cristine Griffo and João Paulo A. Almeida and João Paulo A. Almeida and Giancarlo Guizzardi and Giancarlo Guizzardi and Julio Cesar Nardi and Julio Cesar Nardi},doi={10.1109/edoc.2017.15},pmid={null},pmcid={null},mag_id={2766097821},journal={null},abstract={Service contracts bind parties legally, regulating their behavior in the scope of a (business) service relationship. Given that there are legal consequences attached to service contracts, understanding the elements of a contract is key to managing services in an enterprise. After all, provisions in a service contract establish obligations and rights for service providers and customers that must be respected in service delivery. The importance of service contracts to service provisioning in an enterprise has motivated us to investigate their representation in enterprise models. We have observed that approaches fall into two extremes of a spectrum. Some approaches, such as ArchiMate, offer an opaque "contract" construct, not revealing the rights and obligations in the scope of the governed service relationship. Other approaches, under the umbrella term "contract languages", are devoted exactly to the formal representation of the contents of contracts. Despite the applications of contract languages, they operate at a level of detail that does not match that of enterprise architecture models. In this paper, we explore the gap between these two extremes. We address the representation of service contract elements with a systematic approach: we first propose a well-founded service contract ontology, and then extend the ArchiMate language to reflect the elements of the service contract ontology. The applicability of the proposed extension is assessed in the representation of a real-world cloud service contract.}}
@ARTICLE{Rigoni_2018,title={Representing dimensions within the reason model of precedent},year={2018},author={Adam Rigoni and Adam Rigoni},doi={10.1007/s10506-017-9216-7},pmid={null},pmcid={null},mag_id={2766777273},journal={Artificial Intelligence and Law},abstract={This paper gives an account of dimensions in the reason model found in Horty (Legal Theory 17(1): 1–33, 2011), Horty and Bench-Capon (in: Proceedings of the 15th international conference on artificial intelligence and law, pp 109–118, ACM Press, 2012) and Rigoni (Artif Intell Law 23(2):133–160, 2015. doi: 10.1007/s10506-015-9166-x). The account is constructed with the purpose of rectifying problems with the approach to incorporating dimensions in Horty (2017), namely, the problems arising from the collapse of the distinction between the reason model and the result model on that approach. Examination of the newly constructed theory revealed that the importance of dimensions in the reason model lies in their ability to establish weighings between reasons/factors of the same polarity. This permits past cases to constrain future cases in ways they could not on the reason model with just factors. The paper then discusses how dimensions might be established from a case base and how dimensions that are incomplete in various aspects might be dealt with in the theory. It closes with comparisons to other work in the literature on AI and Law and suggestions for further improvement of the reason model.}}
@ARTICLE{Goodhart_1930,title={DETERMINING THE RATIO DECIDENMI OF A CASE},year={1930},author={Arthur L. Goodhart and Arthur L. Goodhart},doi={10.2307/790205},pmid={null},pmcid={null},mag_id={2773639084},journal={Yale Law Journal},abstract={null}}
@ARTICLE{Conrad_2017,title={Scenario analytics: analyzing jury verdicts to evaluate legal case outcomes},year={2017},author={Jack G. Conrad and Jack G. Conrad and Khalid Al-Kofahi and Khalid Al-Kofahi},doi={10.1145/3086512.3086516},pmid={null},pmcid={null},mag_id={2777022249},journal={null},abstract={Scenario Analytics is a type of analysis that focuses on the evaluation of different scenarios, their merits and their consequences. In the context of the legal domain, this could be in the form of analyzing large databases of legal cases, their facts and their claims, to answer questions such as: Do the current facts warrant litigation?, Is the litigation best pursued before a judge or a jury?, How long is it likely to take?, and What are the best strategies to use for achieving the most favorable outcome for the client? In this work, we report on research directed at answering such questions. We use one of a set of jury verdicts databases totaling nearly a half-million records. At the same time, we conduct a series of experiments that answer key questions and build, sequentially, a powerful data-driven legal decision support system, one that can assist an attorney to differentiate more effective from less effective legal principles and strategies. Ultimately, it represents a productivity tool that can help a litigation attorney make the most prudent decisions for his or her client.}}
@ARTICLE{Grabmair_2017,title={Predicting trade secret case outcomes using argument schemes and learned quantitative value effect tradeoffs},year={2017},author={Matthias Grabmair and Matthias Grabmair},doi={10.1145/3086512.3086521},pmid={null},pmcid={null},mag_id={2777640491},journal={null},abstract={This paper presents the Value Judgment Formalism and its experimental implementation in the VJAP system, which is capable of arguing about, and predicting outcomes of, a set of trade secret misappropriation cases. VJAP creates an argument graph for each case using argument schemes and a representation of values underlying trade secret law and effects of facts on these values. It balances effects on values in each case and analogizes it to tradeoffs in precedents. It predicts case outcomes using a confidence measure computed from the graph and generates textual legal arguments justifying its predictions. The confidence propagation uses quantitative weights learned from past cases using an iterative optimization method. Prediction performance on a limited dataset is competitive with common machine learning models. The results and VJAP's behavior are discussed in detail.}}
@ARTICLE{Bennett_2017,title={A scalable approach to legal question answering},year={2017},author={Zachary Bennett and Zachary W. Bennett and Tony Russell-Rose and Tony Russell-Rose and Kate Farmer and Kate Farmer},doi={10.1145/3086512.3086547},pmid={null},pmcid={null},mag_id={2778343541},journal={null},abstract={Lexis Answers is a question answering service deployed within a live production system. In this paper we provide an overview of the system, an insight into some of the key AI challenges, and a brief description of current evaluation techniques.}}
@ARTICLE{García-Constantino_2017,title={CLIEL: context-based information extraction from commercial law documents},year={2017},author={Matías García-Constantino and Matias Garcia-Constantino and Matias Garcia-Constantino and Katie Atkinson and Katie Atkinson and Danushka Bollegala and Danushka Bollegala and Karl Chapman and Karl Chapman and Frans Coenen and Frans Coenen and Claire Roberts and Claire Roberts and Katy Robson and Katy Robson},doi={10.1145/3086512.3086520},pmid={null},pmcid={null},mag_id={2778931183},journal={null},abstract={The effectiveness of document Information Extraction (IE) is greatly affected by the structure and layout of the documents being considered. In the case of legal documents relating to commercial law, an additional challenge is the many different and varied formats, structures and layouts used. In this paper, we present work on a flexible and scalable IE environment, the CLIEL (Commercial Law Information Extraction based on Layout) environment, for application to commercial law documentation that allows layout rules to be derived and then utilised to support IE. The proposed CLIEL environment operates using NLP (Natural Language Processing) techniques, JAPE (Java Annotation Patterns Engine) rules and some GATE (General Architecture for Text Engineering) modules. The system is fully described and evaluated using a commercial law document corpus. The results demonstrate that considering the layout is beneficial for extracting data point instances from legal document collections.}}
@ARTICLE{Chalkidis_2017,title={Extracting contract elements},year={2017},author={Ilias Chalkidis and Ilias Chalkidis and Ion Androutsopoulos and Ion Androutsopoulos and Achilleas Michos and Achilleas Michos},doi={10.1145/3086512.3086515},pmid={null},pmcid={null},mag_id={2779545291},journal={null},abstract={We study how contract element extraction can be automated. We provide a labeled dataset with gold contract element annotations, along with an unlabeled dataset of contracts that can be used to pre-train word embeddings. Both datasets are provided in an encoded form to bypass privacy issues. We describe and experimentally compare several contract element extraction methods that use manually written rules and linear classifiers (logistic regression, SVMs) with hand-crafted features, word embeddings, and part-of-speech tag embeddings. The best results are obtained by a hybrid method that combines machine learning (with hand-crafted features and embeddings) and manually written post-processing rules.}}
@ARTICLE{Prakken_2017,title={Argument schemes for discussing Bayesian modellings of complex criminal cases},year={2017},author={Henry Prakken and Henry Prakken and Hendrik Prakken},doi={null},pmid={null},pmcid={null},mag_id={2782092012},journal={null},abstract={In this paper two discussions between experts about Bayesian modellings of complex criminal cases are analysed on their argumentation structure. The usefulness of several recognised argument schemes is confirmed, two new schemes for interpretation arguments and for arguments from statistics are proposed, and an analysis is given of debates about the validity of arguments. From a practical point of view the case study yields insights into the design of support software for discussions about Bayesian modellings of complex criminal cases.}}
@ARTICLE{Bench‐Capon_2017,title={Dimensions and values for legal CBR},year={2017},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson},doi={null},pmid={null},pmcid={null},mag_id={2783242362},journal={null},abstract={© 2017 The authors and IOS Press. We build on two recent attempts to formalise reasoning with dimensions which effectively map dimensions into factors. These enable propositional reasoning, but sometimes a balance between dimensions needs to be struck, and to permit trade offs we need to keep the magnitudes and so reason more geometrically. We discuss dimensions and values, arguing that values can play several distinct roles, both explaining preferences between factors and indicating the purposes of the law.}}
@ARTICLE{Hashmi_2018,title={Are we done with business process compliance: state of the art and challenges ahead},year={2018},author={Mustafa Hashmi and Mustafa Hashmi and Guido Governatori and Guido Governatori and Ho-Pun Lam and Ho-Pun Lam and Moe Thandar Wynn and Moe Thandar Wynn},doi={10.1007/s10115-017-1142-1},pmid={null},pmcid={null},mag_id={2785156082},journal={Knowledge and Information Systems},abstract={Literature on business process compliance (BPC) has predominantly focused on the alignment of the regulatory rules with the design, verification and validation of business processes. Previously, surveys on BPC have been conducted with specific context in mind; however, the literature on BPC management research is largely sparse and does not accumulate a detailed understanding on existing literature and related issues faced by the domain. This survey provides a holistic view of the literature on existing BPC management approaches and categorises them based on different compliance management strategies in the context of formulated research questions. A systematic literature approach is used where search terms pertaining keywords were used to identify literature related to the research questions from scholarly databases. From initially 183 papers, we selected 79 papers related to the themes of this survey published between 2000 and 2015. The survey results reveal that mostly compliance management approaches centre around three distinct categories, namely design-time (\(28\%\)), run-time (\(32\%\)) and auditing (\(10\%\)). Also, organisational and internal control-based compliance management frameworks (\(21\%\)) and hybrid approaches make (\(9\%\)) of the surveyed approaches. Furthermore, open research challenges and gaps are identified and discussed with respect to the compliance problem.}}
@ARTICLE{Dam_2018,title={Explainable Software Analytics},year={2018},author={Hoa Khanh Dam and Hoa Khanh Dam and Truyen Tran and Truyen Tran and Aditya Ghose and Aditya Ghose},doi={null},pmid={null},pmcid={null},mag_id={2787839658},journal={arXiv: Software Engineering},abstract={Software analytics has been the subject of considerable recent attention but is yet to receive significant industry traction. One of the key reasons is that software practitioners are reluctant to trust predictions produced by the analytics machinery without understanding the rationale for those predictions. While complex models such as deep learning and ensemble methods improve predictive performance, they have limited explainability. In this paper, we argue that making software analytics models explainable to software practitioners is as \emph{important} as achieving accurate predictions. Explainability should therefore be a key measure for evaluating software analytics models. We envision that explainability will be a key driver for developing software analytics models that are useful in practice. We outline a research roadmap for this space, building on social science, explainable artificial intelligence and software engineering.}}
@ARTICLE{Needles_2009,title={The Data Game: Learning to Love the State-Based Approach to Data Breach Notification Law},year={2009},author={Sara A. Needles and Sara A. Needles},doi={null},pmid={null},pmcid={null},mag_id={2795016136},journal={North Carolina Law Review},abstract={null}}
@ARTICLE{Abdul_2018,title={Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda},year={2018},author={Ashraf Abdul and Ashraf Abdul and Jo Vermeulen and Jo Vermeulen and Danding Wang and Danding Wang and Brian Y. Lim and Brian Y. Lim and Mohan Kankanhalli and Mohan S. Kankanhalli},doi={10.1145/3173574.3174156},pmid={null},pmcid={null},mag_id={2795530988},journal={null},abstract={Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.}}
@ARTICLE{Schauer_1989,title={Is the Common Law Law},year={1989},author={Frederick Schauer and Frederick Schauer and Melvin A. Eisenberg and Melvin Aron Eisenberg},doi={10.2307/3480610},pmid={null},pmcid={null},mag_id={2796851335},journal={California Law Review},abstract={null}}
@ARTICLE{Walter_1992,title={Practical Statistics for Medical Research.},year={1992},author={Sophie Walter and Stephen D. Walter and S. D. Walter and Daniel Altman and Douglas G. Altman},doi={10.2307/2532320},pmid={null},pmcid={null},mag_id={2797597138},journal={Biometrics},abstract={null}}
@ARTICLE{Phillips_2010,title={On Balance},year={2010},author={Adam Phillips and Adam Phillips},doi={null},pmid={null},pmcid={null},mag_id={2798974679},journal={null},abstract={null}}
@ARTICLE{Roegiest_2018,title={A Dataset and an Examination of Identifying Passages for Due Diligence},year={2018},author={Adam Roegiest and Adam Roegiest and Alexander K. Hudek and Alexander K. Hudek and Alexander K. Hudek and Anne McNulty and Anne McNulty},doi={10.1145/3209978.3210015},pmid={null},pmcid={null},mag_id={2798978891},journal={null},abstract={We present and formalize the due diligence problem, where lawyers extract data from legal documents to assess risk in a potential merger or acquisition, as an information retrieval task. Furthermore, we describe the creation and annotation of a document collection for the due diligence problem that will foster research in this area. This dataset comprises 50 topics over 4,412 documents and ~15 million sentences and is a subset of our own internal training data. Using this dataset, we present what we have found to be the state of the art for information extraction in the due diligence problem. In particular, we find that when treating documents as sequences of labelled and unlabelled sentences, Conditional Random Fields significantly and substantially outperform other techniques for sequence-based (Hidden Markov Models) and non-sequence based machine learning (logistic regression). Included in this is an analysis of what we perceive to be the major failure cases when extraction is performed based upon sentence labels.}}
@ARTICLE{Akhigbe_2019,title={A systematic literature mapping of goal and non-goal modelling methods for legal and regulatory compliance},year={2019},author={Okhaide Akhigbe and Okhaide Akhigbe and Daniel Amyot and Daniel Amyot and Gregory Richards and Gregory Richards},doi={10.1007/s00766-018-0294-1},pmid={null},pmcid={null},mag_id={2799759507},journal={Requirements Engineering},abstract={Much research is ongoing to assess and improve compliance to laws and regulations. As this domain continues to grow and mature, and with more modelling methods introduced to support compliance tasks, important questions need to be asked. What exactly are these methods used for? Where have they been applied? What benefits do they offer? This paper explores how goal-oriented and non-goal-oriented modelling methods have been used for legal and regulatory compliance, and identifies their main claimed benefits and drawbacks based on the kind of compliance tasks they perform. Using a systematic literature mapping approach, we evaluated 103 articles describing the use of modelling methods obtained from a pool of 286 articles. The results indicate that modelling methods focus on the intent of a law, but goal-oriented modelling methods do so while also reflecting the structure of a law, generally with substantial benefits for all compliance tasks. In addition, whereas modelling methods are used for compliance modelling, checking, analysis and enactment tasks, our analysis indicates that the coverage of these methods is more frequent in the healthcare domain with 55% of the articles reviewed targeting it. In terms of the contexts modelling methods address, privacy has the highest level of attention with a focus from 54% of the reviewed articles. The articles reviewed revealed a total of 60 different laws and regulations from 14 different countries, with 62% focusing on privacy. Moreover, while 82% of the articles reviewed addressed concerns of regulated parties, only 12% addressed the concerns of regulators, and 6% addressed concerns of both regulating and regulated parties. This study highlights the benefits and drawbacks of both types ofmodelling methods and identifies potential benefits and common drawbacks that will be of interest to researchers and practitioners in the selection of modelling methods or in the identification of selection criteria. Finally, the mapping results emphasize the need for more studies outside of healthcare, that are related to contexts other than privacy, that target compliance enactment tasks or that take the concerns of regulators into consideration.}}
@ARTICLE{Kornhauser_1986,title={Unpacking the Court},year={1986},author={Lewis A. Kornhauser and Lewis A. Kornhauser and Lawrence G. Sager and Lawrence G. Sager},doi={10.2307/796436},pmid={null},pmcid={null},mag_id={2800660311},journal={Yale Law Journal},abstract={null}}
@ARTICLE{Zeni_2018,title={NómosT: Building large models of law with a tool-supported process},year={2018},author={Nicola Zeni and Nicola Zeni and Elias A. Seid and Elias A. Seid and Priscila Engiel and Priscila Engiel and John Mylopoulos and John Mylopoulos},doi={10.1016/j.datak.2018.04.009},pmid={null},pmcid={null},mag_id={2803026876},journal={null},abstract={null}}
@ARTICLE{He_2018,title={SPESC: A Specification Language for Smart Contracts},year={2018},author={Xiao He and Xiao He and Bohan Qin and Bohan Qin and Bohan Qin and Yan Zhu and Yan Zhu and Xing Chen and Xing Chen and Xing Chen and Xing Chen and Yi Liu and Yi Liu},doi={10.1109/compsac.2018.00025},pmid={null},pmcid={null},mag_id={2808749305},journal={null},abstract={The smart contract is an interdisciplinary concept that concerns business, finance, contract law and information technology. Designing and developing a smart contract may require the close cooperation of many experts coming from different fields. How to support such collaborative development is a challenging problem in blockchain-oriented software engineering. This paper proposes SPESC, a specification language for smart contracts, which can define the specification of a smart contract for the purpose of collaborative design. SPESC can specify a smart contract in a similar form to real-world contracts using a natural-language-like grammar, in which the obligations and rights of parties and the transaction rules of cryptocurrencies are clearly defined. The preliminary study results demonstrated that SPESC can be easily learned and understood by both IT and non-IT users and thus has greater potential to facilitate collaborative smart contract development.}}
@ARTICLE{Gordon_2018,title={Representing argumentation schemes with Constraint Handling Rules (CHR)},year={2018},author={Thomas F. Gordon and Thomas F. Gordon and Horst E. Friedrich and Horst Friedrich and Douglas Walton and Douglas Walton},doi={10.3233/aac-180039},pmid={null},pmcid={null},mag_id={2809892404},journal={Argument & Computation},abstract={null}}
@ARTICLE{Sackmann_2018,title={Using Business Process Compliance Approaches for Compliance Management with Regard to Digitization: Evidence from a Systematic Literature Review},year={2018},author={Stefan Sackmann and Stefan Sackmann and Stephan Kühnel and Stephan Kuehnel and Tobias Seyffarth and Tobias Seyffarth},doi={10.1007/978-3-319-98648-7_24},pmid={null},pmcid={null},mag_id={2886595164},journal={null},abstract={Business Process Compliance (BPC) means ensuring that business processes are in accordance with relevant compliance requirements. Thus, BPC is an essential part of both business process management (BPM) and compliance management (CM). Digitization has also been referred to as a “digital revolution” that describes a technological change that has extended to many organizational areas and tasks, including compliance. Current efforts to digitize, e.g., by realizing cyber-physical systems, rely on the automation and interoperability of systems. In order for CM not to hamper these efforts, it becomes an increasingly relevant issue to digitize compliance as well.}}
@ARTICLE{Pedrycz_2018,title={Granular computing for data analytics: a manifesto of human-centric computing},year={2018},author={Witold Pedrycz and Witold Pedrycz},doi={10.1109/jas.2018.7511213},pmid={null},pmcid={null},mag_id={2887660732},journal={IEEE/CAA Journal of Automatica Sinica},abstract={In the plethora of conceptual and algorithmic developments supporting data analytics and system modeling, humancentric pursuits assume a particular position owing to ways they emphasize and realize interaction between users and the data. We advocate that the level of abstraction, which can be flexibly adjusted, is conveniently realized through Granular Computing. Granular Computing is concerned with the development and processing information granules – formal entities which facilitate a way of organizing knowledge about the available data and relationships existing there. This study identifies the principles of Granular Computing, shows how information granules are constructed and subsequently used in describing relationships present among the data.}}
@ARTICLE{Chamoso_2018,title={Tendencies of Technologies and Platforms in Smart Cities: A State-of-the-Art Review},year={2018},author={Pablo Chamoso and Pablo Chamoso and Alfonso González‐Briones and Alfonso González-Briones and Sara Rodrı́guez and Sara Rodríguez and Juan M. Corchado and Juan M. Corchado},doi={10.1155/2018/3086854},pmid={null},pmcid={null},mag_id={2887907818},journal={Wireless Communications and Mobile Computing},abstract={Technology is starting to play a key role in cities’ urban sustainability plans. This is because new technologies can provide them with robust solutions that are of benefit to citizens. Cities aim to incorporate smart systems in their industrial, infrastructural, educational, and social activities. A Smart City is managed with intelligent technologies which allow improving the quality of the services offered to citizens and make all processes more efficient. However, the Smart City concept is fairly recent. The ideas that it encompasses have not yet been consolidated due to the large number of fields and technologies that fit under this concept. All of this led to confusion about the definition of a Smart City and this is evident in the literature. This article explores the literature that addresses the topic of Smart Cities; a comprehensive analysis of the concept and existing platforms is performed. We gain a clear understanding of the services that a Smart City must provide, the technology it should employ for the development of these services, and the scope that this concept covers. Moreover, the shortcomings and needs of Smart Cities are identified and a model for designing a Smart City architecture is proposed. In addition, three case studies have been proposed: the first is a simulator to study the implementation of various services and technologies, the second case study to manage incidents that occur in a Smart City, and the third case study to monitor the deployment of large-scale sensors in a Smart City.}}
@ARTICLE{Garey_1999,title={Computers and Intractability: A Guide to the Theory of NP-Completeness},year={1999},author={Michael Randolph Garey and D. S. Johanson},doi={null},pmid={null},pmcid={null},mag_id={2891212941},journal={null},abstract={null}}
@ARTICLE{Żurek_2018,title={Modelling Legal Interpretation in Structured Argumentation Framework},year={2018},author={Tomasz Żurek and Tomasz Zurek and Michał Araszkiewicz and Michał Araszkiewicz},doi={10.15439/2018f79},pmid={null},pmcid={null},mag_id={2894432977},journal={null},abstract={The paper discusses the problem of formal modeling of the interpretation of statutory legal norms. The authors propose a comprehensive framework that allows the representation of the interpretation process. The authors' proposal is illustrated by a real-life example.}}
@ARTICLE{Zhu_2018,title={Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},year={2018},author={Jichen Zhu and Jichen Zhu and Antonios Liapis and Antonios Liapis and Sebastian Risi and Sebastian Risi and Rafael Bidarra and Rafael Bidarra and G. Michael Youngblood and G. Michael Youngblood},doi={10.1109/cig.2018.8490433},pmid={null},pmcid={null},mag_id={2896010852},journal={null},abstract={Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users’ needs, and we identify key open challenges.}}
@ARTICLE{Al-Abdulkarim_2018,title={Noise induced hearing loss: Building an application using the ANGELIC methodology},year={2018},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Stuart Whittle and Stuart Whittle and Rob Williams and Rob Williams and Catriona Wolfenden and Catriona Wolfenden},doi={10.3233/aac-181005},pmid={null},pmcid={null},mag_id={2897626439},journal={Argument & Computation},abstract={null}}
@ARTICLE{Bench‐Capon_2018,title={Lessons from Implementing Factors with Magnitude},year={2018},author={Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Katie Atkinson and Katie Atkinson},doi={null},pmid={null},pmcid={null},mag_id={2906727503},journal={null},abstract={© 2018 The authors and IOS Press. We discuss the lessons learned from implementing a CATO style system using factors with magnitude. In particular we identify that giving factors magnitudes enables a diversity of reasoning styles and arguments. We distinguish a variety of ways in which factors combine to determine abstract factors. We discuss several different roles for values. Finally we identify the additional value related information required to produce a working program: thresholds and weights as well as a simple preference ordering.}}
@ARTICLE{Allam_2019,title={On big data, artificial intelligence and smart cities},year={2019},author={Zaheer Allam and Zaheer Allam and Zaheer Allam and Zaynah A. Dhunny and Zaynah A. Dhunny},doi={10.1016/j.cities.2019.01.032},pmid={null},pmcid={null},mag_id={2912275277},journal={Cities},abstract={null}}
@ARTICLE{Parnas_2001,title={Software aging},year={2001},author={David Lorge Parnas and David Lorge Parnas},doi={null},pmid={null},pmcid={null},mag_id={2912459328},journal={null},abstract={Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limits its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable. A sign that the software engineering profession has matured will be that we lose our preoccupation with the first release and focus on the long-term health of our products. Researchers and practitioners must change their perception of the problems of software development. Only then will software engineering deserve to be called "engineering". >}}
@ARTICLE{Zadeh_1996,title={Fuzzy sets},year={1996},author={Lotfi A. Zadeh and Lotfi A. Zadeh},doi={null},pmid={null},pmcid={null},mag_id={2912565176},journal={null},abstract={null}}
@ARTICLE{Mochales_2011,title={Argumentation mining},year={2011},author={Raquel Mochales and Raquel Mochales and Marie‐Francine Moens and Marie-Francine Moens},doi={10.1007/s10506-010-9104-x},pmid={null},pmcid={null},mag_id={2912804155},journal={null},abstract={Argumentation mining aims to automatically detect, classify and structure argumentation in text. Therefore, argumentation mining is an important part of a complete argumentation analyisis, i.e. understanding the content of serial arguments, their linguistic structure, the relationship between the preceding and following arguments, recognizing the underlying conceptual beliefs, and understanding within the comprehensive coherence of the specific topic. We present different methods to aid argumentation mining, starting with plain argumentation detection and moving forward to a more structural analysis of the detected argumentation. Different state-of-the-art techniques on machine learning and context free grammars are applied to solve the challenges of argumentation mining. We also highlight fundamental questions found during our research and analyse different issues for future research on argumentation mining.}}
@ARTICLE{Sarawagi_2008,title={Information Extraction},year={2008},author={Sunita Sarawagi and Sunita Sarawagi},doi={10.1561/1900000003},pmid={null},pmcid={null},mag_id={2913389685},journal={null},abstract={The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.

This review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process.}}
@ARTICLE{Al-Abdulkarim_2016,title={Accommodating change},year={2016},author={Latifa Al-Abdulkarim and Latifa Al-Abdulkarim and Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon},doi={10.1007/s10506-016-9190-5},pmid={null},pmcid={null},mag_id={2913409160},journal={null},abstract={The third of Berman and Hafner's early nineties papers on reasoning with legal cases concerned temporal context, in particular the evolution of case law doctrine over time in response to new cases and against a changing background of social values and purposes. In this paper we consider the ways in which changes in case law doctrine can be accommodated in a recently proposed methodology for encapsulating case law theories (the ANGELIC methodology based on Abstract Dialectical Frameworks), and relate these changes the sources of change identified by Berman and Hafner.}}
@ARTICLE{Kowalski_1996,title={Abstract argumentation},year={1996},author={Robert Kowalski and Robert A. Kowalski and Francesca Toni and Francesca Toni},doi={10.1007/bf00118494},pmid={null},pmcid={null},mag_id={2914176469},journal={null},abstract={In this paper we explore the thesis that the role of argumentation in practical reasoning in general and legal reasoning in particular is to justify the use of defeasible rules to derive a conclusion in preference to the use of other defeasible rules to derive a conflicting conclusion. The defeasibility of rules is expressed by means of non-provability claims as additional conditions of the rules.

We outline an abstract approach to defeasible reasoning and argumentation which includes many existing formalisms, including default logic, extended logic programming, non-monotonic modal logic and auto-epistemic logic, as special cases. We show, in particular, that the `admissibility' semantics for all these formalisms has a natural argumentation-theoretic interpretation and proof procedure, which seem to correspond well with informal argumentation.

In the admissibility semantics there is only one way for one argument to attack another, namely by undermining one of its non-provability claims. In this paper, we show how other kinds of attack between arguments, specifically how rebuttal and priority attacks, can be reduced to the undermining of non-provability claims.}}
@ARTICLE{Bex_2011,title={Legal Knowledge and Information Systems. JURIX 2011},year={2011},author={Floris Bex and Floris Bex and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Bart Verheij and Bart Verheij},doi={null},pmid={null},pmcid={null},mag_id={2917045119},journal={null},abstract={null}}
@ARTICLE{Zuckerberg_2011,title={Why Software Is Eating the World},year={2011},author={Mark Zuckerberg},doi={null},pmid={null},pmcid={null},mag_id={2917746498},journal={null},abstract={null}}
@ARTICLE{Prakken_2019,title={An Argumentation‐Based Analysis of the Simonshaven Case},year={2019},author={Henry Prakken and Henry Prakken and Hendrik Prakken},doi={10.1111/tops.12418},pmid={null},pmcid={null},mag_id={2921096706},journal={Topics in Cognitive Science},abstract={In an argumentation approach, legal evidential reasoning is modeled as the construction and attack of "trees of inference" from evidence to conclusions by applying generalizations to evidence or intermediate conclusions. In this paper, an argumentation-based analysis of the Simonshaven case is given in terms of a logical formalism for argumentation. The formalism combines abstract argumentation frameworks with accounts of the structure of arguments, of the ways they can be attacked and of ways to evaluate conflicting arguments. The purpose of this paper is not to demonstrate or argue that the argumentation approach to modeling legal evidential reasoning is feasible or even preferable but to have a fully worked-out example that can be used in the comparison with alternative Bayesian or scenario-based analyses.}}
@ARTICLE{Bex_2019,title={The Hybrid Theory of Stories and Arguments Applied to the Simonshaven Case.},year={2019},author={Floris Bex and Floris Bex},doi={10.1111/tops.12426},pmid={null},pmcid={null},mag_id={2944387294},journal={Topics in Cognitive Science},abstract={This paper presents the hybrid theory of stories and arguments for reasoning with evidence in legal cases and applies this theory to the Simonshaven case. In the hybrid theory, alternative hypothetical stories about “what happened” in a case are constructed and discussed in a dialectical process of argument and counterargument. After informally explaining stories, arguments, and the ways in which they interact, this paper gives a method for rational proof based on critical questions and shows how this method can be used in the Simonshaven case.}}
@ARTICLE{Bello_2020,title={Evidence & decision making in the law: theoretical, computational and empirical approaches},year={2020},author={Marcello Di Bello and Marcello Di Bello and Bart Verheij and Bart Verheij},doi={10.1007/s10506-019-09253-0},pmid={null},pmcid={null},mag_id={2950186649},journal={Artificial Intelligence and Law},abstract={null}}
@ARTICLE{Avižienis_2001,title={Fundamental Concepts of Dependability},year={2001},author={A. Avižienis and Algirdas Avizienis and Jean-Claude Laprie and J-C Laprie and Brian Randell and Brian Randell},doi={null},pmid={null},pmcid={null},mag_id={2952753488},journal={null},abstract={1. Origins and Integration of the Concepts Critical Applications was held in 1989. This and the six working conferences that followed fostered the interaction of the dependability and security communities, and advanced the integration of security (confidentiality, integrity and availability) into the framework of dependable computing [22]. A summary of [22] is presented next. The concept of dependable computing first appears in the 1830’s in the context of Babbage’s Calculating Engine [1,2]. The first generation of electronic computers (late 1940’s to mid-50’s) used rather unreliable components, therefore practical techniques were employed to improve their reliability, such as error control codes, duplexing with comparison, triplication with voting, diagnostics to locate failed components, etc. [3-5]. 2. The Principal Concepts: a Summary}}
@ARTICLE{Loevinger_1949,title={Jurimetrics--The Next Step Forward},year={1949},author={Lee Loevinger and Lee Loevinger and Lee Loevinger},doi={null},pmid={null},pmcid={null},mag_id={2953471993},journal={null},abstract={null}}
@ARTICLE{James_2012,title={interpretation of the law},year={2012},author={Simon James and Simon James and Simon James},doi={null},pmid={null},pmcid={null},mag_id={2956617952},journal={null},abstract={null}}
@ARTICLE{Sleimi_2019,title={A Query System for Extracting Requirements-Related Information from Legal Texts},year={2019},author={Amin Sleimi and Amin Sleimi and Marcello Ceci and Marcello Ceci and Nicolas Sannier and Nicolas Sannier and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand and John Dann and John Dann},doi={10.1109/re.2019.00041},pmid={null},pmcid={null},mag_id={2962221540},journal={null},abstract={Searching legal texts for relevant information is a complex and expensive activity. The search solutions offered by present-day legal portals are targeted primarily at legal professionals. These solutions are not adequate for requirements analysts whose objective is to extract domain knowledge including stakeholders, rights and duties, and business processes that are relevant to legal requirements. Semantic Web technologies now enable smart search capabilities and can be exploited to help requirements analysts in elaborating legal requirements. In our previous work, we developed an automated framework for extracting semantic metadata from legal texts. In this paper, we investigate the use of our metadata extraction framework as an enabler for smart legal search with a focus on requirements engineering activities. We report on our industrial experience helping the Government of Luxembourg provide an advanced search facility over Luxembourg's Income Tax Law. The experience shows that semantic legal metadata can be successfully exploited for answering requirements engineering-related legal queries. Our results also suggest that our conceptualization of semantic legal metadata can be further improved with new information elements and relations.}}
@ARTICLE{Torre_2019,title={Using Models to Enable Compliance Checking Against the GDPR: An Experience Report},year={2019},author={Damiano Torre and Damiano Torre and Damiano Torre and Ghanem Soltana and Ghanem Soltana and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand and Yuri Auffinger and Yuri Auffinger and Peter Goes and Peter Goes},doi={10.1109/models.2019.00-20},pmid={null},pmcid={null},mag_id={2963094141},journal={null},abstract={The General Data Protection Regulation (GDPR) harmonizes data privacy laws and regulations across Europe. Through the GDPR, individuals are able to better control their personal data in the face of new technological developments. While the GDPR is highly advantageous to individuals, complying with it poses major challenges for organizations that control or process personal data. Since no automated solution with broad industrial applicability currently exists for GDPR compliance checking, organizations have no choice but to perform costly manual audits to ensure compliance. In this paper, we share our experience building a UML representation of the GDPR as a first step towards the development of future automated methods for assessing compliance with the GDPR. Given that a concrete implementation of the GDPR is affected by the national laws of the EU member states, GDPR's expanding body of case law and other contextual information, we propose a two-tiered representation of the GDPR: a generic tier and a specialized tier. The generic tier captures the concepts and principles of the GDPR that apply to all contexts, whereas the specialized tier describes a specific tailoring of the generic tier to a given context, including the contextual variations that may impact the interpretation and application of the GDPR. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it, and future directions for research.}}
@ARTICLE{Branting_2019,title={Semi-Supervised Methods for Explainable Legal Prediction},year={2019},author={L. Karl Branting and Karl Branting and Brandy Weiss and Brandy Weiss and B. Weiss and Bradford Brown and Bradford Brown and B. Brown and Craig Pfeifer and Craig Pfeifer and Amartya Chakraborty and A. Chakraborty and Lisa Ferro and Lisa Ferro and Mark S. Pfaff and M. Pfaff and Mark S. Pfaff and Alexander Yeh and Alexander S. Yeh},doi={10.1145/3322640.3326723},pmid={null},pmcid={null},mag_id={2963594477},journal={null},abstract={Legal decision-support systems have the potential to improve access to justice, administrative efficiency, and judicial consistency, but broad adoption of such systems is contingent on development of technologies with low knowledge-engineering, validation, and maintenance costs. This paper describes two approaches to an important form of legal decision support---explainable outcome prediction---that obviate both annotation of an entire decision corpus and manual processing of new cases. The first approach, which uses an Attention Network for prediction and attention weights to highlight salient case text, was shown to be capable of predicting decisions, but attention-weight-based text highlighting did not demonstrably improve human decision speed or accuracy in an evaluation with 61 human subjects. The second approach, termed SCALE (Semi-supervised Case Annotation for Legal Explanations), exploits structural and semantic regularities in case corpora to identify textual patterns that have both predictable relationships to case decisions and explanatory value.}}
@ARTICLE{Dam_2018,title={Explainable software analytics},year={2018},author={Hoa Khanh Dam and Hoa Khanh Dam and Truyen Tran and Truyen Tran and Aditya Ghose and Aditya Ghose},doi={10.1145/3183399.3183424},pmid={null},pmcid={null},mag_id={2963998044},journal={null},abstract={Software analytics has been the subject of considerable recent attention but is yet to receive significant industry traction. One of the key reasons is that software practitioners are reluctant to trust predictions produced by the analytics machinery without understanding the rationale for those predictions. While complex models such as deep learning and ensemble methods improve predictive performance, they have limited explainability. In this paper, we argue that making software analytics models explainable to software practitioners is as important as achieving accurate predictions. Explainability should therefore be a key measure for evaluating software analytics models. We envision that explainability will be a key driver for developing software analytics models that are useful in practice. We outline a research roadmap for this space, building on social science, explainable artificial intelligence and software engineering.}}
@ARTICLE{Libal_2019,title={Automated reasoning in normative detachment structures with ideal conditions},year={2019},author={Tomer Libal and Tomer Libal and Tomer Libal and Matteo Pascucci and Matteo Pascucci},doi={10.1145/3322640.3326707},pmid={null},pmcid={null},mag_id={2964230523},journal={null},abstract={In this article we introduce a logical structure for normative reasoning, called Normative Detachment Structure with Ideal Conditions, that can be used to represent the content of certain legal texts in a normalized way. The structure exploits the deductive properties of a system of bimodal logic able to distinguish between ideal and actual normative statements, as well as a novel formalization of conditional normative statements able to capture interesting cases of contrary-to-duty reasoning and to avoid deontic paradoxes. Furthermore, we illustrate how the theoretical framework proposed can be mechanized to get an automated procedure of query-answering on an example of legal text.}}
@ARTICLE{Besnard_2008,title={Computational Models of Argument:Proceedings of COMMA 2008},year={2008},author={Philippe Besnard and Philippe Besnard},doi={null},pmid={null},pmcid={null},mag_id={2970113970},journal={null},abstract={null}}
@ARTICLE{Clewley_2019,title={Eliciting Expert Knowledge to Inform Training Design},year={2019},author={Natalie Clewley and Natalie Clewley and Lorraine Dodd and Lorraine Dodd and Victoria Smy and Victoria Smy and Annamaria Witheridge and Annamaria Witheridge and Panos Louvieris and Panos Louvieris},doi={10.1145/3335082.3335091},pmid={null},pmcid={null},mag_id={2971583435},journal={null},abstract={Purpose: To determine the elicitation methodologies best placed to uncover and capture the expert operator’s reflective cognitive judgements in complex and dynamic military operating environments (e.g., explosive ordinance disposal) in order to develop the specification for a reflective eXplainable Artificial Intelligence (XAI) agent to support the training of domain novices. Approach: A bounded literature review of the latest developments in expert knowledge elicitation was undertaken to determine the ’art-of-the-possible’ in respects to uncovering an expert’s cognitive judgements in complex and dynamic environments. Candidate methodologies were systematically and critically reviewed in order to identify the most promising methodologies for uncovering expert situational awareness and metacognitive evaluations in pursuit of actionable threat mitigation strategies in high-risk contexts. Research outputs are synthesized into an interview protocol for eliciting and understanding the in-situ actions and decisions of experts in high-risk, complex operating environments. Practical implications: Trainees entering high-risk operating environments can benefit from exposure to expert reflective strategies whilst learning the trade. Typical operator training focuses on technical aspects of threat mitigation but often overlooks reflective self-evaluation. The present study represents an initial step towards determining the feasibility of designing a reflective XAI agent to augment the performance of trainees entering high-risk operations. Outputs of the expert knowledge elicitation protocol documented here shall be used to refine a theoretical framework of expert operator judgement, in order to determine decision support strategies of benefit to domain novices.}}
@ARTICLE{Lécué_2020,title={On the role of knowledge graphs in explainable AI},year={2020},author={Freddy Lécué and Freddy Lecue and Freddy Lecue and Freddy Lécué},doi={10.3233/sw-190374},pmid={null},pmcid={null},mag_id={2982378168},journal={Social Work},abstract={The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s. Explainable AI (XAI) now refers to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search. We expose the XAI challenges of AI fields, their existing approaches, limitations and opportunities for Knowledge Graphs and their underlying technologies.}}
@ARTICLE{Seeliger_2019,title={Semantic Web Technologies for Explainable Machine Learning Models: A Literature Review.},year={2019},author={Arne Seeliger and Arne Seeliger and Arne Seeliger and Matthias Pfaff and Matthias Pfaff and Helmut Krcmar and Helmut Krcmar},doi={null},pmid={null},pmcid={null},mag_id={2982534028},journal={null},abstract={null}}
@ARTICLE{Alsaadi_2019,title={Minimizing the ambiguities in medical devices regulations based on software requirement engineering techniques},year={2019},author={Mohmood Alsaadi and Mohmood Alsaadi and Alexei Lisitsa and Alexei Lisitsa and Malik Qasaimeh and Malik Qasaimeh},doi={10.1145/3368691.3368709},pmid={null},pmcid={null},mag_id={2991813970},journal={null},abstract={Trusted medical software devices, must comply with one of the healthcare regulations such as Food and Drug Administration (FDA), EU Medical Device Regulation (MDR), and Health Insurance Portability and Accountability Act (HIPAA). Since these regulations are enacted by law legislators and written in a legal text, these regulations are typically written with ambiguities. However, people have a different interpretation for the legal text for example, software developers faced challenges in identifying and understanding the regulatory requirements that are related to the software development process. Moreover, ambiguous in regulatory requirements play a big role in software compliance with regulatory particularly, when the requirements are legal text.   In this paper, we intend to minimize the ambiguities in EU MDR requirements based on requirement engineering techniques in order to make MDR requirements clear and precise for software developers. In our previous work, we extracted the requirements of MDR that would affect the software development life cycle (SDLC) directly or indirectly. Herein, we will identify the ambiguity types in the extracted requirements of MDR. Moreover, this paper will present a method to minimize the ambiguities in MDR requirements based on requirement engineering techniques (user story and use case diagram). Finally, this work will be evaluated by the critical-to-quality tree measurement technique.}}
@ARTICLE{Flick_2009,title={An introduction to qualitative research, 4th ed.},year={2009},author={Uwe Flick and Uwe Flick and Uwe Flick},doi={null},pmid={null},pmcid={null},mag_id={2998008903},journal={null},abstract={null}}
@ARTICLE{Walton_2020,title={Using argumentation schemes to find motives and intentions of a rational agent},year={2020},author={Douglas Walton and Douglas Walton},doi={10.3233/aac-190480},pmid={null},pmcid={null},mag_id={3001675652},journal={Argument & Computation},abstract={null}}
@ARTICLE{MoulinB._2002,title={explanation and argumentation capabilities},year={2002},author={MoulinB. and IrandoustH. and BélangerM. and DesbordesG.},doi={null},pmid={null},pmcid={null},mag_id={3005127088},journal={Artificial Intelligence Review},abstract={During the past two decades many research teams have worked on the enhancement of the explanation capabilities of knowledge-based systems and decision support systems. During the same period, other...}}
@ARTICLE{Rabinia_2020,title={A Methodology for Implementing the Formal Legal-GRL Framework: A Research Preview},year={2020},author={Amin Rabinia and Amin Rabinia and Sepideh Ghanavati and Sepideh Ghanavati and Sepideh Ghanavati and Sepideh Ghanavati and Llio Humphreys and Llio Humphreys and Torsten Hahmann and Torsten Hahmann and Torsten Hahmann},doi={null},pmid={null},pmcid={null},mag_id={3011262713},journal={null},abstract={[Context and motivation] Legal provisions create a distinct set of requirements for businesses to be compliant with. Capturing legal requirements and managing regulatory compliance is a challenging task in system development. [Question/problem] Part of this task involves modeling legal requirements, which is not trivial for requirements engineers as non-experts in law. The resultant legal requirements models also tend to be very complex and hard to understand. [Principal ideas/results] To facilitate the modeling process, we propose a formal framework for modeling legal requirements. This framework includes a methodology that helps to resolve complexities of legal requirements models. [Contribution] In this paper, we outline this methodology and present a procedure that reduces modal and conditional complexities of legal models and facilitates automation of the modeling process.}}
@ARTICLE{Grassofloriana_2000,title={dialectical argumentation to solve conflicts in advice giving},year={2000},author={Grassofloriana and Cawseyalison and Jonesray},doi={null},pmid={null},pmcid={null},mag_id={3012530077},journal={International Journal of Human-computer Studies \/ International Journal of Man-machine Studies},abstract={Conflict situations do not only arise from misunderstandings, erroneous perceptions, partial knowledge, false beliefs, etc., but also from differences in “opinions” and in the different agents' val...}}
@ARTICLE{Wolf_2020,title={Designing accessible, explainable AI (XAI) experiences},year={2020},author={Christine T. Wolf and Christine T. Wolf and T WolfChristine and Kathryn E. Ringland and Kathryn E. Ringland and E RinglandKathryn},doi={null},pmid={null},pmcid={null},mag_id={3019674878},journal={ACM Sigaccess Accessibility and Computing},abstract={Explainable Artificial Intelligence (XAI) has taken off in recent years, a field that develops techniques to render complex AI and machine learning (ML) models comprehensible to humans. Despite the...}}
@ARTICLE{Saias_2005,title={A methodology to create legal ontologies in a logic programming information retrieval system},year={2005},author={José Saias and José Saias and Paulo Quaresma and Paulo Quaresma and Paulo Quaresma},doi={null},pmid={null},pmcid={null},mag_id={3023218560},journal={null},abstract={Legal web information retrieval systems need the capability to reason with the knowledge modeled by legal ontologies. Using this knowledge it is possible to represent and to make inferences about the semantic content of legal documents.

In this paper a methodology for applying NLP techniques to automatically create a legal ontology is proposed. The ontology is defined in the OWL semantic web language and it is used in a logic programming framework, EVOLP+ISCO, to allow users to query the semantic content of the documents. ISCO allows an easy and efficient integration of declarative, object-oriented and constraint-based programming techniques with the capability to create connections with external databases. EVOLP is a dynamic logic programming framework allowing the definition of rules for actions and events.

An application of the proposed methodology to the legal information retrieval system of the Portuguese Attorney General's Office is described.}}
@ARTICLE{Atkinson_2020,title={In memoriam Douglas N. Walton: the influence of Doug Walton on AI and law},year={2020},author={Katie Atkinson and Katie Atkinson and Trevor Bench‐Capon and Trevor J. M. Bench-Capon and Floris Bex and Floris Bex and Thomas F. Gordon and Thomas F. Gordon and Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor and Bart Verheij and Bart Verheij},doi={10.1007/s10506-020-09272-2},pmid={null},pmcid={null},mag_id={3036166843},journal={Artificial Intelligence and Law},abstract={Doug Walton, who died in January 2020, was a prolific author whose work in informal logic and argumentation had a profound influence on Artificial Intelligence, including Artificial Intelligence and Law. He was also very interested in interdisciplinary work, and a frequent and generous collaborator. In this paper seven leading researchers in AI and Law, all past programme chairs of the International Conference on AI and Law who have worked with him, describe his influence on their work.}}
@ARTICLE{Sharifi_2020,title={Symboleo: Towards a Specification Language for Legal Contracts},year={2020},author={Sepehr Sharifi and Sepehr Sharifi and Daniel Amyot and Alireza Parvizimosaed and Alireza Parvizimosaed and Daniel Amyot and Daniel Amyot and Daniel Amyot and Luigi Logrippo and Luigi Logrippo and John Mylopoulos and John Mylopoulos},doi={10.1109/re48521.2020.00049},pmid={null},pmcid={null},mag_id={3040098638},journal={null},abstract={Legal contracts specify the terms and conditions (in essence, requirements) that apply to business transactions. Smart contracts are software systems that monitor and control the execution of contracts to ensure compliance. This paper proposes a formal specification language for contracts, called Symboleo, where contracts consist of collections of obligations and powers that define the legal contract’s compliant executions. The formal semantics of Symboleo is based on an extension of an ontology for Law and is described in terms of logical axioms on statecharts that describe the lifetimes of contracts, obligations and powers. Our proposal includes a preliminary evaluation through the specification of a real life-inspired Sale-of-Goods contract, with a prototype execution engine. We envision this language to enable formally verifying contracts to detect requirements-level issues and to generate executable smart contracts (e.g., on blockchain technology).}}
@ARTICLE{Imtiaz_2017,title={Validation standard operating procedures},year={2017},author={Imtiaz Syed Imtiaz and Syed Imtiaz Haider and Syed Haider and Imtiaz Syed Imtiaz},doi={null},pmid={null},pmcid={null},mag_id={3087365361},journal={null},abstract={Validation standard operating procedures , Validation standard operating procedures , کتابخانه مرکزی دانشگاه علوم پزشکی تهران}}
@ARTICLE{Sleimi_2020,title={Automated Recommendation of Templates for Legal Requirements},year={2020},author={Amin Sleimi and Amin Sleimi and Marcello Ceci and Marcello Ceci and Mehrdad Sabetzadeh and Mehrdad Sabetzadeh and Lionel C. Briand and Lionel C. Briand and John Dann and John Dann},doi={null},pmid={null},pmcid={null},mag_id={3089148641},journal={null},abstract={[Context] In legal requirements elicitation, requirements analysts need to extract obligations from legal texts. However, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart. This phenomenon has already been described in the Requirements Engineering (RE) literature [1]. [Objectives] We investigate the use of requirements templates for the systematic elicitation of legal requirements. Our work is motivated by two observations: (1) The existing literature does not provide a harmonized view on the requirements templates that are useful for legal RE; (2) Despite the promising recent advancements in natural language processing (NLP), automated support for legal RE through the suggestion of requirements templates has not been achieved yet. Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the legal requirement templates proposed in RE. Subsequently, we conduct a qualitative study to define NLP rules for template recommendation. [Results and Conclusions] Our contributions consist of (a) a harmonized list of requirements templates pertinent to legal RE, and (b) rules for the automatic recommendation of such templates. We evaluate our rules through a case study on 400 statements from two legal domains. The results indicate a recall and precision of 82,3% and 79,8%, respectively. We show that introducing some limited interaction with the analyst considerably improves accuracy. Specifically, our human-feedback strategy increases recall by 12% and precision by 10,8%, thus yielding an overall recall of 94,3% and overall precision of 90,6%.}}
@ARTICLE{Sainani_2020,title={Extracting and Classifying Requirements from Software Engineering Contracts},year={2020},author={Abhishek Sainani and Abhishek Sainani and Preethu Rose Anish and Preethu Rose Anish and Veena Joshi and Vivek Joshi and Vivek Joshi and Smita Ghaisas and Smita Ghaisas},doi={10.1109/re48521.2020.00026},pmid={null},pmcid={null},mag_id={3092221863},journal={null},abstract={In this paper, we present our work on extracting and classifying requirements from large software engineering contracts. Typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants. Our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high-level requirements relevant to the success of software engineering projects. We posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub-discipline. This is because software engineering-specific requirements are but a part of the success story of any large project. The requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development. Therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project. We discuss the results of an exploratory study and a range of experiments from the use of regular expressions to Bidirectional Encoder Representations from Transformers for automating the extraction and classification of requirements from software engineering contracts. With Bidirectional Encoder Representations from Transformers, we obtained a high f-score of greater than eighty four percent for classification of requirements.}}
@ARTICLE{Parvizimosaed_2020,title={Subcontracting, Assignment, and Substitution for Legal Contracts in Symboleo},year={2020},author={Alireza Parvizimosaed and Alireza Parvizimosaed and Sepehr Sharifi and Sepehr Sharifi and Daniel Amyot and Daniel Amyot and Luigi Logrippo and Luigi Logrippo and John Mylopoulos and John Mylopoulos},doi={null},pmid={null},pmcid={null},mag_id={3095899133},journal={null},abstract={AbstractLegal contracts specify obligations and powers among legal subjects, involve assets, and are subject to quality constraints. Smart contracts are software systems that monitor the execution of contracts to ensure compliance. As a starting point for developing software engineering concepts, tools, and techniques for smart contracts, we have proposed Symboleo, a formal specification language for contracts. The complexity of real-life contracts (e.g., in the construction and transportation industries) requires specification languages to support execution-time operations for contracts, such as subcontracting, assignment, delegation, and substitution. This paper formalizes such concepts by proposing for them a syntax and axiomatic semantics within Symboleo. This formalization makes use of primitive operations that support the transfer or sharing of right, responsibility, and performance among contracting and subcontracting parties. A prototype compliance checking tool for Symboleo has also been created to support monitoring compliance for contracts that include subcontracting aspects. A realistic freight contract specified in Symboleo is provided as an illustrative example for our proposal, and is used for a preliminary evaluation with positive results.KeywordsContractsFormal specification languagesLegal subcontractsSmart contractsSubcontracting}}
@ARTICLE{Soavi_2020,title={ContracT – from Legal Contracts to Formal Specifications: Preliminary Results},year={2020},author={Michele Soavi and Michele Soavi and Nicola Zeni and Nicola Zeni and John Mylopoulos and John Mylopoulos and Luisa Mich and Luisa Mich},doi={10.1007/978-3-030-63479-7_9},pmid={null},pmcid={null},mag_id={3098731117},journal={null},abstract={We are interested in semi-automating the process of generating a formal specification from a legal contract in natural language text form. Towards this end, we present a tool, named ContracT, that annotates legal contract text using an ontology for legal contracts. In the last part of the paper, we present results from a preliminary empirical evaluation of the tool that provided encouraging results in identifying contract concepts in text and discuss critical points to be tackled in future studies.}}
@ARTICLE{Montazeri_2011,title={From Contracts in Structured English to CL Specifications},year={2011},author={Seyed Morteza Montazeri and Nivir Kanti Singha Roy and Gerardo Schneider},doi={10.4204/eptcs.68.6},pmid={null},pmcid={null},mag_id={3101222257},journal={null},abstract={In this paper we present a framework to analyze conflicts of contracts written in structured English. A contract that has manually been rewritten in a structured English is automatically translated into  a formal language using the Grammatical Framework (GF). In particular we use the contract language CL as a target formal language for this translation. In our framework CL specifications could then be input into the tool CLAN to detect the presence of conflicts (whether there are contradictory obligations, permissions, and prohibitions. We also use GF to get a version in (restricted) English of CL formulae. We discuss the implementation of such a framework.}}
@ARTICLE{Prakken_2007,title={Formalising arguments about the burden of persuasion},year={2007},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={10.1145/1276318.1276338},pmid={null},pmcid={null},mag_id={3121673662},journal={null},abstract={This paper presents an argument-based logic for reasoning about allocations of the burden of persuasion. The logic extends the system of Prakken (2001), which in turn modified the system of Prakken & Sartor (1996) with the possibility to distribute the burden of proof over both sides in an argument game. First the (2001) system is put in the context of a distinction of three types of proof burdens and it is argued that the proof burdens of that system are in fact burdens of persuasion. Then the (2001) system is modified to allow for defeasible reasoning about allocations of such burdens within the logic. The usefulness of the resulting system is illustrated with applications to real legal cases.}}
@ARTICLE{Bommarito_2009,title={Law as a seamless web?: comparison of various network representations of the United States Supreme Court corpus (1791-2005)},year={2009},author={Michael James Bommarito and Michael James Bommarito and Daniel Katz and Daniel Martin Katz and Jon Zelner and Jon Zelner},doi={10.1145/1568234.1568270},pmid={null},pmcid={null},mag_id={3121774995},journal={null},abstract={In this paper, we compare several network representations of the corpus of United States Supreme Court decisions (1791--2005). This corpus is not only of seminal importance, but also represents a highly structured and largely self-contained body of case law. As constructed herein, nodes represent whole cases or individual "opinion units" within cases. Edges represent either citations or semantic connections. As our broader goal is to better understand American common law development, we are particularly interested in the union, intersect and compliment of these various citation networks as they offer potential insight into the time developing structure of the "web of the law."}}
@ARTICLE{Grosof_2003,title={Sweetdeal: Representing Agent Contracts with Exceptions Using Xml Rules, Ontologies, and Process Descriptions},year={2003},author={Benjamin N. Grosof and Benjamin N. Grosof and T. C. Poon and Terrence C. Poon},doi={10.2139/ssrn.442040},pmid={null},pmcid={null},mag_id={3122623289},journal={null},abstract={SweetDeal is a rule-based approach to representation of business contracts that enables software agents to create, evaluate, negotiate, and execute contracts with substantial automation and modularity. It builds upon the situated courteous logic programs knowledge representation in RuleML, the emerging standard for Semantic Web XML rules. Here, we newly extend the SweetDeal approach by also incorporating process knowledge descriptions whose ontologies are represented in DAML+OIL (the close predecessor of W3C's OWL, the emerging standard for Semantic Web ontologies), thereby enabling more complex contracts with behavioral provisions, especially for handling exception conditions (e.g., late delivery or non-payment) that might arise during the execution of the contract. This provides a foundation for representing and automating deals about services - in particular, about Web Services, so as to help search, select, and compose them. We give a detailed application scenario of late delivery in manufacturing supply chain management (SCM). In doing so, we draw upon our new formalization of process ontology knowledge from the MIT Process Handbook, a large, previously-existing repository used by practical industrial process designers. Our system is the first to combine emerging Semantic Web standards for knowledge representation of rules (RuleML) with ontologies (DAML+OIL/OWL) with each other, and moreover for a practical e-business application domain, and further to do so with process knowledge. This also newly fleshes out the evolving concept of Semantic Web Services. A prototype (soon public) is running. e-business, information technologies}}
@ARTICLE{Scott_2000,title={Is Too Much Privacy Bad for Your Health? An Introduction to the Law, Ethics, and HIPAA Rule on Medical Privacy},year={2000},author={Charity Scott and Charity Scott},doi={10.1385/1-59259-089-6:3},pmid={null},pmcid={null},mag_id={3122819648},journal={Georgia State University law review},abstract={After years of Congressional debates and agency rule-making, the HIPAA health care privacy rule has finally become effective. Why did it take so long? Many of the issues which deadlocked Congressional attempts to pass federal privacy legislation reflect deep disagreements over how much individual privacy should be protected. On one hand, patients often believe that no one except their closest health caregivers should be able to see their medical records without their permission. On the other hand, many other people and entities, mostly strangers to the patient, believe they should have access to those records without having to ask permission. They argue that such relatively free access to individual health care information is good for the patient, good for other patients, and good for society - in other words, that too much privacy protection is bad for our individual and collective health. Indeed, society has often tolerated sacrifices in individual medical privacy in order to promote the public good. This article examines the ethical trade-offs that we have made between the benefits of privacy protection and the benefits of sacrificing it in the name of social welfare, such as advances in scientific research, protection of the public health, higher efficiency and better quality in the delivery of health care, and even improved law enforcement. It analyzes how these ethical trade-offs have been reflected in our laws, Congressional debates, and recent HIPAA privacy rule. It also examines several key issues which created the impasse in enacting comprehensive federal legislation, and which continue to be controversial as providers face the task of compliance. The article serves as a guide for legal and health care professionals who would like to be introduced to what the controversy over health care privacy has been all about and how the HIPAA rule has, at least for the time being, resolved some aspects of it.}}
@ARTICLE{Popple_1991,title={Legal Expert Systems: The Inadequacy of a Rule-Based Approach.},year={1991},author={James Popple and James Popple},doi={null},pmid={null},pmcid={null},mag_id={3123414505},journal={null},abstract={The two different categories of legal AI system are described, and legal analysis systems are chosen as objects of study. So-called judgment machines are discussed, but it is decided that research in legal AI systems would be best carried-out in the area of legal expert systems. A model of legal reasoning is adopted, and two different methods of legal knowledge representation are examined: rule-based systems and case-based systems. It is argued that a rule-based approach to legal expert systems is inadequate given the requirements of lawyers and the nature of legal reasoning about cases. A new, eclectic approach is proposed, incorporating both rule-based and case-based knowledge representation. It is claimed that such an approach can form the basis of an effective and useful legal expert system.}}
@ARTICLE{Prakken_2006,title={Presumptions and Burdens of Proof},year={2006},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={null},pmid={null},pmcid={null},mag_id={3123509826},journal={null},abstract={This paper studies the logical modelling of presumptions and their effects on the burden of proof. Presumptions are modelled as default rules and their effect on the burden of proof is defined in terms of a distinction between the burden of production, the burden of persuasion and the tactical burden of proof. These notions are logically characterised in such a way that presumptions enable a party to fulfil a burden of production or persuasion while shifting a tactical burden to the other party. Finally, it is shown how debates about what can be presumed can be modelled as debates about the backings of default rules.}}
@ARTICLE{Gordon_2016,title={Formalizing Balancing Arguments.},year={2016},author={Thomas F. Gordon and Thomas F. Gordon and Douglas Walton and Douglas Walton},doi={null},pmid={null},pmcid={null},mag_id={3124046462},journal={null},abstract={This paper presents a formal model of structured argument that can handle balancing arguments, arguments were pros and cons are balanced to choose among alternative options. The model can handle cumulative arguments, arguments that provide bits of evidence that can support or undermine hypothesis as new evidence comes in. The model has been fully implemented in version 4 of the Carneades Argumentation System.}}
@ARTICLE{Polinsky_2006,title={OPTIMAL FINES AND AUDITING WHEN WEALTH IS COSTLY TO OBSERVE},year={2006},author={Maria Polinsky and A. Mitchell Polinsky},doi={10.1016/j.irle.2006.11.004},pmid={null},pmcid={null},mag_id={3124249812},journal={International Review of Law and Economics},abstract={Abstract   This article studies optimal fines when an offender's wealth is private information that can be obtained by the enforcement authority only after a costly audit. I derive the optimal fine for the underlying offense, the optimal fine for misrepresenting one's wealth level, and the optimal audit probability. The optimal fine for misrepresenting wealth equals the fine for the offense divided by the audit probability, and therefore generally exceeds the fine for the offense. The optimal audit probability is positive, increases as the cost of an audit declines, and equals unity if the cost is sufficiently low. If the optimal audit probability is less than unity, there are some individuals who are capable of paying the fine for the offense who misrepresent their wealth levels. Additionally, the optimal fine for the offense results in underdeterrence due to the cost of auditing wealth levels.}}
@ARTICLE{Prakken_2008,title={More on Presumptions and Burdens of Proof},year={2008},author={Henry Prakken and Henry Prakken and Giovanni Sartor and Giovanni Sartor},doi={null},pmid={null},pmcid={null},mag_id={3124742527},journal={null},abstract={This paper extends our previous logical analysis of presumptions and burden of proof by studying the force of a presumption once counterevidence has been offered. In the jurisprudential literature different accounts of this issue have been given: some have argued that a presumption is nullified by counterarguments while others have maintained that this gives presumptions a force that is too slight. We argue that these differences largely are not a matter of logic but of legal policy, and we show how the various accounts can be logically formalised.}}
@ARTICLE{Christensen_2007,title={The Paradox of Legal Expertise: A Study of Experts and Novices Reading the Law},year={2007},author={Leah M. Christensen and Leah M. Christensen and Leah M. Christensen},doi={null},pmid={null},pmcid={null},mag_id={3124822066},journal={Brigham Young University Education and Law Journal},abstract={What strategies do lawyers and judges use to read the law? The study described in this article examined the way in which 10 legal experts (8 lawyers and 2 judges) and 10 novices (law students in the top 50% of their class) read a judicial opinion. Whereas the experts read efficiently (taking less overall time), the beginning law students read less efficiently. Where the experts read the text flexibly, moving back and forth between different parts of the opinion, the novices read inflexibly. The experts connected to the purpose of their reading more consistently than the novices and drew upon their prior knowledge and experience with the law. The results of this study suggest that we can give our students the following advice: (1) Read with a purpose; (2) Use background knowledge to situate the case; (3) Establish the context of the case before beginning to read; (4) Evaluate the case and have an opinion about its outcome; and (5) Read flexibly; skim and skip when appropriate. We can teach students the way in which a legal expert reads the law. The earlier they achieve these skills, the better for the individual students, the more likely their success in law school and the better for the legal profession as a whole.}}
@ARTICLE{Zhang_2007,title={Economic consequences of the Sarbanes–Oxley Act of 2002 ☆},year={2007},author={Ivy Xiying Zhang and Ivy Xiying Zhang},doi={10.1016/j.jacceco.2007.02.002},pmid={null},pmcid={null},mag_id={3125373908},journal={Journal of Accounting and Economics},abstract={Abstract   This paper investigates the economic consequences of the Sarbanes–Oxley Act (SOX) by examining market reactions to related legislative events. Using concurrent stock returns of non-U.S.-traded foreign firms to estimate normal U.S. returns, I find that U.S. firms experienced a statistically significant negative cumulative abnormal return around key SOX events. I then examine the cross-sectional variation of U.S. firms’ returns around these events. Regression results are consistent with the non-audit services and governance provisions imposing net costs. Additional tests show that deferring the compliance of Section 404, which mandates an internal control test, resulted in significant cost savings for non-accelerated filers.}}
@ARTICLE{Amaya_2007,title={Formal models of coherence and legal epistemology},year={2007},author={Amalia Amaya and Amalia Amaya},doi={10.1007/s10506-007-9050-4},pmid={null},pmcid={null},mag_id={3125399593},journal={Artificial Intelligence and Law},abstract={This paper argues that formal models of coherence are useful for constructing a legal epistemology. Two main formal approaches to coherence are examined: coherence-based models of belief revision and the theory of coherence as constraint satisfaction. It is shown that these approaches shed light on central aspects of a coherentist legal epistemology, such as the concept of coherence, the dynamics of coherentist justification in law, and the mechanisms whereby coherence may be built in the course of legal decision-making.}}
@ARTICLE{Jørgensen_2007,title={A Systematic Review of Software Development Cost Estimation Studies},year={2007},author={Henrik L. Jørgensen and Jorgensen and Martin Shepperd and Shepperd},doi={10.1109/tse.2007.256943},pmid={null},pmcid={null},mag_id={3143822685},journal={IEEE Transactions on Software Engineering},abstract={This paper aims to provide a basis for the improvement of software-estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A Web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) increase the breadth of the search for relevant studies, 2) search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) conduct more studies on estimation methods commonly used by the software industry, and 4) increase the awareness of how properties of the data sets impact the results when evaluating estimation methods}}
@ARTICLE{María_2015,title={Interpretation of Data in Psychology： A False Problem, a True Issue},year={2015},author={. María and Maria and Del and del and Rio and Rio and Carral and Carral and Mariê and Marie and Santiago-Delefosse and Santiago-Delefosse},doi={null},pmid={null},pmcid={null},mag_id={3149447472},journal={null},abstract={null}}
@ARTICLE{Fischbach_2021,title={Automatic Detection of Causality in Requirement Artifacts: The CiRA Approach},year={2021},author={Jannik Fischbach and Jannik Fischbach and Julian Frattini and Julian Frattini and Arjen Spaans and Arjen Spaans and Maximilian Kummeth and Maximilian Kummeth and Andreas Vogelsang and Andreas Vogelsang and Daniel Méndez Fernández and Daniel Méndez and Daniel Mendez and Michael Unterkalmsteiner and Michael Unterkalmsteiner},doi={10.1007/978-3-030-73128-1_2},pmid={null},pmcid={null},mag_id={3158607471},journal={null},abstract={System behavior is often expressed by causal relations in requirements (e.g., If event 1, then event 2). Automatically extracting this embedded causal knowledge supports not only reasoning about requirements dependencies, but also various automated engineering tasks such as seamless derivation of test cases. However, causality extraction from natural language is still an open research challenge as existing approaches fail to extract causality with reasonable performance. We understand causality extraction from requirements as a two-step problem: First, we need to detect if requirements have causal properties or not. Second, we need to understand and extract their causal relations. At present, though, we lack knowledge about the form and complexity of causality in requirements, which is necessary to develop a suitable approach addressing these two problems. We conduct an exploratory case study with 14,983 sentences from 53 requirements documents originating from 18 different domains and shed light on the form and complexity of causality in requirements. Based on our findings, we develop a tool-supported approach for causality detection (CiRA). This constitutes a first step towards causality extraction from NL requirements. We report on a case study and the resulting tool-supported approach for causality detection in requirements. Our case study corroborates, among other things, that causality is, in fact, a widely used linguistic pattern to describe system behavior, as about a third of the analyzed sentences are causal. We further demonstrate that our tool CiRA achieves a macro-F1 score of 82 % on real word data and that it outperforms related approaches with an average gain of 11.06 % in macro-Recall and 11.43 % in macro-Precision. Finally, we disclose our open data sets as well as our tool to foster the discourse on the automatic detection of causality in the RE community.}}
@ARTICLE{Dwivedi_2021,title={A Formal Specification Smart-Contract Language for Legally Binding Decentralized Autonomous Organizations},year={2021},author={Vimal Dwivedi and Vimal Dwivedi and Alex Norta and Alex Norta and Alexander J. Wulf and Alexander Wulf and Alexander J. Wulf and Benjamin Leiding and Benjamin Leiding and Benjamin Leiding and Sandeep Saxena and Sandeep Saxena and Sandeep Saxena and Chibuzor Udokwu and Chibuzor Udokwu},doi={10.1109/access.2021.3081926},pmid={null},pmcid={null},mag_id={3161304054},journal={IEEE Access},abstract={Blockchain- and smart-contract technology enhance the effectiveness and automation of business processes. The rising interest in the development of decentralized autonomous organizations (DAO) shows that blockchain technology has the potential to reform business and society. A DAO is an organization wherein business rules are encoded in smart-contract programs that are executed when specified rules are met. The contractual- and business semantics are sine qua non for drafting a legally-binding smart contract in DAO collaborations. Several smart-contract languages (SCLs) exist, such as SPESC, or Symboleo to specify a legally-binding contract. However, their primary focus is on designing and developing smart contracts with the cooperation of IT- and non-IT users. Therefore, this paper fills a gap in the state of the art by specifying a smart-legal-contract markup language (SLCML) for legal- and business constructs to draft a legally-binding DAO. To achieve the paper objective, we first present a formal SCL ontology to describe the legal- and business semantics of a DAO. Secondly, we translate the SCL ontology into SLCML, for which we present the XML schema definition. We demonstrate and evaluate our SLCML language through the specification of a real life-inspired Sale-of-Goods contract. Finally, the SLCML use-case code is translated into Solidity to demonstrate its feasibility for blockchain platform implementations.}}
@ARTICLE{Araszkiewicz_2021,title={Critical Questions to Argumentation Schemes in Statutory Interpretation.},year={2021},author={Michał Araszkiewicz and Michal Araszkiewicz},doi={null},pmid={null},pmcid={null},mag_id={3164494754},journal={null},abstract={null}}
@ARTICLE{Villata_2020,title={Legal Knowledge and Information Systems},year={2020},author={Serena Villata and Jakub Harašta and Jakub Harašta and Petr Křemen},doi={10.3233/faia334},pmid={null},pmcid={null},mag_id={3210984362},journal={null},abstract={null}}
@ARTICLE{Pang_2008,title={Opinion Mining and Sentiment Analysis},year={2008},author={Bo Pang and Bo Pang and Lillian Lee and Lillian Lee},doi={10.1561/1500000011},pmid={null},pmcid={null},mag_id={4205184193},journal={Foundations and Trends in Information Retrieval},abstract={null}}
@ARTICLE{Haider_2006,title={Validation Standard Operating Procedures},year={2006},author={Syed Imtiaz Haider and Syed Imtiaz Haider},doi={10.3109/9781420009415},pmid={null},pmcid={null},mag_id={4205749346},journal={null},abstract={null}}
@ARTICLE{Lloyd_1987,title={Foundations of Logic Programming},year={1987},author={John Wylie Lloyd and John W. Lloyd},doi={10.1007/978-3-642-83189-8},pmid={null},pmcid={null},mag_id={4206370914},journal={null},abstract={null}}
@ARTICLE{Zadeh_1965,title={Fuzzy sets},year={1965},author={L.A. Zadeh and L. A. Zadeh},doi={10.1016/s0019-9958(65)90241-x},pmid={null},pmcid={null},mag_id={4211007335},journal={Information and control},abstract={A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.}}
@ARTICLE{Lehmann_1992,title={Semantic networks},year={1992},author={Fritz Lehmann and Fritz Lėhmann},doi={10.1016/0898-1221(92)90135-5},pmid={null},pmcid={null},mag_id={4211084664},journal={Computers & mathematics with applications},abstract={A semantic network is a graph of the structure of meaning. This article introduces semantic network systems and their importance in Artificial Intelligence, followed by I. the early background; II. a summary of the basic ideas and issues including link types, frame systems, case relations, link valence, abstraction, inheritance hierarchies and logic extensions; and III. a survey of ‘world-structuring’ systems including ontologies, causal link models, continuous models, relevance, formal dictionaries, semantic primitives and intersecting inference hierarchies. Speed and practical implementation are briefly discussed. The conclusion argues for a synthesis of relational graph theory, graph-grammar theory and order theory based on semantic primitives and multiple intersecting inference hierarchies.}}
@ARTICLE{Amgoud_2008,title={On bipolarity in argumentation frameworks},year={2008},author={Leila Amgoud and Leïla Amgoud and Claudette Cayrol and Claudette Cayrol and M. C. Lagasquie-Schiex and Marie-Christine Lagasquie-Schiex and P. Livet and Pierre Livet},doi={10.1002/int.20307},pmid={null},pmcid={null},mag_id={4213419683},journal={International Journal of Intelligent Systems},abstract={In this article, we propose a survey of the use of bipolarity in argumentation frameworks. On the one hand, the notion of bipolarity relies on the presence of two kinds of entities that have a diametrically opposed nature and that represent repellent forces (a positive entity and a negative entity). The notion exists in various domains (for example with the representation of preferences in artificial intelligence, or in cognitive psychology). On the other hand, argumentation process is a promising approach for reasoning, based on the construction and the comparison of arguments. It follows five steps: building the arguments, defining the interactions between these arguments, valuating the arguments, selecting the most acceptable arguments and, finally, drawing a conclusion. Using the nomenclature proposed by Dubois and Prade, this article shows on various applications, and with some formal definitions, that bipolarity appears in argumentation (in some cases if not always) and can be used in each step of this process under different forms. © 2008 Wiley Periodicals, Inc.}}
@ARTICLE{Chisholm_1963,title={Contrary-to-Duty Imperatives and Deontic Logic},year={1963},author={Roderick M. Chisholm and Roderick M. Chisholm},doi={10.2307/3327064},pmid={null},pmcid={null},mag_id={4214815610},journal={Analysis},abstract={Contrary-To-Duty Imperatives and Deontic Logic Get access Roderick M. Chisholm Roderick M. Chisholm Brown University Search for other works by this author on: Oxford Academic Google Scholar Analysis, Volume 24, Issue 2, December 1963, Pages 33–36, https://doi.org/10.1093/analys/24.2.33 Published: 01 December 1963}}
@ARTICLE{Ross_null,title={Tu-Tu},year={null},author={Alf Ross and Alf Ross},doi={10.2307/1337744},pmid={null},pmcid={null},mag_id={4232603964},journal={Harvard Law Review},abstract={null}}
@ARTICLE{Robinson_null,title={Likert Scale},year={null},author={John A. Robinson and John Robinson},doi={10.1007/978-94-007-0753-5_1654},pmid={null},pmcid={null},mag_id={4233238987},journal={null},abstract={null}}
@ARTICLE{Alexander_null,title={The Rule of Rules},year={null},author={Larry Alexander and Larry Alexander and Emily Sherwin and Emily Sherwin},doi={10.1215/9780822380023},pmid={null},pmcid={null},mag_id={4235131312},journal={null},abstract={null}}
@ARTICLE{Sadiq_null,title={Managing Regulatory Compliance in Business Processes},year={null},author={Shazia Sadiq and Shazia Sadiq and Guido Governatori and Guido Governatori},doi={10.1007/978-3-642-01982-1_8},pmid={null},pmcid={null},mag_id={4237666566},journal={null},abstract={null}}
@ARTICLE{Vanek_null,title={Systems engineering metrics and applications in product development: A critical literature review and agenda for further research},year={null},author={Francis Vanek and Francis Vanek and Peter L. Jackson and Peter Jackson and Richard Grzybowski and Richard Grzybowski},doi={10.1002/sys.20089},pmid={null},pmcid={null},mag_id={4238193669},journal={Systems Engineering},abstract={Abstract The military/aerospace industries have several decades of experience in the application of systems engineering. In the world of complex systems developed under contract, systems engineering has a well‐articulated, generally accepted, client‐mandated methodology. This is not the case currently in most commercially oriented research and development organizations. Unlike the military/aerospace environment, it is not widely accepted that systems engineering can or will deliver value within commercial enterprises. This said, it is possible that a systems engineering‐like methodology may be routinely employed but not so stated, or is recognized under a different label. This seems likely since complex products continue to be successfully developed and commercialized via the use of methods that are closely related to systems engineering. The purpose of this paper is to establish the groundwork for measuring the effectiveness of the systems engineering methodology, and closely associated analogs, as part of the product development process in a commercial research and development organization. In this paper, we review the literature in systems engineering and related fields to identify measurement issues and methodologies, specific metrics proposed and in use, and case studies and best practices. We also identify gaps in the literature and propose the next steps in a research agenda. © 2008 Wiley Periodicals, Inc. Syst Eng}}
@ARTICLE{Schneider_null,title={Knowledge Integration},year={null},author={Michael Schneider and Michael Schneider},doi={10.1007/978-1-4419-1428-6_807},pmid={null},pmcid={null},mag_id={4238734312},journal={null},abstract={null}}
@ARTICLE{Aristotle_null,title={Topics},year={null},author={Aristotle and None Aristotle},doi={10.1093/oseo/instance.00258598},pmid={null},pmcid={null},mag_id={4239476733},journal={null},abstract={null}}
@ARTICLE{Hage_null,title={Reasoning with Rules},year={null},author={Jaap Hage and Jaap C. Hage},doi={10.1007/978-94-015-8873-7},pmid={null},pmcid={null},mag_id={4241923257},journal={Law and philosophy library},abstract={Rule-applying legal arguments are traditionally treated as a kind of syllogism. Such a treatment overlooks the fact that legal principles and rules are not statements which describe the world, but rat}}
@ARTICLE{Verheij_null,title={Virtual Arguments},year={null},author={Bart Verheij and Bart Verheij and Bart Verheij},doi={10.1007/978-90-6704-661-9},pmid={null},pmcid={null},mag_id={4245504577},journal={Information technology & law series},abstract={null}}
@ARTICLE{Evans_null,title={Metaknowledge},year={null},author={James A. Evans and James A. Evans and Jacob G. Foster and Jacob G. Foster},doi={10.1126/science.1201765},pmid={null},pmcid={null},mag_id={4246649397},journal={Science},abstract={The growth of electronic publication and informatics archives makes it possible to harvest vast quantities of knowledge about knowledge, or “metaknowledge.” We review the expanding scope of metaknowledge research, which uncovers regularities in scientific claims and infers the beliefs, preferences, research tools, and strategies behind those regularities. Metaknowledge research also investigates the effect of knowledge context on content. Teams and collaboration networks, institutional prestige, and new technologies all shape the substance and direction of research. We argue that as metaknowledge grows in breadth and quality, it will enable researchers to reshape science—to identify areas in need of reexamination, reweight former certainties, and point out new paths that cut across revealed assumptions, heuristics, and disciplinary boundaries.}}
@ARTICLE{Valerdi_null,title={Empirical research in systems engineering: challenges and opportunities of a new frontier},year={null},author={Ricardo Valerdi and Ricardo Valerdi and Heidi L. Davidz and Heidi L. Davidz},doi={10.1002/sys.20117},pmid={null},pmcid={null},mag_id={4247123013},journal={Systems Engineering},abstract={This paper aims to advance the pedagogy of systems engineering by identifying opportunities and challenges in empirical research in the field. After an introduction to how empirical research could be further utilized in systems engineering, this paper discusses challenges faced when conducting empirical research in the field, threats to validity associated with systems engineering data collection, and considerations for empirical mixed-methods research. Two recently completed systems engineering empirical studies are used to illustrate specific examples. Finally, suggestions are given on how a professional society might provide additional support for researchers completing empirical research in systems engineering. The overarching goal of this paper is to describe how the increased use of empirical methods can be used to enrich the quality of research results which will in turn enhance the position of systems engineering as a widely recognized academic field. It is proposed that utilizing well-grounded, valid theory will improve understanding of systems engineering phenomena and advance the maturity of the field. © 2008 Wiley Periodicals, Inc. Syst Eng}}
@ARTICLE{Allen_null,title={Symbolic Logic: A Razor-Edged Tool for Drafting and Interpreting Legal Documents},year={null},author={Layman E. Allen and Layman E. Allen},doi={10.2307/794073},pmid={null},pmcid={null},mag_id={4249318453},journal={Yale Law Journal},abstract={null}}
@ARTICLE{Lodder_null,title={DiaLaw},year={null},author={A.R. Lodder and Arno R. Lodder},doi={10.1007/978-94-011-3957-1},pmid={null},pmcid={null},mag_id={4249557263},journal={Law and philosophy library},abstract={This book is a revised version of my dissertation 'DiaLaw - on legal th justification and dialog games' that I defended on June 5 1998 at the Universiteit Maastricht. The chapters 1, 4 & 5 (now: 1, 5}}
@ARTICLE{Midgley_null,title={The sacred and profane in critical systems thinking},year={null},author={Gerald Midgley and Gerald Midgley},doi={10.1007/bf01060044},pmid={null},pmcid={null},mag_id={4250884815},journal={Systems practice},abstract={null}}
@ARTICLE{Hume_null,title={A Treatise of Human Nature},year={null},author={David Hume and David Hume},doi={10.1093/oseo/instance.00046221},pmid={null},pmcid={null},mag_id={4254892326},journal={null},abstract={null}}
@ARTICLE{Noirie_null,title={Semantic networking: Flow-based, traffic-aware, and self-managed networking},year={null},author={Ludovic Noirie and Ludovic Noirie and Emmanuel Dotaro and Emmanuel Dotaro and Giovanna Carofiglio and Giovanna Carofiglio and A. Dupas and Arnaud Dupas and Pascal Pecci and Pascal Pecci and D. Popa and Daniel Popa and Georg Post and Georg Post},doi={10.1002/bltj.20371},pmid={null},pmcid={null},mag_id={4256417770},journal={Bell Labs Technical Journal},abstract={In order to overcome current Internet limitations on overall network scalability and complexity, we introduce a new paradigm of semantic networking for the networks of the future, which brings together flow-based networking, traffic awareness, and self-management concepts to deliver plug-and-play networks. The natural traffic granularity is the flow between packet and circuit and between connection-less and connection-oriented modes. Using flow aggregation capabilities, we simplify traffic processing in the nodes through elastic fluid switching, and simplify traffic control through flow admission control, policing, and implicit quality of service (QoS) routing. By leveraging deep packet inspection and behavioral traffic analysis, network elements can autonomously and efficiently process the traffic flows they transport through real-time awareness gained via semantic analysis. The global consistency of node decisions within the whole network is ensured by self-management, applying the concepts of “knowledge plane” and “network mining.”}}
@ARTICLE{Pollock_null,title={Defeasible Reasoning},year={null},author={John L. Pollock and John L. Pollock},doi={10.1207/s15516709cog1104_4},pmid={null},pmcid={null},mag_id={4256682192},journal={Cognitive Science},abstract={What philosophers call defeasible reasoning is roughly the same as nonmonotonic reasoning in AI. Some brief remarks are made about the nature of reasoning and the relationship between work in epistemology, AI, and cognitive psychology. This is followed by a general description of human rational architecture. This description has the consequence that defeasible reasoning has a more complicated structure than has generally been recognized in AI. We define a proposition to be warranted if it would be believed by an ideal reasoner. A general theory of warrant, based on defeasible reasons, is developed. This theory is then used as a guide in the construction of a theory of defeasible reasoning, and a computer program implementing that theory. The theory constructed deals with only a subset of defeasible reasoning, but it is an important subset.}}
@ARTICLE{Levi_1948,title={An Introduction to Legal Reasoning},year={1948},author={Edward H. Levi and Edward H. Levi},doi={10.2307/1597535},pmid={null},pmcid={null},mag_id={4297215370},journal={University of Chicago Law Review},abstract={null}}
